{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISCA-k - Diagnóstico Completo\n",
    "\n",
    "Este notebook testa o ISCA-k a 100% para identificar exactamente onde falha e não falha.\n",
    "\n",
    "**Diferenças do notebook anterior:**\n",
    "- Testes com missings em MÚLTIPLAS colunas (para testar PDS de verdade)\n",
    "- Diagnóstico explícito do PDS (scale factors, donors rejeitados)\n",
    "- Comparação directa PDS on vs off\n",
    "- Análise detalhada do Adaptive K\n",
    "- Testes de Phase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports OK\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from iscak_core import ISCAkCore\n",
    "from core.adaptive_k import adaptive_k_hybrid\n",
    "from core.mi_calculator import calculate_mi_mixed\n",
    "from core.distances import (\n",
    "    weighted_euclidean_batch, \n",
    "    weighted_euclidean_pds,\n",
    "    mixed_distance_pds,\n",
    "    range_normalized_mixed_distance\n",
    ")\n",
    "from preprocessing.type_detection import MixedDataHandler\n",
    "from preprocessing.scaling import get_scaled_data, compute_range_factors\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"Imports OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## PARTE 1: Diagnóstico do PDS\n",
    "\n",
    "O PDS (Partial Distance Strategy) permite usar donors com overlap parcial de features.\n",
    "Quando o sample tem NaN em várias colunas, o PDS aplica um scale_factor para compensar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnose_pds(sample, donors, weights, n_features):\n",
    "    \"\"\"\n",
    "    Diagnóstico detalhado do que o PDS está a fazer.\n",
    "    \n",
    "    Mostra para cada donor:\n",
    "    - Overlap (quantas features em comum)\n",
    "    - Scale factor aplicado\n",
    "    - Distância raw vs distância scaled\n",
    "    - Se foi aceite ou rejeitado\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"DIAGNÓSTICO PDS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Contar features disponíveis no sample\n",
    "    sample_avail = ~np.isnan(sample)\n",
    "    n_sample_avail = sample_avail.sum()\n",
    "    print(f\"\\nSample tem {n_sample_avail}/{n_features} features disponíveis\")\n",
    "    print(f\"Features missing no sample: {np.where(~sample_avail)[0].tolist()}\")\n",
    "    \n",
    "    min_overlap = max(2, n_features // 2)\n",
    "    print(f\"Min overlap requerido: {min_overlap} (50% de {n_features})\")\n",
    "    \n",
    "    print(f\"\\n{'Donor':<8} {'Overlap':<12} {'Scale':<10} {'Dist Raw':<12} {'Dist Scaled':<12} {'Status'}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    results = []\n",
    "    for i, donor in enumerate(donors):\n",
    "        # Calcular overlap\n",
    "        donor_avail = ~np.isnan(donor)\n",
    "        overlap_mask = sample_avail & donor_avail\n",
    "        overlap = overlap_mask.sum()\n",
    "        \n",
    "        # Calcular distância raw (só nas features comuns)\n",
    "        if overlap > 0:\n",
    "            diff_sq = 0.0\n",
    "            weight_sum = 0.0\n",
    "            for j in range(n_features):\n",
    "                if overlap_mask[j]:\n",
    "                    diff = sample[j] - donor[j]\n",
    "                    diff_sq += weights[j] * diff * diff\n",
    "                    weight_sum += weights[j]\n",
    "            dist_raw = np.sqrt(diff_sq) if weight_sum > 0 else np.inf\n",
    "        else:\n",
    "            dist_raw = np.inf\n",
    "        \n",
    "        # Scale factor e distância scaled\n",
    "        if overlap >= min_overlap:\n",
    "            scale_factor = np.sqrt(n_features / overlap)\n",
    "            dist_scaled = dist_raw * scale_factor\n",
    "            status = \"✓ ACEITE\"\n",
    "        else:\n",
    "            scale_factor = np.inf\n",
    "            dist_scaled = np.inf\n",
    "            status = \"✗ REJEITADO (overlap insuficiente)\"\n",
    "        \n",
    "        results.append({\n",
    "            'donor_idx': i,\n",
    "            'overlap': overlap,\n",
    "            'scale': scale_factor,\n",
    "            'dist_raw': dist_raw,\n",
    "            'dist_scaled': dist_scaled,\n",
    "            'accepted': overlap >= min_overlap\n",
    "        })\n",
    "        \n",
    "        scale_str = f\"{scale_factor:.3f}\" if np.isfinite(scale_factor) else \"N/A\"\n",
    "        dist_raw_str = f\"{dist_raw:.4f}\" if np.isfinite(dist_raw) else \"inf\"\n",
    "        dist_scaled_str = f\"{dist_scaled:.4f}\" if np.isfinite(dist_scaled) else \"inf\"\n",
    "        \n",
    "        print(f\"{i:<8} {overlap}/{n_features:<10} {scale_str:<10} {dist_raw_str:<12} {dist_scaled_str:<12} {status}\")\n",
    "    \n",
    "    # Resumo\n",
    "    n_accepted = sum(1 for r in results if r['accepted'])\n",
    "    n_rejected = len(results) - n_accepted\n",
    "    print(f\"\\n--- RESUMO ---\")\n",
    "    print(f\"Donors aceites: {n_accepted}/{len(results)}\")\n",
    "    print(f\"Donors rejeitados: {n_rejected}/{len(results)}\")\n",
    "    \n",
    "    if n_accepted > 0:\n",
    "        accepted = [r for r in results if r['accepted']]\n",
    "        avg_scale = np.mean([r['scale'] for r in accepted])\n",
    "        print(f\"Scale factor médio (aceites): {avg_scale:.3f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste PDS 1: Missings em UMA coluna (PDS não actua)\n",
    "\n",
    "Quando só há missing numa coluna, o overlap é quase total e o scale_factor ≈ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TESTE PDS 1: Missings em UMA coluna\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "DIAGNÓSTICO PDS\n",
      "======================================================================\n",
      "\n",
      "Sample tem 9/10 features disponíveis\n",
      "Features missing no sample: [9]\n",
      "Min overlap requerido: 5 (50% de 10)\n",
      "\n",
      "Donor    Overlap      Scale      Dist Raw     Dist Scaled  Status\n",
      "----------------------------------------------------------------------\n",
      "0        9/10         1.054      0.0949       0.1000       ✓ ACEITE\n",
      "1        9/10         1.054      0.1897       0.2000       ✓ ACEITE\n",
      "2        9/10         1.054      3.7947       4.0000       ✓ ACEITE\n",
      "\n",
      "--- RESUMO ---\n",
      "Donors aceites: 3/3\n",
      "Donors rejeitados: 0/3\n",
      "Scale factor médio (aceites): 1.054\n",
      "\n",
      "⚠️  CONCLUSÃO: Com missing em apenas 1 coluna:\n",
      "   - Overlap = 9/10 = 90%\n",
      "   - Scale factor = sqrt(10/9) = 1.054\n",
      "   - O PDS praticamente NÃO ACTUA!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TESTE PDS 1: Missings em UMA coluna\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Dataset simples: 10 features, target na última\n",
    "n_features = 10\n",
    "\n",
    "# Sample: só tem missing no target (coluna 9)\n",
    "sample = np.array([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, np.nan])\n",
    "\n",
    "# Donors: todos completos\n",
    "donors = np.array([\n",
    "    [1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 10.0],  # Próximo\n",
    "    [1.2, 1.2, 1.2, 1.2, 1.2, 1.2, 1.2, 1.2, 1.2, 11.0],  # Próximo\n",
    "    [5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 50.0],  # Distante\n",
    "])\n",
    "\n",
    "weights = np.ones(n_features) / n_features\n",
    "\n",
    "results = diagnose_pds(sample, donors, weights, n_features)\n",
    "\n",
    "print(f\"\\n⚠️  CONCLUSÃO: Com missing em apenas 1 coluna:\")\n",
    "print(f\"   - Overlap = {n_features-1}/{n_features} = {(n_features-1)/n_features*100:.0f}%\")\n",
    "print(f\"   - Scale factor = sqrt({n_features}/{n_features-1}) = {np.sqrt(n_features/(n_features-1)):.3f}\")\n",
    "print(f\"   - O PDS praticamente NÃO ACTUA!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste PDS 2: Missings em MÚLTIPLAS colunas (PDS actua)\n",
    "\n",
    "Quando há missings em várias colunas, o scale_factor aumenta significativamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TESTE PDS 2: Missings em MÚLTIPLAS colunas\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "DIAGNÓSTICO PDS\n",
      "======================================================================\n",
      "\n",
      "Sample tem 6/10 features disponíveis\n",
      "Features missing no sample: [6, 7, 8, 9]\n",
      "Min overlap requerido: 5 (50% de 10)\n",
      "\n",
      "Donor    Overlap      Scale      Dist Raw     Dist Scaled  Status\n",
      "----------------------------------------------------------------------\n",
      "0        6/10         1.291      0.0775       0.1000       ✓ ACEITE\n",
      "1        6/10         1.291      0.1549       0.2000       ✓ ACEITE\n",
      "2        6/10         1.291      0.2324       0.3000       ✓ ACEITE\n",
      "3        6/10         1.291      3.0984       4.0000       ✓ ACEITE\n",
      "4        2/10         N/A        0.0000       inf          ✗ REJEITADO (overlap insuficiente)\n",
      "\n",
      "--- RESUMO ---\n",
      "Donors aceites: 4/5\n",
      "Donors rejeitados: 1/5\n",
      "Scale factor médio (aceites): 1.291\n",
      "\n",
      "✓ CONCLUSÃO: Com missings em MÚLTIPLAS colunas:\n",
      "   - O PDS aplica scale factors significativos\n",
      "   - Donors com pouco overlap são REJEITADOS\n",
      "   - Isto muda completamente a selecção de vizinhos!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TESTE PDS 2: Missings em MÚLTIPLAS colunas\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Sample: missings em 4 colunas (cols 6, 7, 8, 9)\n",
    "sample_multi = np.array([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, np.nan, np.nan, np.nan, np.nan])\n",
    "\n",
    "# Donors com diferentes níveis de completude\n",
    "donors_multi = np.array([\n",
    "    [1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 10.0],  # Completo - overlap 6/10\n",
    "    [1.2, 1.2, 1.2, 1.2, 1.2, 1.2, np.nan, 1.2, 1.2, 11.0],  # Missing col 6 - overlap 5/10\n",
    "    [1.3, 1.3, 1.3, 1.3, 1.3, 1.3, np.nan, np.nan, 1.3, 12.0],  # Missing cols 6,7 - overlap 4/10\n",
    "    [5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 50.0],  # Completo mas distante\n",
    "    [1.0, 1.0, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, 10.0],  # overlap 2/10 - REJEITADO\n",
    "])\n",
    "\n",
    "results = diagnose_pds(sample_multi, donors_multi, weights, n_features)\n",
    "\n",
    "print(f\"\\n✓ CONCLUSÃO: Com missings em MÚLTIPLAS colunas:\")\n",
    "print(f\"   - O PDS aplica scale factors significativos\")\n",
    "print(f\"   - Donors com pouco overlap são REJEITADOS\")\n",
    "print(f\"   - Isto muda completamente a selecção de vizinhos!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste PDS 3: Impacto do PDS na qualidade da imputação\n",
    "\n",
    "Comparar resultados COM e SEM PDS num dataset com múltiplos missings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TESTE PDS 3: Impacto na qualidade (COM vs SEM PDS)\n",
      "======================================================================\n",
      "Dataset: 60 linhas x 9 colunas\n",
      "Cluster 1 (linhas 0-29): features ~1, target ~10\n",
      "Cluster 2 (linhas 30-59): features ~5, target ~50\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TESTE PDS 3: Impacto na qualidade (COM vs SEM PDS)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Criar dataset com 2 clusters bem separados\n",
    "# Cluster 1: features ~1, targets ~10\n",
    "# Cluster 2: features ~5, targets ~50\n",
    "\n",
    "def make_cluster(center, target_center, n_samples, n_features):\n",
    "    data = np.random.normal(center, 0.2, (n_samples, n_features))\n",
    "    targets = np.random.normal(target_center, 1.0, n_samples)\n",
    "    return np.column_stack([data, targets])\n",
    "\n",
    "cluster1 = make_cluster(1.0, 10.0, 30, 8)\n",
    "cluster2 = make_cluster(5.0, 50.0, 30, 8)\n",
    "data_full = np.vstack([cluster1, cluster2])\n",
    "\n",
    "# Criar DataFrame\n",
    "cols = [f'F{i}' for i in range(8)] + ['Target']\n",
    "df_complete = pd.DataFrame(data_full, columns=cols)\n",
    "\n",
    "print(f\"Dataset: {df_complete.shape[0]} linhas x {df_complete.shape[1]} colunas\")\n",
    "print(f\"Cluster 1 (linhas 0-29): features ~1, target ~10\")\n",
    "print(f\"Cluster 2 (linhas 30-59): features ~5, target ~50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missings introduzidos: 92\n",
      "\n",
      "Missings por coluna:\n",
      "  F0: 16 (26.7%)\n",
      "  F1: 7 (11.7%)\n",
      "  F2: 11 (18.3%)\n",
      "  F3: 8 (13.3%)\n",
      "  F4: 11 (18.3%)\n",
      "  F5: 6 (10.0%)\n",
      "  F6: 14 (23.3%)\n",
      "  F7: 10 (16.7%)\n",
      "  Target: 9 (15.0%)\n",
      "\n",
      "Missings por linha: min=0, max=4, média=1.5\n"
     ]
    }
   ],
   "source": [
    "# Introduzir missings em MÚLTIPLAS colunas (padrão realista)\n",
    "# 20% de missings distribuídos aleatoriamente\n",
    "\n",
    "np.random.seed(42)\n",
    "df_missing = df_complete.copy()\n",
    "\n",
    "n_total_values = df_missing.shape[0] * df_missing.shape[1]\n",
    "n_missing = int(0.20 * n_total_values)  # 20% missing\n",
    "\n",
    "# Seleccionar posições aleatórias para missings\n",
    "missing_positions = []\n",
    "rows = np.random.choice(60, n_missing, replace=True)\n",
    "cols_idx = np.random.choice(9, n_missing, replace=True)\n",
    "\n",
    "true_values = {}\n",
    "for r, c in zip(rows, cols_idx):\n",
    "    col_name = cols[c]\n",
    "    if (r, col_name) not in true_values:  # Evitar duplicados\n",
    "        true_values[(r, col_name)] = df_complete.loc[r, col_name]\n",
    "        df_missing.loc[r, col_name] = np.nan\n",
    "\n",
    "# Contar missings por coluna\n",
    "print(f\"\\nMissings introduzidos: {len(true_values)}\")\n",
    "print(f\"\\nMissings por coluna:\")\n",
    "for col in cols:\n",
    "    n_miss = df_missing[col].isna().sum()\n",
    "    print(f\"  {col}: {n_miss} ({n_miss/60*100:.1f}%)\")\n",
    "\n",
    "# Missings por linha (para ver padrão)\n",
    "missings_per_row = df_missing.isna().sum(axis=1)\n",
    "print(f\"\\nMissings por linha: min={missings_per_row.min()}, max={missings_per_row.max()}, média={missings_per_row.mean():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================== COM PDS ==============================\n",
      "\\nCLASSIFICACAO DE VARIAVEIS:\n",
      "  Numericas: 9\n",
      "    ['F0', 'F1', 'F2', 'F3', 'F4']\n",
      "  Binarias: 0\n",
      "  Nominais: 0\n",
      "  Ordinais: 0\n",
      "\n",
      "======================================================================\n",
      "      ISCA-k: Information-theoretic Smart Collaborative Approach      \n",
      "======================================================================\n",
      "\n",
      "Dataset: 60 x 9\n",
      "Missings: 92 (17.0%)\n",
      "Parametros: min_friends=3, max_friends=10\n",
      "MI neighbors: 3\n",
      "Adaptive k alpha: 0.5\n",
      "Fast mode: False\n",
      "FCM clustering: False\n",
      "PDS (partial donors): True\n",
      "  Overlap: adaptativo (maximiza por valor)\n",
      "Max cycles: 3\n",
      "\n",
      "Linhas 100% completas: 7/60 (11.7%)\n",
      "Estratégia: ISCA-k+PDS primeiro, fallback se necessário\n",
      "\n",
      "======================================================================\n",
      "FASE 1: ISCA-k + PDS\n",
      "======================================================================\n",
      "  [1/3] Calculando Informacao Mutua...\n",
      "  [2/3] Ordenando colunas por facilidade...\n",
      "  [3/3] Imputando colunas...\n",
      "\n",
      "  Resultado: 92 → 0 missings\n",
      "             92 imputados (100.0%)\n",
      "\n",
      "======================================================================\n",
      "RESULTADO FINAL\n",
      "======================================================================\n",
      "\n",
      "Fases:\n",
      "  ISCA-k + PDS: 92 → 0 (92 imputados, 100.0%)\n",
      "\n",
      "✅ Fase 1 resolveu tudo (Fase 2 não necessária)\n",
      "\n",
      "Total: 92 → 0 missings\n",
      "Status: SUCESSO - Dataset 100% completo\n",
      "Taxa de imputação: 100.0%\n",
      "Tempo total: 0.15s\n",
      "======================================================================\n",
      "\n",
      "\n",
      "MAE COM PDS: 0.2019\n",
      "Fase 2 activada: False\n"
     ]
    }
   ],
   "source": [
    "# Executar COM PDS\n",
    "print(f\"\\n{'='*30} COM PDS {'='*30}\")\n",
    "imputer_pds = ISCAkCore(verbose=True, use_pds=True, min_friends=3, max_friends=10)\n",
    "result_pds = imputer_pds.impute(df_missing.copy(), interactive=False)\n",
    "\n",
    "# Calcular erros\n",
    "errors_pds = []\n",
    "for (r, col), true_val in true_values.items():\n",
    "    imputed = result_pds.loc[r, col]\n",
    "    if not np.isnan(imputed):\n",
    "        errors_pds.append(abs(imputed - true_val))\n",
    "\n",
    "mae_pds = np.mean(errors_pds) if errors_pds else np.nan\n",
    "print(f\"\\nMAE COM PDS: {mae_pds:.4f}\")\n",
    "print(f\"Fase 2 activada: {imputer_pds.execution_stats.get('phase2_activated', False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================== SEM PDS ==============================\n",
      "\\nCLASSIFICACAO DE VARIAVEIS:\n",
      "  Numericas: 9\n",
      "    ['F0', 'F1', 'F2', 'F3', 'F4']\n",
      "  Binarias: 0\n",
      "  Nominais: 0\n",
      "  Ordinais: 0\n",
      "\n",
      "======================================================================\n",
      "      ISCA-k: Information-theoretic Smart Collaborative Approach      \n",
      "======================================================================\n",
      "\n",
      "Dataset: 60 x 9\n",
      "Missings: 92 (17.0%)\n",
      "Parametros: min_friends=3, max_friends=10\n",
      "MI neighbors: 3\n",
      "Adaptive k alpha: 0.5\n",
      "Fast mode: False\n",
      "FCM clustering: False\n",
      "PDS (partial donors): False\n",
      "Max cycles: 3\n",
      "\n",
      "Linhas 100% completas: 7/60 (11.7%)\n",
      "Estratégia: ISCA-k clássico\n",
      "\n",
      "======================================================================\n",
      "FASE 1: ISCA-k\n",
      "======================================================================\n",
      "  [1/3] Calculando Informacao Mutua...\n",
      "  [2/3] Ordenando colunas por facilidade...\n",
      "  [3/3] Imputando colunas...\n",
      "\n",
      "  Resultado: 92 → 0 missings\n",
      "             92 imputados (100.0%)\n",
      "\n",
      "======================================================================\n",
      "RESULTADO FINAL\n",
      "======================================================================\n",
      "\n",
      "Fases:\n",
      "  ISCA-k: 92 → 0 (92 imputados, 100.0%)\n",
      "\n",
      "✅ Fase 1 resolveu tudo (Fase 2 não necessária)\n",
      "\n",
      "Total: 92 → 0 missings\n",
      "Status: SUCESSO - Dataset 100% completo\n",
      "Taxa de imputação: 100.0%\n",
      "Tempo total: 0.14s\n",
      "======================================================================\n",
      "\n",
      "\n",
      "MAE SEM PDS: 0.3798\n",
      "Fase 2 activada: False\n"
     ]
    }
   ],
   "source": [
    "# Executar SEM PDS\n",
    "print(f\"\\n{'='*30} SEM PDS {'='*30}\")\n",
    "imputer_no_pds = ISCAkCore(verbose=True, use_pds=False, min_friends=3, max_friends=10)\n",
    "result_no_pds = imputer_no_pds.impute(df_missing.copy(), interactive=False)\n",
    "\n",
    "# Calcular erros\n",
    "errors_no_pds = []\n",
    "for (r, col), true_val in true_values.items():\n",
    "    imputed = result_no_pds.loc[r, col]\n",
    "    if not np.isnan(imputed):\n",
    "        errors_no_pds.append(abs(imputed - true_val))\n",
    "\n",
    "mae_no_pds = np.mean(errors_no_pds) if errors_no_pds else np.nan\n",
    "print(f\"\\nMAE SEM PDS: {mae_no_pds:.4f}\")\n",
    "print(f\"Fase 2 activada: {imputer_no_pds.execution_stats.get('phase2_activated', False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "COMPARAÇÃO PDS vs SEM PDS\n",
      "======================================================================\n",
      "\n",
      "MAE COM PDS:  0.2019\n",
      "MAE SEM PDS:  0.3798\n",
      "\n",
      "✓ PDS é MELHOR por 46.8%\n",
      "\n",
      "Valores NÃO imputados:\n",
      "  COM PDS: 0\n",
      "  SEM PDS: 0\n"
     ]
    }
   ],
   "source": [
    "# Comparação\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"COMPARAÇÃO PDS vs SEM PDS\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nMAE COM PDS:  {mae_pds:.4f}\")\n",
    "print(f\"MAE SEM PDS:  {mae_no_pds:.4f}\")\n",
    "\n",
    "if mae_pds < mae_no_pds:\n",
    "    diff = (mae_no_pds - mae_pds) / mae_no_pds * 100\n",
    "    print(f\"\\n✓ PDS é MELHOR por {diff:.1f}%\")\n",
    "elif mae_no_pds < mae_pds:\n",
    "    diff = (mae_pds - mae_no_pds) / mae_pds * 100\n",
    "    print(f\"\\n⚠️  SEM PDS é MELHOR por {diff:.1f}%\")\n",
    "else:\n",
    "    print(f\"\\n= Resultados iguais\")\n",
    "\n",
    "# Valores não imputados\n",
    "n_nan_pds = result_pds.isna().sum().sum()\n",
    "n_nan_no_pds = result_no_pds.isna().sum().sum()\n",
    "print(f\"\\nValores NÃO imputados:\")\n",
    "print(f\"  COM PDS: {n_nan_pds}\")\n",
    "print(f\"  SEM PDS: {n_nan_no_pds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## PARTE 2: Diagnóstico do Adaptive K\n",
    "\n",
    "O Adaptive K escolhe quantos vizinhos usar baseado em:\n",
    "- density_trust: quão próximos estão os vizinhos\n",
    "- consistency_trust: quão concordantes são os targets dos vizinhos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnose_adaptive_k(distances, values, min_k=3, max_k=15, alpha=0.5, is_categorical=False):\n",
    "    \"\"\"\n",
    "    Diagnóstico detalhado do Adaptive K.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"DIAGNÓSTICO ADAPTIVE K\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    print(f\"\\nParâmetros: min_k={min_k}, max_k={max_k}, alpha={alpha}\")\n",
    "    print(f\"Tipo: {'Categórico' if is_categorical else 'Numérico'}\")\n",
    "    \n",
    "    # Usar max_k vizinhos para avaliar\n",
    "    k_eval = min(max_k, len(distances))\n",
    "    sorted_idx = np.argsort(distances)\n",
    "    closest_dist = distances[sorted_idx[:k_eval]]\n",
    "    closest_vals = values[sorted_idx[:k_eval]]\n",
    "    \n",
    "    print(f\"\\nTop {k_eval} vizinhos (ordenados por distância):\")\n",
    "    for i in range(min(10, k_eval)):  # Mostrar top 10\n",
    "        print(f\"  {i+1}. dist={closest_dist[i]:.4f}, target={closest_vals[i]:.2f}\")\n",
    "    \n",
    "    # Calcular density_trust\n",
    "    mean_dist = np.mean(closest_dist[np.isfinite(closest_dist)])\n",
    "    density_trust = 1.0 / (1.0 + mean_dist) if np.isfinite(mean_dist) else 0.5\n",
    "    \n",
    "    print(f\"\\n--- DENSITY TRUST ---\")\n",
    "    print(f\"  Distância média: {mean_dist:.4f}\")\n",
    "    print(f\"  density_trust = 1/(1+{mean_dist:.4f}) = {density_trust:.4f}\")\n",
    "    \n",
    "    # Calcular consistency_trust\n",
    "    if is_categorical:\n",
    "        unique, counts = np.unique(closest_vals, return_counts=True)\n",
    "        consistency_trust = counts.max() / len(closest_vals) if len(counts) > 0 else 0.5\n",
    "        print(f\"\\n--- CONSISTENCY TRUST (Categórico) ---\")\n",
    "        print(f\"  Classes: {dict(zip(unique, counts))}\")\n",
    "        print(f\"  Classe dominante: {unique[np.argmax(counts)]} ({counts.max()}/{len(closest_vals)})\")\n",
    "        print(f\"  consistency_trust = {consistency_trust:.4f}\")\n",
    "    else:\n",
    "        mean_val = np.mean(closest_vals)\n",
    "        std_val = np.std(closest_vals)\n",
    "        cv = std_val / abs(mean_val) if abs(mean_val) > 1e-10 else std_val\n",
    "        consistency_trust = 1.0 / (1.0 + cv)\n",
    "        print(f\"\\n--- CONSISTENCY TRUST (Numérico) ---\")\n",
    "        print(f\"  Média targets: {mean_val:.4f}\")\n",
    "        print(f\"  Std targets: {std_val:.4f}\")\n",
    "        print(f\"  CV (coef. variação) = {std_val:.4f}/{abs(mean_val):.4f} = {cv:.4f}\")\n",
    "        print(f\"  consistency_trust = 1/(1+{cv:.4f}) = {consistency_trust:.4f}\")\n",
    "    \n",
    "    # Calcular trust final e k\n",
    "    trust = alpha * density_trust + (1 - alpha) * consistency_trust\n",
    "    k = int(round(min_k + (max_k - min_k) * trust))\n",
    "    k = max(min_k, min(max_k, k))\n",
    "    \n",
    "    print(f\"\\n--- K FINAL ---\")\n",
    "    print(f\"  trust = {alpha}*{density_trust:.4f} + {1-alpha}*{consistency_trust:.4f} = {trust:.4f}\")\n",
    "    print(f\"  k = {min_k} + ({max_k}-{min_k})*{trust:.4f} = {min_k + (max_k - min_k) * trust:.2f}\")\n",
    "    print(f\"  k final (arredondado): {k}\")\n",
    "    \n",
    "    # Análise de sensibilidade\n",
    "    print(f\"\\n--- ANÁLISE DE SENSIBILIDADE ---\")\n",
    "    print(f\"  Se trust=0.0 → k={min_k}\")\n",
    "    print(f\"  Se trust=0.5 → k={min_k + (max_k - min_k) * 0.5:.0f}\")\n",
    "    print(f\"  Se trust=1.0 → k={max_k}\")\n",
    "    print(f\"  Trust actual: {trust:.4f} → k={k}\")\n",
    "    \n",
    "    return k, trust, density_trust, consistency_trust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TESTE ADAPTIVE K 1: Vizinhos CONSISTENTES\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "DIAGNÓSTICO ADAPTIVE K\n",
      "======================================================================\n",
      "\n",
      "Parâmetros: min_k=3, max_k=15, alpha=0.5\n",
      "Tipo: Numérico\n",
      "\n",
      "Top 15 vizinhos (ordenados por distância):\n",
      "  1. dist=0.1000, target=10.00\n",
      "  2. dist=0.1500, target=10.10\n",
      "  3. dist=0.2000, target=10.20\n",
      "  4. dist=0.2500, target=9.90\n",
      "  5. dist=0.3000, target=10.00\n",
      "  6. dist=0.3500, target=10.10\n",
      "  7. dist=0.4000, target=9.80\n",
      "  8. dist=0.4500, target=10.20\n",
      "  9. dist=0.5000, target=10.00\n",
      "  10. dist=0.5500, target=10.10\n",
      "\n",
      "--- DENSITY TRUST ---\n",
      "  Distância média: 0.4833\n",
      "  density_trust = 1/(1+0.4833) = 0.6742\n",
      "\n",
      "--- CONSISTENCY TRUST (Numérico) ---\n",
      "  Média targets: 10.0400\n",
      "  Std targets: 0.1143\n",
      "  CV (coef. variação) = 0.1143/10.0400 = 0.0114\n",
      "  consistency_trust = 1/(1+0.0114) = 0.9887\n",
      "\n",
      "--- K FINAL ---\n",
      "  trust = 0.5*0.6742 + 0.5*0.9887 = 0.8315\n",
      "  k = 3 + (15-3)*0.8315 = 12.98\n",
      "  k final (arredondado): 13\n",
      "\n",
      "--- ANÁLISE DE SENSIBILIDADE ---\n",
      "  Se trust=0.0 → k=3\n",
      "  Se trust=0.5 → k=9\n",
      "  Se trust=1.0 → k=15\n",
      "  Trust actual: 0.8315 → k=13\n",
      "\n",
      "ESPERADO: k ALTO porque vizinhos concordam\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TESTE ADAPTIVE K 1: Vizinhos CONSISTENTES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Vizinhos próximos e com targets similares\n",
    "distances_consistent = np.array([0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "values_consistent = np.array([10.0, 10.1, 10.2, 9.9, 10.0, 10.1, 9.8, 10.2, 10.0, 10.1, 9.9, 10.0, 10.1, 10.2, 10.0])\n",
    "\n",
    "k1, trust1, dens1, cons1 = diagnose_adaptive_k(distances_consistent, values_consistent)\n",
    "\n",
    "print(f\"\\nESPERADO: k ALTO porque vizinhos concordam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TESTE ADAPTIVE K 2: Vizinhos INCONSISTENTES\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "DIAGNÓSTICO ADAPTIVE K\n",
      "======================================================================\n",
      "\n",
      "Parâmetros: min_k=3, max_k=15, alpha=0.5\n",
      "Tipo: Numérico\n",
      "\n",
      "Top 15 vizinhos (ordenados por distância):\n",
      "  1. dist=0.1000, target=10.00\n",
      "  2. dist=0.1500, target=50.00\n",
      "  3. dist=0.2000, target=5.00\n",
      "  4. dist=0.2500, target=100.00\n",
      "  5. dist=0.3000, target=20.00\n",
      "  6. dist=0.3500, target=80.00\n",
      "  7. dist=0.4000, target=15.00\n",
      "  8. dist=0.4500, target=90.00\n",
      "  9. dist=0.5000, target=30.00\n",
      "  10. dist=0.5500, target=70.00\n",
      "\n",
      "--- DENSITY TRUST ---\n",
      "  Distância média: 0.4833\n",
      "  density_trust = 1/(1+0.4833) = 0.6742\n",
      "\n",
      "--- CONSISTENCY TRUST (Numérico) ---\n",
      "  Média targets: 45.6667\n",
      "  Std targets: 28.6860\n",
      "  CV (coef. variação) = 28.6860/45.6667 = 0.6282\n",
      "  consistency_trust = 1/(1+0.6282) = 0.6142\n",
      "\n",
      "--- K FINAL ---\n",
      "  trust = 0.5*0.6742 + 0.5*0.6142 = 0.6442\n",
      "  k = 3 + (15-3)*0.6442 = 10.73\n",
      "  k final (arredondado): 11\n",
      "\n",
      "--- ANÁLISE DE SENSIBILIDADE ---\n",
      "  Se trust=0.0 → k=3\n",
      "  Se trust=0.5 → k=9\n",
      "  Se trust=1.0 → k=15\n",
      "  Trust actual: 0.6442 → k=11\n",
      "\n",
      "ESPERADO: k BAIXO porque vizinhos discordam\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TESTE ADAPTIVE K 2: Vizinhos INCONSISTENTES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Vizinhos próximos mas com targets muito diferentes\n",
    "distances_inconsistent = np.array([0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "values_inconsistent = np.array([10.0, 50.0, 5.0, 100.0, 20.0, 80.0, 15.0, 90.0, 30.0, 70.0, 25.0, 60.0, 35.0, 55.0, 40.0])\n",
    "\n",
    "k2, trust2, dens2, cons2 = diagnose_adaptive_k(distances_inconsistent, values_inconsistent)\n",
    "\n",
    "print(f\"\\nESPERADO: k BAIXO porque vizinhos discordam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TESTE ADAPTIVE K 3: Vizinhos DISTANTES\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "DIAGNÓSTICO ADAPTIVE K\n",
      "======================================================================\n",
      "\n",
      "Parâmetros: min_k=3, max_k=15, alpha=0.5\n",
      "Tipo: Numérico\n",
      "\n",
      "Top 15 vizinhos (ordenados por distância):\n",
      "  1. dist=2.0000, target=10.00\n",
      "  2. dist=2.5000, target=10.10\n",
      "  3. dist=3.0000, target=10.20\n",
      "  4. dist=3.5000, target=9.90\n",
      "  5. dist=4.0000, target=10.00\n",
      "  6. dist=4.5000, target=10.10\n",
      "  7. dist=5.0000, target=9.80\n",
      "  8. dist=5.5000, target=10.20\n",
      "  9. dist=6.0000, target=10.00\n",
      "  10. dist=6.5000, target=10.10\n",
      "\n",
      "--- DENSITY TRUST ---\n",
      "  Distância média: 5.5000\n",
      "  density_trust = 1/(1+5.5000) = 0.1538\n",
      "\n",
      "--- CONSISTENCY TRUST (Numérico) ---\n",
      "  Média targets: 10.0400\n",
      "  Std targets: 0.1143\n",
      "  CV (coef. variação) = 0.1143/10.0400 = 0.0114\n",
      "  consistency_trust = 1/(1+0.0114) = 0.9887\n",
      "\n",
      "--- K FINAL ---\n",
      "  trust = 0.5*0.1538 + 0.5*0.9887 = 0.5713\n",
      "  k = 3 + (15-3)*0.5713 = 9.86\n",
      "  k final (arredondado): 10\n",
      "\n",
      "--- ANÁLISE DE SENSIBILIDADE ---\n",
      "  Se trust=0.0 → k=3\n",
      "  Se trust=0.5 → k=9\n",
      "  Se trust=1.0 → k=15\n",
      "  Trust actual: 0.5713 → k=10\n",
      "\n",
      "ESPERADO: k BAIXO porque vizinhos estão longe (density_trust baixo)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TESTE ADAPTIVE K 3: Vizinhos DISTANTES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Vizinhos distantes mas consistentes\n",
    "distances_far = np.array([2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6.0, 6.5, 7.0, 7.5, 8.0, 8.5, 9.0])\n",
    "values_far = np.array([10.0, 10.1, 10.2, 9.9, 10.0, 10.1, 9.8, 10.2, 10.0, 10.1, 9.9, 10.0, 10.1, 10.2, 10.0])\n",
    "\n",
    "k3, trust3, dens3, cons3 = diagnose_adaptive_k(distances_far, values_far)\n",
    "\n",
    "print(f\"\\nESPERADO: k BAIXO porque vizinhos estão longe (density_trust baixo)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "COMPARAÇÃO ADAPTIVE K\n",
      "======================================================================\n",
      "\n",
      "Cenário                   Density      Consistency  Trust      K\n",
      "----------------------------------------------------------------------\n",
      "Consistente (próximo)     0.6742       0.9887       0.8315     13\n",
      "Inconsistente (próximo)   0.6742       0.6142       0.6442     11\n",
      "Consistente (distante)    0.1538       0.9887       0.5713     10\n",
      "\n",
      "--- ANÁLISE ---\n",
      "✓ k_consistente (13) > k_inconsistente (11) - CORRECTO\n",
      "\n",
      "Variação total de k: 10 a 13\n"
     ]
    }
   ],
   "source": [
    "# Comparação\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"COMPARAÇÃO ADAPTIVE K\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\n{'Cenário':<25} {'Density':<12} {'Consistency':<12} {'Trust':<10} {'K'}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Consistente (próximo)':<25} {dens1:<12.4f} {cons1:<12.4f} {trust1:<10.4f} {k1}\")\n",
    "print(f\"{'Inconsistente (próximo)':<25} {dens2:<12.4f} {cons2:<12.4f} {trust2:<10.4f} {k2}\")\n",
    "print(f\"{'Consistente (distante)':<25} {dens3:<12.4f} {cons3:<12.4f} {trust3:<10.4f} {k3}\")\n",
    "\n",
    "print(f\"\\n--- ANÁLISE ---\")\n",
    "if k1 > k2:\n",
    "    print(f\"✓ k_consistente ({k1}) > k_inconsistente ({k2}) - CORRECTO\")\n",
    "else:\n",
    "    print(f\"⚠️  k_consistente ({k1}) <= k_inconsistente ({k2}) - PROBLEMA!\")\n",
    "\n",
    "print(f\"\\nVariação total de k: {min(k1,k2,k3)} a {max(k1,k2,k3)}\")\n",
    "if max(k1,k2,k3) - min(k1,k2,k3) < 3:\n",
    "    print(f\"⚠️  K varia pouco! A fórmula pode precisar de ajuste.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## PARTE 3: Diagnóstico da Phase 2\n",
    "\n",
    "A Phase 2 é activada quando a Phase 1 não consegue imputar todos os valores.\n",
    "Isto acontece quando há dependências circulares entre colunas com missings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TESTE PHASE 2: Dependências circulares\n",
      "======================================================================\n",
      "\n",
      "Dataset:\n",
      "     A    B    C    D\n",
      "0  1.0  2.0  3.0  4.0\n",
      "1  1.1  2.1  3.1  4.1\n",
      "2  1.2  2.2  3.2  4.2\n",
      "3  1.3  2.3  3.3  4.3\n",
      "4  1.4  2.4  3.4  4.4\n",
      "5  NaN  2.5  3.5  4.5\n",
      "6  1.6  NaN  3.6  4.6\n",
      "7  1.7  2.7  NaN  4.7\n",
      "8  1.8  2.8  3.8  NaN\n",
      "\n",
      "Linhas 0-4: Completas (donors)\n",
      "Linhas 5-8: Cada uma tem 1 missing diferente\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TESTE PHASE 2: Dependências circulares\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Dataset onde cada linha tem missings em posições diferentes\n",
    "# que criam dependência circular\n",
    "\n",
    "# Base: linhas completas para referência\n",
    "base_data = pd.DataFrame({\n",
    "    'A': [1.0, 1.1, 1.2, 1.3, 1.4],\n",
    "    'B': [2.0, 2.1, 2.2, 2.3, 2.4],\n",
    "    'C': [3.0, 3.1, 3.2, 3.3, 3.4],\n",
    "    'D': [4.0, 4.1, 4.2, 4.3, 4.4],\n",
    "})\n",
    "\n",
    "# Linhas com missings em padrão que força Phase 2\n",
    "# Linha 5: missing em A, tem B,C,D\n",
    "# Linha 6: missing em B, tem A,C,D\n",
    "# Linha 7: missing em C, tem A,B,D\n",
    "# Linha 8: missing em D, tem A,B,C\n",
    "# Para imputar A da linha 5, preciso de donors com A preenchido\n",
    "# Mas se todos os donors também têm missings...\n",
    "\n",
    "problem_data = pd.DataFrame({\n",
    "    'A': [np.nan, 1.6, 1.7, 1.8],\n",
    "    'B': [2.5, np.nan, 2.7, 2.8],\n",
    "    'C': [3.5, 3.6, np.nan, 3.8],\n",
    "    'D': [4.5, 4.6, 4.7, np.nan],\n",
    "})\n",
    "\n",
    "df_phase2 = pd.concat([base_data, problem_data], ignore_index=True)\n",
    "\n",
    "print(f\"\\nDataset:\")\n",
    "print(df_phase2.to_string())\n",
    "print(f\"\\nLinhas 0-4: Completas (donors)\")\n",
    "print(f\"Linhas 5-8: Cada uma tem 1 missing diferente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nCLASSIFICACAO DE VARIAVEIS:\n",
      "  Numericas: 4\n",
      "    ['A', 'B', 'C', 'D']\n",
      "  Binarias: 0\n",
      "  Nominais: 0\n",
      "  Ordinais: 0\n",
      "\n",
      "======================================================================\n",
      "      ISCA-k: Information-theoretic Smart Collaborative Approach      \n",
      "======================================================================\n",
      "\n",
      "Dataset: 9 x 4\n",
      "Missings: 4 (11.1%)\n",
      "Parametros: min_friends=2, max_friends=15\n",
      "MI neighbors: 3\n",
      "Adaptive k alpha: 0.5\n",
      "Fast mode: False\n",
      "FCM clustering: False\n",
      "PDS (partial donors): True\n",
      "  Overlap: adaptativo (maximiza por valor)\n",
      "Max cycles: 3\n",
      "\n",
      "Linhas 100% completas: 5/9 (55.6%)\n",
      "Estratégia: ISCA-k+PDS primeiro, fallback se necessário\n",
      "\n",
      "======================================================================\n",
      "FASE 1: ISCA-k + PDS\n",
      "======================================================================\n",
      "  [1/3] Calculando Informacao Mutua...\n",
      "  [2/3] Ordenando colunas por facilidade...\n",
      "  [3/3] Imputando colunas...\n",
      "\n",
      "  Resultado: 4 → 0 missings\n",
      "             4 imputados (100.0%)\n",
      "\n",
      "======================================================================\n",
      "RESULTADO FINAL\n",
      "======================================================================\n",
      "\n",
      "Fases:\n",
      "  ISCA-k + PDS: 4 → 0 (4 imputados, 100.0%)\n",
      "\n",
      "✅ Fase 1 resolveu tudo (Fase 2 não necessária)\n",
      "\n",
      "Total: 4 → 0 missings\n",
      "Status: SUCESSO - Dataset 100% completo\n",
      "Taxa de imputação: 100.0%\n",
      "Tempo total: 0.04s\n",
      "======================================================================\n",
      "\n",
      "\n",
      "--- RESULTADO ---\n",
      "          A         B         C        D\n",
      "0  1.000000  2.000000  3.000000  4.00000\n",
      "1  1.100000  2.100000  3.100000  4.10000\n",
      "2  1.200000  2.200000  3.200000  4.20000\n",
      "3  1.300000  2.300000  3.300000  4.30000\n",
      "4  1.400000  2.400000  3.400000  4.40000\n",
      "5  1.451275  2.500000  3.500000  4.50000\n",
      "6  1.600000  2.500359  3.600000  4.60000\n",
      "7  1.700000  2.700000  3.537093  4.70000\n",
      "8  1.800000  2.800000  3.800000  4.50729\n",
      "\n",
      "--- ESTATÍSTICAS ---\n",
      "Phase 2 activada: False\n",
      "Phase 2 ciclos: 0\n",
      "Phase 2 imputados: 0\n"
     ]
    }
   ],
   "source": [
    "# Executar e ver se Phase 2 activa\n",
    "imputer_p2 = ISCAkCore(verbose=True, use_pds=True, min_friends=2)\n",
    "result_p2 = imputer_p2.impute(df_phase2.copy(), interactive=False)\n",
    "\n",
    "print(f\"\\n--- RESULTADO ---\")\n",
    "print(result_p2.to_string())\n",
    "\n",
    "print(f\"\\n--- ESTATÍSTICAS ---\")\n",
    "stats = imputer_p2.execution_stats\n",
    "print(f\"Phase 2 activada: {stats.get('phase2_activated', 'N/A')}\")\n",
    "print(f\"Phase 2 ciclos: {stats.get('phase2_cycles', 'N/A')}\")\n",
    "print(f\"Phase 2 imputados: {stats.get('phase2_imputed', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TESTE PHASE 2 EXTREMO: Todos com múltiplos missings\n",
      "======================================================================\n",
      "\n",
      "Dataset extremo:\n",
      "     A    B    C    D\n",
      "0  1.0  2.0  3.0  4.0\n",
      "1  1.1  2.1  3.1  4.1\n",
      "2  NaN  2.2  NaN  NaN\n",
      "3  NaN  NaN  3.3  NaN\n",
      "4  NaN  NaN  NaN  4.4\n",
      "5  NaN  2.2  NaN  NaN\n",
      "6  1.0  NaN  3.0  NaN\n",
      "7  1.1  NaN  NaN  4.1\n",
      "\n",
      "Missings por linha: [0, 0, 3, 3, 3, 3, 2, 2]\n",
      "Linhas completas: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TESTE PHASE 2 EXTREMO: Todos com múltiplos missings\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Dataset mais extremo: poucas linhas completas, maioria com múltiplos missings\n",
    "df_extreme = pd.DataFrame({\n",
    "    'A': [1.0, 1.1, np.nan, np.nan, np.nan, np.nan, 1.0, 1.1],\n",
    "    'B': [2.0, 2.1, 2.2, np.nan, np.nan, 2.2, np.nan, np.nan],\n",
    "    'C': [3.0, 3.1, np.nan, 3.3, np.nan, np.nan, 3.0, np.nan],\n",
    "    'D': [4.0, 4.1, np.nan, np.nan, 4.4, np.nan, np.nan, 4.1],\n",
    "})\n",
    "\n",
    "print(f\"\\nDataset extremo:\")\n",
    "print(df_extreme.to_string())\n",
    "print(f\"\\nMissings por linha: {df_extreme.isna().sum(axis=1).tolist()}\")\n",
    "print(f\"Linhas completas: {(~df_extreme.isna().any(axis=1)).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nCLASSIFICACAO DE VARIAVEIS:\n",
      "  Numericas: 4\n",
      "    ['A', 'B', 'C', 'D']\n",
      "  Binarias: 0\n",
      "  Nominais: 0\n",
      "  Ordinais: 0\n",
      "\n",
      "======================================================================\n",
      "      ISCA-k: Information-theoretic Smart Collaborative Approach      \n",
      "======================================================================\n",
      "\n",
      "Dataset: 8 x 4\n",
      "Missings: 16 (50.0%)\n",
      "Parametros: min_friends=2, max_friends=15\n",
      "MI neighbors: 3\n",
      "Adaptive k alpha: 0.5\n",
      "Fast mode: False\n",
      "FCM clustering: False\n",
      "PDS (partial donors): True\n",
      "  Overlap: adaptativo (maximiza por valor)\n",
      "Max cycles: 3\n",
      "\n",
      "Linhas 100% completas: 2/8 (25.0%)\n",
      "Estratégia: ISCA-k+PDS primeiro, fallback se necessário\n",
      "\n",
      "======================================================================\n",
      "FASE 1: ISCA-k + PDS\n",
      "======================================================================\n",
      "  [1/3] Calculando Informacao Mutua...\n",
      "  [2/3] Ordenando colunas por facilidade...\n",
      "  [3/3] Imputando colunas...\n",
      "\n",
      "  Resultado: 16 → 0 missings\n",
      "             16 imputados (100.0%)\n",
      "\n",
      "======================================================================\n",
      "RESULTADO FINAL\n",
      "======================================================================\n",
      "\n",
      "Fases:\n",
      "  ISCA-k + PDS: 16 → 0 (16 imputados, 100.0%)\n",
      "\n",
      "✅ Fase 1 resolveu tudo (Fase 2 não necessária)\n",
      "\n",
      "Total: 16 → 0 missings\n",
      "Status: SUCESSO - Dataset 100% completo\n",
      "Taxa de imputação: 100.0%\n",
      "Tempo total: 0.04s\n",
      "======================================================================\n",
      "\n",
      "\n",
      "--- RESULTADO ---\n",
      "          A         B         C         D\n",
      "0  1.000000  2.000000  3.000000  4.000000\n",
      "1  1.100000  2.100000  3.100000  4.100000\n",
      "2  1.066667  2.200000  3.066667  4.066667\n",
      "3  1.042857  2.060000  3.300000  4.060000\n",
      "4  1.072727  2.057143  3.057143  4.400000\n",
      "5  1.066667  2.200000  3.066667  4.066667\n",
      "6  1.000000  2.000000  3.000000  4.000000\n",
      "7  1.100000  2.100000  3.100000  4.100000\n",
      "\n",
      "--- ESTATÍSTICAS ---\n",
      "Phase 2 activada: False\n",
      "Phase 2 ciclos: 0\n",
      "Phase 2 imputados: 0\n",
      "\n",
      "NaN restantes: 0\n"
     ]
    }
   ],
   "source": [
    "imputer_extreme = ISCAkCore(verbose=True, use_pds=True, min_friends=2)\n",
    "result_extreme = imputer_extreme.impute(df_extreme.copy(), interactive=False)\n",
    "\n",
    "print(f\"\\n--- RESULTADO ---\")\n",
    "print(result_extreme.to_string())\n",
    "\n",
    "print(f\"\\n--- ESTATÍSTICAS ---\")\n",
    "stats = imputer_extreme.execution_stats\n",
    "print(f\"Phase 2 activada: {stats.get('phase2_activated', 'N/A')}\")\n",
    "print(f\"Phase 2 ciclos: {stats.get('phase2_cycles', 'N/A')}\")\n",
    "print(f\"Phase 2 imputados: {stats.get('phase2_imputed', 'N/A')}\")\n",
    "\n",
    "# Verificar se sobrou algum NaN\n",
    "n_remaining_nan = result_extreme.isna().sum().sum()\n",
    "print(f\"\\nNaN restantes: {n_remaining_nan}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## PARTE 4: Teste Integrado - Simular Benchmark Real\n",
    "\n",
    "Testar com condições similares aos benchmarks (MCAR, MAR, MNAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def introduce_mcar(df, rate=0.2, seed=42):\n",
    "    \"\"\"Introduz missings MCAR (completamente aleatórios)\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    df_missing = df.copy()\n",
    "    mask = np.random.random(df.shape) < rate\n",
    "    df_missing = df_missing.mask(mask)\n",
    "    return df_missing\n",
    "\n",
    "def introduce_mar(df, rate=0.2, seed=42):\n",
    "    \"\"\"Introduz missings MAR (dependentes de outras colunas)\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    df_missing = df.copy()\n",
    "    \n",
    "    # Escolher coluna causativa (numérica)\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if len(numeric_cols) < 2:\n",
    "        return introduce_mcar(df, rate, seed)\n",
    "    \n",
    "    cause_col = numeric_cols[0]\n",
    "    affect_cols = numeric_cols[1:]\n",
    "    \n",
    "    # Missings mais prováveis onde cause_col é alto\n",
    "    cause_vals = df[cause_col].values\n",
    "    prob = (cause_vals - cause_vals.min()) / (cause_vals.max() - cause_vals.min() + 1e-10)\n",
    "    prob = prob * rate * 2  # Escalar para taxa desejada\n",
    "    \n",
    "    for col in affect_cols:\n",
    "        mask = np.random.random(len(df)) < prob\n",
    "        df_missing.loc[mask, col] = np.nan\n",
    "    \n",
    "    return df_missing\n",
    "\n",
    "def introduce_mnar(df, rate=0.2, seed=42):\n",
    "    \"\"\"Introduz missings MNAR (dependentes do próprio valor)\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    df_missing = df.copy()\n",
    "    \n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        vals = df[col].values\n",
    "        # Missings mais prováveis para valores altos\n",
    "        prob = (vals - vals.min()) / (vals.max() - vals.min() + 1e-10)\n",
    "        prob = prob * rate * 2\n",
    "        mask = np.random.random(len(df)) < prob\n",
    "        df_missing.loc[mask, col] = np.nan\n",
    "    \n",
    "    return df_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TESTE INTEGRADO: Dataset tipo benchmark\n",
      "======================================================================\n",
      "Dataset: (150, 5)\n",
      "\n",
      "Correlações com Target:\n",
      "  F1: 0.9113\n",
      "  F2: 0.8955\n",
      "  F3: 0.0304\n",
      "  F4: 0.3415\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TESTE INTEGRADO: Dataset tipo benchmark\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Criar dataset com estrutura conhecida\n",
    "np.random.seed(42)\n",
    "n = 150\n",
    "\n",
    "# Features correlacionadas\n",
    "F1 = np.random.normal(0, 1, n)\n",
    "F2 = F1 * 0.8 + np.random.normal(0, 0.5, n)  # Correlação forte com F1\n",
    "F3 = np.random.normal(0, 1, n)  # Independente\n",
    "F4 = F1 * 0.3 + F3 * 0.3 + np.random.normal(0, 0.7, n)  # Correlação média\n",
    "Target = F1 * 2 + F2 * 1.5 + F4 * 0.5 + np.random.normal(0, 1, n)  # Target depende de F1, F2, F4\n",
    "\n",
    "df_bench = pd.DataFrame({\n",
    "    'F1': F1, 'F2': F2, 'F3': F3, 'F4': F4, 'Target': Target\n",
    "})\n",
    "\n",
    "print(f\"Dataset: {df_bench.shape}\")\n",
    "print(f\"\\nCorrelações com Target:\")\n",
    "for col in ['F1', 'F2', 'F3', 'F4']:\n",
    "    corr = np.corrcoef(df_bench[col], df_bench['Target'])[0, 1]\n",
    "    print(f\"  {col}: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark_test(df_complete, missing_type, rate, use_pds=True):\n",
    "    \"\"\"Executa um teste de benchmark\"\"\"\n",
    "    try:\n",
    "        # Cópia com índice limpo\n",
    "        df_comp = df_complete.copy().reset_index(drop=True)\n",
    "        \n",
    "        # Introduzir missings\n",
    "        if missing_type == 'MCAR':\n",
    "            df_missing = introduce_mcar(df_comp, rate)\n",
    "        elif missing_type == 'MAR':\n",
    "            df_missing = introduce_mar(df_comp, rate)\n",
    "        else:\n",
    "            df_missing = introduce_mnar(df_comp, rate)\n",
    "        \n",
    "        df_missing = df_missing.reset_index(drop=True)\n",
    "        \n",
    "        # Guardar ground truth\n",
    "        missing_mask = df_missing.isna()\n",
    "        \n",
    "        # Imputar\n",
    "        imputer = ISCAkCore(verbose=False, use_pds=use_pds, min_friends=3, max_friends=10)\n",
    "        result = imputer.impute(df_missing.copy(), interactive=False)\n",
    "        result = result.reset_index(drop=True)\n",
    "        \n",
    "        # Calcular erros\n",
    "        errors = []\n",
    "        for col in df_comp.columns:\n",
    "            for i in range(len(df_comp)):\n",
    "                if missing_mask.iloc[i][col]:  # Era missing\n",
    "                    true_val = df_comp.iloc[i][col]\n",
    "                    imp_val = result.iloc[i][col]\n",
    "                    if pd.notna(imp_val):\n",
    "                        errors.append(abs(imp_val - true_val))\n",
    "        \n",
    "        mae = np.mean(errors) if errors else np.nan\n",
    "        n_missing = missing_mask.sum().sum()\n",
    "        n_imputed = len(errors)\n",
    "        \n",
    "        return {\n",
    "            'type': missing_type,\n",
    "            'rate': rate,\n",
    "            'pds': use_pds,\n",
    "            'n_missing': n_missing,\n",
    "            'n_imputed': n_imputed,\n",
    "            'mae': mae,\n",
    "            'phase2': imputer.execution_stats.get('phase2_activated', False)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"ERRO em {missing_type} {rate*100:.0f}% PDS={use_pds}: {e}\")\n",
    "        return {\n",
    "            'type': missing_type,\n",
    "            'rate': rate,\n",
    "            'pds': use_pds,\n",
    "            'n_missing': 0,\n",
    "            'n_imputed': 0,\n",
    "            'mae': np.nan,\n",
    "            'phase2': False\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "BENCHMARK: MCAR, MAR, MNAR x 10%, 20%, 30%, 40%\n",
      "======================================================================\n",
      "A testar MCAR 10%... PDS=0.7922, NO_PDS=0.7265\n",
      "A testar MCAR 20%... PDS=0.9281, NO_PDS=0.8399\n",
      "A testar MCAR 30%... ERRO em MCAR 30% PDS=True: single positional indexer is out-of-bounds\n",
      "ERRO em MCAR 30% PDS=False: single positional indexer is out-of-bounds\n",
      "PDS=nan, NO_PDS=nan\n",
      "A testar MCAR 40%... ERRO em MCAR 40% PDS=True: single positional indexer is out-of-bounds\n",
      "ERRO em MCAR 40% PDS=False: single positional indexer is out-of-bounds\n",
      "PDS=nan, NO_PDS=nan\n",
      "A testar MAR 10%... PDS=0.7775, NO_PDS=0.7357\n",
      "A testar MAR 20%... PDS=0.7312, NO_PDS=0.6721\n",
      "A testar MAR 30%... PDS=0.7349, NO_PDS=0.6861\n",
      "A testar MAR 40%... PDS=0.8109, NO_PDS=0.7295\n",
      "A testar MNAR 10%... PDS=0.8736, NO_PDS=0.7026\n",
      "A testar MNAR 20%... PDS=0.9298, NO_PDS=0.6695\n",
      "A testar MNAR 30%... PDS=1.0193, NO_PDS=0.8765\n",
      "A testar MNAR 40%... ERRO em MNAR 40% PDS=True: single positional indexer is out-of-bounds\n",
      "ERRO em MNAR 40% PDS=False: single positional indexer is out-of-bounds\n",
      "PDS=nan, NO_PDS=nan\n",
      "\n",
      "Type   Rate   PDS   Missing  Imputed  MAE        Phase2\n",
      "----------------------------------------------------------------------\n",
      "MCAR   10%   ON    83       83       0.7922     no\n",
      "MCAR   10%   OFF   83       83       0.7265     no\n",
      "MCAR   20%   ON    169      169      0.9281     no\n",
      "MCAR   20%   OFF   169      169      0.8399     no\n",
      "MCAR   30%   ON    0        0        N/A        no\n",
      "MCAR   30%   OFF   0        0        N/A        no\n",
      "MCAR   40%   ON    0        0        N/A        no\n",
      "MCAR   40%   OFF   0        0        N/A        no\n",
      "MAR    10%   ON    72       72       0.7775     no\n",
      "MAR    10%   OFF   72       72       0.7357     no\n",
      "MAR    20%   ON    121      121      0.7312     no\n",
      "MAR    20%   OFF   121      121      0.6721     no\n",
      "MAR    30%   ON    179      179      0.7349     no\n",
      "MAR    30%   OFF   179      179      0.6861     no\n",
      "MAR    40%   ON    234      234      0.8109     no\n",
      "MAR    40%   OFF   234      234      0.7295     no\n",
      "MNAR   10%   ON    84       84       0.8736     no\n",
      "MNAR   10%   OFF   84       84       0.7026     no\n",
      "MNAR   20%   ON    158      158      0.9298     no\n",
      "MNAR   20%   OFF   158      158      0.6695     no\n",
      "MNAR   30%   ON    225      225      1.0193     no\n",
      "MNAR   30%   OFF   225      225      0.8765     no\n",
      "MNAR   40%   ON    0        0        N/A        no\n",
      "MNAR   40%   OFF   0        0        N/A        no\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"BENCHMARK: MCAR, MAR, MNAR x 10%, 20%, 30%, 40%\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results = []\n",
    "\n",
    "for missing_type in ['MCAR', 'MAR', 'MNAR']:\n",
    "    for rate in [0.10, 0.20, 0.30, 0.40]:\n",
    "        print(f\"A testar {missing_type} {rate*100:.0f}%...\", end=\" \")\n",
    "        \n",
    "        # COM PDS\n",
    "        r_pds = run_benchmark_test(df_bench, missing_type, rate, use_pds=True)\n",
    "        results.append(r_pds)\n",
    "        \n",
    "        # SEM PDS\n",
    "        r_no_pds = run_benchmark_test(df_bench, missing_type, rate, use_pds=False)\n",
    "        results.append(r_no_pds)\n",
    "        \n",
    "        print(f\"PDS={r_pds['mae']:.4f}, NO_PDS={r_no_pds['mae']:.4f}\")\n",
    "\n",
    "# Mostrar resultados\n",
    "print(f\"\\n{'Type':<6} {'Rate':<6} {'PDS':<5} {'Missing':<8} {'Imputed':<8} {'MAE':<10} {'Phase2'}\")\n",
    "print(\"-\" * 70)\n",
    "for r in results:\n",
    "    pds_str = \"ON\" if r['pds'] else \"OFF\"\n",
    "    phase2_str = \"YES\" if r['phase2'] else \"no\"\n",
    "    mae_str = f\"{r['mae']:.4f}\" if pd.notna(r['mae']) else \"N/A\"\n",
    "    print(f\"{r['type']:<6} {r['rate']*100:.0f}%   {pds_str:<5} {r['n_missing']:<8} {r['n_imputed']:<8} {mae_str:<10} {phase2_str}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ANÁLISE: PDS vs SEM PDS\n",
      "======================================================================\n",
      "\n",
      "MCAR:\n",
      "  10%: PDS=0.7922, NO_PDS=0.7265 → NO PDS (0.0657)\n",
      "  20%: PDS=0.9281, NO_PDS=0.8399 → NO PDS (0.0882)\n",
      "  30%: MAE não disponível\n",
      "  40%: MAE não disponível\n",
      "\n",
      "MAR:\n",
      "  10%: PDS=0.7775, NO_PDS=0.7357 → NO PDS (0.0417)\n",
      "  20%: PDS=0.7312, NO_PDS=0.6721 → NO PDS (0.0592)\n",
      "  30%: PDS=0.7349, NO_PDS=0.6861 → NO PDS (0.0489)\n",
      "  40%: PDS=0.8109, NO_PDS=0.7295 → NO PDS (0.0814)\n",
      "\n",
      "MNAR:\n",
      "  10%: PDS=0.8736, NO_PDS=0.7026 → NO PDS (0.1710)\n",
      "  20%: PDS=0.9298, NO_PDS=0.6695 → NO PDS (0.2603)\n",
      "  30%: PDS=1.0193, NO_PDS=0.8765 → NO PDS (0.1428)\n",
      "  40%: MAE não disponível\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"ANÁLISE: PDS vs SEM PDS\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "for missing_type in ['MCAR', 'MAR', 'MNAR']:\n",
    "    print(f\"\\n{missing_type}:\")\n",
    "    for rate in [0.10, 0.20, 0.30, 0.40]:\n",
    "        r_pds_list = [r for r in results if r['type'] == missing_type and r['rate'] == rate and r['pds']]\n",
    "        r_no_list = [r for r in results if r['type'] == missing_type and r['rate'] == rate and not r['pds']]\n",
    "        \n",
    "        if not r_pds_list or not r_no_list:\n",
    "            print(f\"  {rate*100:.0f}%: DADOS EM FALTA\")\n",
    "            continue\n",
    "            \n",
    "        r_pds = r_pds_list[0]\n",
    "        r_no = r_no_list[0]\n",
    "        \n",
    "        if pd.isna(r_pds['mae']) or pd.isna(r_no['mae']):\n",
    "            print(f\"  {rate*100:.0f}%: MAE não disponível\")\n",
    "            continue\n",
    "        \n",
    "        diff = r_pds['mae'] - r_no['mae']\n",
    "        winner = \"PDS\" if diff < 0 else \"NO PDS\" if diff > 0 else \"EQUAL\"\n",
    "        \n",
    "        print(f\"  {rate*100:.0f}%: PDS={r_pds['mae']:.4f}, NO_PDS={r_no['mae']:.4f} → {winner} ({abs(diff):.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ANÁLISE: Degradação com taxa de missing\n",
      "======================================================================\n",
      "\n",
      "MCAR (COM PDS):\n",
      "  10% → 20%: MAE 0.7922 → 0.9281 (+17.2%)\n",
      "\n",
      "MAR (COM PDS):\n",
      "  10% → 20%: MAE 0.7775 → 0.7312 (-5.9%)\n",
      "  20% → 30%: MAE 0.7312 → 0.7349 (+0.5%)\n",
      "  30% → 40%: MAE 0.7349 → 0.8109 (+10.3%)\n",
      "  ✓ Degradação controlada: 4%\n",
      "\n",
      "MNAR (COM PDS):\n",
      "  10% → 20%: MAE 0.8736 → 0.9298 (+6.4%)\n",
      "  20% → 30%: MAE 0.9298 → 1.0193 (+9.6%)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"ANÁLISE: Degradação com taxa de missing\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "for missing_type in ['MCAR', 'MAR', 'MNAR']:\n",
    "    print(f\"\\n{missing_type} (COM PDS):\")\n",
    "    maes = []\n",
    "    for rate in [0.10, 0.20, 0.30, 0.40]:\n",
    "        r_list = [r for r in results if r['type'] == missing_type and r['rate'] == rate and r['pds']]\n",
    "        if r_list and pd.notna(r_list[0]['mae']):\n",
    "            maes.append(r_list[0]['mae'])\n",
    "        else:\n",
    "            maes.append(np.nan)\n",
    "    \n",
    "    rates = [10, 20, 30, 40]\n",
    "    for i in range(len(rates)-1):\n",
    "        if pd.notna(maes[i]) and pd.notna(maes[i+1]) and maes[i] > 0:\n",
    "            pct_increase = (maes[i+1] - maes[i]) / maes[i] * 100\n",
    "            print(f\"  {rates[i]}% → {rates[i+1]}%: MAE {maes[i]:.4f} → {maes[i+1]:.4f} ({pct_increase:+.1f}%)\")\n",
    "    \n",
    "    # Verificar colapso\n",
    "    if pd.notna(maes[0]) and pd.notna(maes[-1]) and maes[0] > 0:\n",
    "        total_increase = (maes[-1] - maes[0]) / maes[0] * 100\n",
    "        if total_increase > 200:\n",
    "            print(f\"  ⚠️  COLAPSO: MAE aumentou {total_increase:.0f}% de 10% para 40%\")\n",
    "        else:\n",
    "            print(f\"  ✓ Degradação controlada: {total_increase:.0f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## PARTE 5: Diagnóstico do MI (Mutual Information)\n",
    "\n",
    "Verificar se o MI está a capturar correctamente as relações entre variáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DIAGNÓSTICO MI: Correlação vs MI\n",
      "======================================================================\n",
      "\n",
      "Correlações de Pearson com Target:\n",
      "  F1: 0.9113\n",
      "  F2: 0.8955\n",
      "  F3: 0.0304\n",
      "  F4: 0.3415\n",
      "\n",
      "Mutual Information com Target:\n",
      "  F1: 0.8953\n",
      "  F2: 0.7635\n",
      "  F3: 0.0000\n",
      "  F4: 0.1284\n",
      "\n",
      "Ordem por |correlação|: ['F1', 'F2', 'F4', 'F3']\n",
      "Ordem por MI: ['F1', 'F2', 'F4', 'F3']\n",
      "\n",
      "✓ MI e correlação concordam\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"DIAGNÓSTICO MI: Correlação vs MI\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Correlações\n",
    "print(f\"\\nCorrelações de Pearson com Target:\")\n",
    "for col in ['F1', 'F2', 'F3', 'F4']:\n",
    "    corr = np.corrcoef(df_bench[col], df_bench['Target'])[0, 1]\n",
    "    print(f\"  {col}: {corr:.4f}\")\n",
    "\n",
    "# Calcular MI\n",
    "handler = MixedDataHandler(df_bench)\n",
    "scaled_data = get_scaled_data(df_bench, handler)\n",
    "\n",
    "mi_matrix = calculate_mi_mixed(\n",
    "    encoded_data=df_bench,\n",
    "    scaled_data=scaled_data,\n",
    "    numeric_cols=handler.numeric_cols,\n",
    "    binary_cols=handler.binary_cols,\n",
    "    nominal_cols=handler.nominal_cols,\n",
    "    ordinal_cols=handler.ordinal_cols\n",
    ")\n",
    "\n",
    "print(f\"\\nMutual Information com Target:\")\n",
    "for col in ['F1', 'F2', 'F3', 'F4']:\n",
    "    mi = mi_matrix.loc[col, 'Target']\n",
    "    print(f\"  {col}: {mi:.4f}\")\n",
    "\n",
    "# Verificar ordem\n",
    "corrs = {col: abs(np.corrcoef(df_bench[col], df_bench['Target'])[0, 1]) for col in ['F1', 'F2', 'F3', 'F4']}\n",
    "mis = {col: mi_matrix.loc[col, 'Target'] for col in ['F1', 'F2', 'F3', 'F4']}\n",
    "\n",
    "corr_order = sorted(corrs.keys(), key=lambda x: -corrs[x])\n",
    "mi_order = sorted(mis.keys(), key=lambda x: -mis[x])\n",
    "\n",
    "print(f\"\\nOrdem por |correlação|: {corr_order}\")\n",
    "print(f\"Ordem por MI: {mi_order}\")\n",
    "\n",
    "if corr_order == mi_order:\n",
    "    print(f\"\\n✓ MI e correlação concordam\")\n",
    "else:\n",
    "    print(f\"\\n⚠️  MI e correlação discordam!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## PARTE 6: Resumo e Conclusões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESUMO DO DIAGNÓSTICO COMPLETO\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "Execute todas as células acima e analise:\n",
    "\n",
    "1. PDS (Partial Distance Strategy)\n",
    "   - Com 1 coluna missing: PDS não actua (scale ~1)\n",
    "   - Com múltiplas colunas missing: PDS aplica scale factors\n",
    "   - Comparar MAE com/sem PDS para ver impacto\n",
    "\n",
    "2. ADAPTIVE K\n",
    "   - Vizinhos consistentes → k alto?\n",
    "   - Vizinhos inconsistentes → k baixo?\n",
    "   - Range de variação é suficiente?\n",
    "\n",
    "3. PHASE 2\n",
    "   - É activada quando há dependências circulares?\n",
    "   - Quantos ciclos são necessários?\n",
    "   - Sobram NaNs no final?\n",
    "\n",
    "4. BENCHMARK (MCAR/MAR/MNAR)\n",
    "   - Qual mecanismo tem pior desempenho?\n",
    "   - Há colapso entre 20% e 40%?\n",
    "   - PDS ajuda ou prejudica?\n",
    "\n",
    "5. MI (Mutual Information)\n",
    "   - Captura correctamente as correlações?\n",
    "   - Ordem de importância está correcta?\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
