{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISCA-k - Diagnóstico Completo\n",
    "\n",
    "Este notebook testa o ISCA-k a 100% para identificar exactamente onde falha e não falha.\n",
    "\n",
    "**Diferenças do notebook anterior:**\n",
    "- Testes com missings em MÚLTIPLAS colunas (para testar PDS de verdade)\n",
    "- Diagnóstico explícito do PDS (scale factors, donors rejeitados)\n",
    "- Comparação directa PDS on vs off\n",
    "- Análise detalhada do Adaptive K\n",
    "- Testes de Phase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from iscak_core import ISCAkCore\n",
    "from core.adaptive_k import adaptive_k_hybrid\n",
    "from core.mi_calculator import calculate_mi_mixed\n",
    "from core.distances import (\n",
    "    weighted_euclidean_batch, \n",
    "    weighted_euclidean_pds,\n",
    "    mixed_distance_pds,\n",
    "    range_normalized_mixed_distance\n",
    ")\n",
    "from preprocessing.type_detection import MixedDataHandler\n",
    "from preprocessing.scaling import get_scaled_data, compute_range_factors\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"Imports OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## PARTE 1: Diagnóstico do PDS\n",
    "\n",
    "O PDS (Partial Distance Strategy) permite usar donors com overlap parcial de features.\n",
    "Quando o sample tem NaN em várias colunas, o PDS aplica um scale_factor para compensar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnose_pds(sample, donors, weights, n_features):\n",
    "    \"\"\"\n",
    "    Diagnóstico detalhado do que o PDS está a fazer.\n",
    "    \n",
    "    Mostra para cada donor:\n",
    "    - Overlap (quantas features em comum)\n",
    "    - Scale factor aplicado\n",
    "    - Distância raw vs distância scaled\n",
    "    - Se foi aceite ou rejeitado\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"DIAGNÓSTICO PDS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Contar features disponíveis no sample\n",
    "    sample_avail = ~np.isnan(sample)\n",
    "    n_sample_avail = sample_avail.sum()\n",
    "    print(f\"\\nSample tem {n_sample_avail}/{n_features} features disponíveis\")\n",
    "    print(f\"Features missing no sample: {np.where(~sample_avail)[0].tolist()}\")\n",
    "    \n",
    "    min_overlap = max(2, n_features // 2)\n",
    "    print(f\"Min overlap requerido: {min_overlap} (50% de {n_features})\")\n",
    "    \n",
    "    print(f\"\\n{'Donor':<8} {'Overlap':<12} {'Scale':<10} {'Dist Raw':<12} {'Dist Scaled':<12} {'Status'}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    results = []\n",
    "    for i, donor in enumerate(donors):\n",
    "        # Calcular overlap\n",
    "        donor_avail = ~np.isnan(donor)\n",
    "        overlap_mask = sample_avail & donor_avail\n",
    "        overlap = overlap_mask.sum()\n",
    "        \n",
    "        # Calcular distância raw (só nas features comuns)\n",
    "        if overlap > 0:\n",
    "            diff_sq = 0.0\n",
    "            weight_sum = 0.0\n",
    "            for j in range(n_features):\n",
    "                if overlap_mask[j]:\n",
    "                    diff = sample[j] - donor[j]\n",
    "                    diff_sq += weights[j] * diff * diff\n",
    "                    weight_sum += weights[j]\n",
    "            dist_raw = np.sqrt(diff_sq) if weight_sum > 0 else np.inf\n",
    "        else:\n",
    "            dist_raw = np.inf\n",
    "        \n",
    "        # Scale factor e distância scaled\n",
    "        if overlap >= min_overlap:\n",
    "            scale_factor = np.sqrt(n_features / overlap)\n",
    "            dist_scaled = dist_raw * scale_factor\n",
    "            status = \"✓ ACEITE\"\n",
    "        else:\n",
    "            scale_factor = np.inf\n",
    "            dist_scaled = np.inf\n",
    "            status = \"✗ REJEITADO (overlap insuficiente)\"\n",
    "        \n",
    "        results.append({\n",
    "            'donor_idx': i,\n",
    "            'overlap': overlap,\n",
    "            'scale': scale_factor,\n",
    "            'dist_raw': dist_raw,\n",
    "            'dist_scaled': dist_scaled,\n",
    "            'accepted': overlap >= min_overlap\n",
    "        })\n",
    "        \n",
    "        scale_str = f\"{scale_factor:.3f}\" if np.isfinite(scale_factor) else \"N/A\"\n",
    "        dist_raw_str = f\"{dist_raw:.4f}\" if np.isfinite(dist_raw) else \"inf\"\n",
    "        dist_scaled_str = f\"{dist_scaled:.4f}\" if np.isfinite(dist_scaled) else \"inf\"\n",
    "        \n",
    "        print(f\"{i:<8} {overlap}/{n_features:<10} {scale_str:<10} {dist_raw_str:<12} {dist_scaled_str:<12} {status}\")\n",
    "    \n",
    "    # Resumo\n",
    "    n_accepted = sum(1 for r in results if r['accepted'])\n",
    "    n_rejected = len(results) - n_accepted\n",
    "    print(f\"\\n--- RESUMO ---\")\n",
    "    print(f\"Donors aceites: {n_accepted}/{len(results)}\")\n",
    "    print(f\"Donors rejeitados: {n_rejected}/{len(results)}\")\n",
    "    \n",
    "    if n_accepted > 0:\n",
    "        accepted = [r for r in results if r['accepted']]\n",
    "        avg_scale = np.mean([r['scale'] for r in accepted])\n",
    "        print(f\"Scale factor médio (aceites): {avg_scale:.3f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste PDS 1: Missings em UMA coluna (PDS não actua)\n",
    "\n",
    "Quando só há missing numa coluna, o overlap é quase total e o scale_factor ≈ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TESTE PDS 1: Missings em UMA coluna\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Dataset simples: 10 features, target na última\n",
    "n_features = 10\n",
    "\n",
    "# Sample: só tem missing no target (coluna 9)\n",
    "sample = np.array([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, np.nan])\n",
    "\n",
    "# Donors: todos completos\n",
    "donors = np.array([\n",
    "    [1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 10.0],  # Próximo\n",
    "    [1.2, 1.2, 1.2, 1.2, 1.2, 1.2, 1.2, 1.2, 1.2, 11.0],  # Próximo\n",
    "    [5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 50.0],  # Distante\n",
    "])\n",
    "\n",
    "weights = np.ones(n_features) / n_features\n",
    "\n",
    "results = diagnose_pds(sample, donors, weights, n_features)\n",
    "\n",
    "print(f\"\\n⚠️  CONCLUSÃO: Com missing em apenas 1 coluna:\")\n",
    "print(f\"   - Overlap = {n_features-1}/{n_features} = {(n_features-1)/n_features*100:.0f}%\")\n",
    "print(f\"   - Scale factor = sqrt({n_features}/{n_features-1}) = {np.sqrt(n_features/(n_features-1)):.3f}\")\n",
    "print(f\"   - O PDS praticamente NÃO ACTUA!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste PDS 2: Missings em MÚLTIPLAS colunas (PDS actua)\n",
    "\n",
    "Quando há missings em várias colunas, o scale_factor aumenta significativamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TESTE PDS 2: Missings em MÚLTIPLAS colunas\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Sample: missings em 4 colunas (cols 6, 7, 8, 9)\n",
    "sample_multi = np.array([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, np.nan, np.nan, np.nan, np.nan])\n",
    "\n",
    "# Donors com diferentes níveis de completude\n",
    "donors_multi = np.array([\n",
    "    [1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 10.0],  # Completo - overlap 6/10\n",
    "    [1.2, 1.2, 1.2, 1.2, 1.2, 1.2, np.nan, 1.2, 1.2, 11.0],  # Missing col 6 - overlap 5/10\n",
    "    [1.3, 1.3, 1.3, 1.3, 1.3, 1.3, np.nan, np.nan, 1.3, 12.0],  # Missing cols 6,7 - overlap 4/10\n",
    "    [5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 50.0],  # Completo mas distante\n",
    "    [1.0, 1.0, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, 10.0],  # overlap 2/10 - REJEITADO\n",
    "])\n",
    "\n",
    "results = diagnose_pds(sample_multi, donors_multi, weights, n_features)\n",
    "\n",
    "print(f\"\\n✓ CONCLUSÃO: Com missings em MÚLTIPLAS colunas:\")\n",
    "print(f\"   - O PDS aplica scale factors significativos\")\n",
    "print(f\"   - Donors com pouco overlap são REJEITADOS\")\n",
    "print(f\"   - Isto muda completamente a selecção de vizinhos!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste PDS 3: Impacto do PDS na qualidade da imputação\n",
    "\n",
    "Comparar resultados COM e SEM PDS num dataset com múltiplos missings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TESTE PDS 3: Impacto na qualidade (COM vs SEM PDS)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Criar dataset com 2 clusters bem separados\n",
    "# Cluster 1: features ~1, targets ~10\n",
    "# Cluster 2: features ~5, targets ~50\n",
    "\n",
    "def make_cluster(center, target_center, n_samples, n_features):\n",
    "    data = np.random.normal(center, 0.2, (n_samples, n_features))\n",
    "    targets = np.random.normal(target_center, 1.0, n_samples)\n",
    "    return np.column_stack([data, targets])\n",
    "\n",
    "cluster1 = make_cluster(1.0, 10.0, 30, 8)\n",
    "cluster2 = make_cluster(5.0, 50.0, 30, 8)\n",
    "data_full = np.vstack([cluster1, cluster2])\n",
    "\n",
    "# Criar DataFrame\n",
    "cols = [f'F{i}' for i in range(8)] + ['Target']\n",
    "df_complete = pd.DataFrame(data_full, columns=cols)\n",
    "\n",
    "print(f\"Dataset: {df_complete.shape[0]} linhas x {df_complete.shape[1]} colunas\")\n",
    "print(f\"Cluster 1 (linhas 0-29): features ~1, target ~10\")\n",
    "print(f\"Cluster 2 (linhas 30-59): features ~5, target ~50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduzir missings em MÚLTIPLAS colunas (padrão realista)\n",
    "# 20% de missings distribuídos aleatoriamente\n",
    "\n",
    "np.random.seed(42)\n",
    "df_missing = df_complete.copy()\n",
    "\n",
    "n_total_values = df_missing.shape[0] * df_missing.shape[1]\n",
    "n_missing = int(0.20 * n_total_values)  # 20% missing\n",
    "\n",
    "# Seleccionar posições aleatórias para missings\n",
    "missing_positions = []\n",
    "rows = np.random.choice(60, n_missing, replace=True)\n",
    "cols_idx = np.random.choice(9, n_missing, replace=True)\n",
    "\n",
    "true_values = {}\n",
    "for r, c in zip(rows, cols_idx):\n",
    "    col_name = cols[c]\n",
    "    if (r, col_name) not in true_values:  # Evitar duplicados\n",
    "        true_values[(r, col_name)] = df_complete.loc[r, col_name]\n",
    "        df_missing.loc[r, col_name] = np.nan\n",
    "\n",
    "# Contar missings por coluna\n",
    "print(f\"\\nMissings introduzidos: {len(true_values)}\")\n",
    "print(f\"\\nMissings por coluna:\")\n",
    "for col in cols:\n",
    "    n_miss = df_missing[col].isna().sum()\n",
    "    print(f\"  {col}: {n_miss} ({n_miss/60*100:.1f}%)\")\n",
    "\n",
    "# Missings por linha (para ver padrão)\n",
    "missings_per_row = df_missing.isna().sum(axis=1)\n",
    "print(f\"\\nMissings por linha: min={missings_per_row.min()}, max={missings_per_row.max()}, média={missings_per_row.mean():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executar COM PDS\n",
    "print(f\"\\n{'='*30} COM PDS {'='*30}\")\n",
    "imputer_pds = ISCAkCore(verbose=True, use_pds=True, min_friends=3, max_friends=10)\n",
    "result_pds = imputer_pds.impute(df_missing.copy(), interactive=False)\n",
    "\n",
    "# Calcular erros\n",
    "errors_pds = []\n",
    "for (r, col), true_val in true_values.items():\n",
    "    imputed = result_pds.loc[r, col]\n",
    "    if not np.isnan(imputed):\n",
    "        errors_pds.append(abs(imputed - true_val))\n",
    "\n",
    "mae_pds = np.mean(errors_pds) if errors_pds else np.nan\n",
    "print(f\"\\nMAE COM PDS: {mae_pds:.4f}\")\n",
    "print(f\"Fase 2 activada: {imputer_pds.execution_stats.get('phase2_activated', False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executar SEM PDS\n",
    "print(f\"\\n{'='*30} SEM PDS {'='*30}\")\n",
    "imputer_no_pds = ISCAkCore(verbose=True, use_pds=False, min_friends=3, max_friends=10)\n",
    "result_no_pds = imputer_no_pds.impute(df_missing.copy(), interactive=False)\n",
    "\n",
    "# Calcular erros\n",
    "errors_no_pds = []\n",
    "for (r, col), true_val in true_values.items():\n",
    "    imputed = result_no_pds.loc[r, col]\n",
    "    if not np.isnan(imputed):\n",
    "        errors_no_pds.append(abs(imputed - true_val))\n",
    "\n",
    "mae_no_pds = np.mean(errors_no_pds) if errors_no_pds else np.nan\n",
    "print(f\"\\nMAE SEM PDS: {mae_no_pds:.4f}\")\n",
    "print(f\"Fase 2 activada: {imputer_no_pds.execution_stats.get('phase2_activated', False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparação\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"COMPARAÇÃO PDS vs SEM PDS\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nMAE COM PDS:  {mae_pds:.4f}\")\n",
    "print(f\"MAE SEM PDS:  {mae_no_pds:.4f}\")\n",
    "\n",
    "if mae_pds < mae_no_pds:\n",
    "    diff = (mae_no_pds - mae_pds) / mae_no_pds * 100\n",
    "    print(f\"\\n✓ PDS é MELHOR por {diff:.1f}%\")\n",
    "elif mae_no_pds < mae_pds:\n",
    "    diff = (mae_pds - mae_no_pds) / mae_pds * 100\n",
    "    print(f\"\\n⚠️  SEM PDS é MELHOR por {diff:.1f}%\")\n",
    "else:\n",
    "    print(f\"\\n= Resultados iguais\")\n",
    "\n",
    "# Valores não imputados\n",
    "n_nan_pds = result_pds.isna().sum().sum()\n",
    "n_nan_no_pds = result_no_pds.isna().sum().sum()\n",
    "print(f\"\\nValores NÃO imputados:\")\n",
    "print(f\"  COM PDS: {n_nan_pds}\")\n",
    "print(f\"  SEM PDS: {n_nan_no_pds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## PARTE 2: Diagnóstico do Adaptive K\n",
    "\n",
    "O Adaptive K escolhe quantos vizinhos usar baseado em:\n",
    "- density_trust: quão próximos estão os vizinhos\n",
    "- consistency_trust: quão concordantes são os targets dos vizinhos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnose_adaptive_k(distances, values, min_k=3, max_k=15, alpha=0.5, is_categorical=False):\n",
    "    \"\"\"\n",
    "    Diagnóstico detalhado do Adaptive K.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"DIAGNÓSTICO ADAPTIVE K\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    print(f\"\\nParâmetros: min_k={min_k}, max_k={max_k}, alpha={alpha}\")\n",
    "    print(f\"Tipo: {'Categórico' if is_categorical else 'Numérico'}\")\n",
    "    \n",
    "    # Usar max_k vizinhos para avaliar\n",
    "    k_eval = min(max_k, len(distances))\n",
    "    sorted_idx = np.argsort(distances)\n",
    "    closest_dist = distances[sorted_idx[:k_eval]]\n",
    "    closest_vals = values[sorted_idx[:k_eval]]\n",
    "    \n",
    "    print(f\"\\nTop {k_eval} vizinhos (ordenados por distância):\")\n",
    "    for i in range(min(10, k_eval)):  # Mostrar top 10\n",
    "        print(f\"  {i+1}. dist={closest_dist[i]:.4f}, target={closest_vals[i]:.2f}\")\n",
    "    \n",
    "    # Calcular density_trust\n",
    "    mean_dist = np.mean(closest_dist[np.isfinite(closest_dist)])\n",
    "    density_trust = 1.0 / (1.0 + mean_dist) if np.isfinite(mean_dist) else 0.5\n",
    "    \n",
    "    print(f\"\\n--- DENSITY TRUST ---\")\n",
    "    print(f\"  Distância média: {mean_dist:.4f}\")\n",
    "    print(f\"  density_trust = 1/(1+{mean_dist:.4f}) = {density_trust:.4f}\")\n",
    "    \n",
    "    # Calcular consistency_trust\n",
    "    if is_categorical:\n",
    "        unique, counts = np.unique(closest_vals, return_counts=True)\n",
    "        consistency_trust = counts.max() / len(closest_vals) if len(counts) > 0 else 0.5\n",
    "        print(f\"\\n--- CONSISTENCY TRUST (Categórico) ---\")\n",
    "        print(f\"  Classes: {dict(zip(unique, counts))}\")\n",
    "        print(f\"  Classe dominante: {unique[np.argmax(counts)]} ({counts.max()}/{len(closest_vals)})\")\n",
    "        print(f\"  consistency_trust = {consistency_trust:.4f}\")\n",
    "    else:\n",
    "        mean_val = np.mean(closest_vals)\n",
    "        std_val = np.std(closest_vals)\n",
    "        cv = std_val / abs(mean_val) if abs(mean_val) > 1e-10 else std_val\n",
    "        consistency_trust = 1.0 / (1.0 + cv)\n",
    "        print(f\"\\n--- CONSISTENCY TRUST (Numérico) ---\")\n",
    "        print(f\"  Média targets: {mean_val:.4f}\")\n",
    "        print(f\"  Std targets: {std_val:.4f}\")\n",
    "        print(f\"  CV (coef. variação) = {std_val:.4f}/{abs(mean_val):.4f} = {cv:.4f}\")\n",
    "        print(f\"  consistency_trust = 1/(1+{cv:.4f}) = {consistency_trust:.4f}\")\n",
    "    \n",
    "    # Calcular trust final e k\n",
    "    trust = alpha * density_trust + (1 - alpha) * consistency_trust\n",
    "    k = int(round(min_k + (max_k - min_k) * trust))\n",
    "    k = max(min_k, min(max_k, k))\n",
    "    \n",
    "    print(f\"\\n--- K FINAL ---\")\n",
    "    print(f\"  trust = {alpha}*{density_trust:.4f} + {1-alpha}*{consistency_trust:.4f} = {trust:.4f}\")\n",
    "    print(f\"  k = {min_k} + ({max_k}-{min_k})*{trust:.4f} = {min_k + (max_k - min_k) * trust:.2f}\")\n",
    "    print(f\"  k final (arredondado): {k}\")\n",
    "    \n",
    "    # Análise de sensibilidade\n",
    "    print(f\"\\n--- ANÁLISE DE SENSIBILIDADE ---\")\n",
    "    print(f\"  Se trust=0.0 → k={min_k}\")\n",
    "    print(f\"  Se trust=0.5 → k={min_k + (max_k - min_k) * 0.5:.0f}\")\n",
    "    print(f\"  Se trust=1.0 → k={max_k}\")\n",
    "    print(f\"  Trust actual: {trust:.4f} → k={k}\")\n",
    "    \n",
    "    return k, trust, density_trust, consistency_trust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TESTE ADAPTIVE K 1: Vizinhos CONSISTENTES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Vizinhos próximos e com targets similares\n",
    "distances_consistent = np.array([0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "values_consistent = np.array([10.0, 10.1, 10.2, 9.9, 10.0, 10.1, 9.8, 10.2, 10.0, 10.1, 9.9, 10.0, 10.1, 10.2, 10.0])\n",
    "\n",
    "k1, trust1, dens1, cons1 = diagnose_adaptive_k(distances_consistent, values_consistent)\n",
    "\n",
    "print(f\"\\nESPERADO: k ALTO porque vizinhos concordam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TESTE ADAPTIVE K 2: Vizinhos INCONSISTENTES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Vizinhos próximos mas com targets muito diferentes\n",
    "distances_inconsistent = np.array([0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "values_inconsistent = np.array([10.0, 50.0, 5.0, 100.0, 20.0, 80.0, 15.0, 90.0, 30.0, 70.0, 25.0, 60.0, 35.0, 55.0, 40.0])\n",
    "\n",
    "k2, trust2, dens2, cons2 = diagnose_adaptive_k(distances_inconsistent, values_inconsistent)\n",
    "\n",
    "print(f\"\\nESPERADO: k BAIXO porque vizinhos discordam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TESTE ADAPTIVE K 3: Vizinhos DISTANTES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Vizinhos distantes mas consistentes\n",
    "distances_far = np.array([2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6.0, 6.5, 7.0, 7.5, 8.0, 8.5, 9.0])\n",
    "values_far = np.array([10.0, 10.1, 10.2, 9.9, 10.0, 10.1, 9.8, 10.2, 10.0, 10.1, 9.9, 10.0, 10.1, 10.2, 10.0])\n",
    "\n",
    "k3, trust3, dens3, cons3 = diagnose_adaptive_k(distances_far, values_far)\n",
    "\n",
    "print(f\"\\nESPERADO: k BAIXO porque vizinhos estão longe (density_trust baixo)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparação\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"COMPARAÇÃO ADAPTIVE K\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\n{'Cenário':<25} {'Density':<12} {'Consistency':<12} {'Trust':<10} {'K'}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Consistente (próximo)':<25} {dens1:<12.4f} {cons1:<12.4f} {trust1:<10.4f} {k1}\")\n",
    "print(f\"{'Inconsistente (próximo)':<25} {dens2:<12.4f} {cons2:<12.4f} {trust2:<10.4f} {k2}\")\n",
    "print(f\"{'Consistente (distante)':<25} {dens3:<12.4f} {cons3:<12.4f} {trust3:<10.4f} {k3}\")\n",
    "\n",
    "print(f\"\\n--- ANÁLISE ---\")\n",
    "if k1 > k2:\n",
    "    print(f\"✓ k_consistente ({k1}) > k_inconsistente ({k2}) - CORRECTO\")\n",
    "else:\n",
    "    print(f\"⚠️  k_consistente ({k1}) <= k_inconsistente ({k2}) - PROBLEMA!\")\n",
    "\n",
    "print(f\"\\nVariação total de k: {min(k1,k2,k3)} a {max(k1,k2,k3)}\")\n",
    "if max(k1,k2,k3) - min(k1,k2,k3) < 3:\n",
    "    print(f\"⚠️  K varia pouco! A fórmula pode precisar de ajuste.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## PARTE 3: Diagnóstico da Phase 2\n",
    "\n",
    "A Phase 2 é activada quando a Phase 1 não consegue imputar todos os valores.\n",
    "Isto acontece quando há dependências circulares entre colunas com missings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TESTE PHASE 2: Dependências circulares\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Dataset onde cada linha tem missings em posições diferentes\n",
    "# que criam dependência circular\n",
    "\n",
    "# Base: linhas completas para referência\n",
    "base_data = pd.DataFrame({\n",
    "    'A': [1.0, 1.1, 1.2, 1.3, 1.4],\n",
    "    'B': [2.0, 2.1, 2.2, 2.3, 2.4],\n",
    "    'C': [3.0, 3.1, 3.2, 3.3, 3.4],\n",
    "    'D': [4.0, 4.1, 4.2, 4.3, 4.4],\n",
    "})\n",
    "\n",
    "# Linhas com missings em padrão que força Phase 2\n",
    "# Linha 5: missing em A, tem B,C,D\n",
    "# Linha 6: missing em B, tem A,C,D\n",
    "# Linha 7: missing em C, tem A,B,D\n",
    "# Linha 8: missing em D, tem A,B,C\n",
    "# Para imputar A da linha 5, preciso de donors com A preenchido\n",
    "# Mas se todos os donors também têm missings...\n",
    "\n",
    "problem_data = pd.DataFrame({\n",
    "    'A': [np.nan, 1.6, 1.7, 1.8],\n",
    "    'B': [2.5, np.nan, 2.7, 2.8],\n",
    "    'C': [3.5, 3.6, np.nan, 3.8],\n",
    "    'D': [4.5, 4.6, 4.7, np.nan],\n",
    "})\n",
    "\n",
    "df_phase2 = pd.concat([base_data, problem_data], ignore_index=True)\n",
    "\n",
    "print(f\"\\nDataset:\")\n",
    "print(df_phase2.to_string())\n",
    "print(f\"\\nLinhas 0-4: Completas (donors)\")\n",
    "print(f\"Linhas 5-8: Cada uma tem 1 missing diferente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executar e ver se Phase 2 activa\n",
    "imputer_p2 = ISCAkCore(verbose=True, use_pds=True, min_friends=2)\n",
    "result_p2 = imputer_p2.impute(df_phase2.copy(), interactive=False)\n",
    "\n",
    "print(f\"\\n--- RESULTADO ---\")\n",
    "print(result_p2.to_string())\n",
    "\n",
    "print(f\"\\n--- ESTATÍSTICAS ---\")\n",
    "stats = imputer_p2.execution_stats\n",
    "print(f\"Phase 2 activada: {stats.get('phase2_activated', 'N/A')}\")\n",
    "print(f\"Phase 2 ciclos: {stats.get('phase2_cycles', 'N/A')}\")\n",
    "print(f\"Phase 2 imputados: {stats.get('phase2_imputed', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TESTE PHASE 2 EXTREMO: Todos com múltiplos missings\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Dataset mais extremo: poucas linhas completas, maioria com múltiplos missings\n",
    "df_extreme = pd.DataFrame({\n",
    "    'A': [1.0, 1.1, np.nan, np.nan, np.nan, np.nan, 1.0, 1.1],\n",
    "    'B': [2.0, 2.1, 2.2, np.nan, np.nan, 2.2, np.nan, np.nan],\n",
    "    'C': [3.0, 3.1, np.nan, 3.3, np.nan, np.nan, 3.0, np.nan],\n",
    "    'D': [4.0, 4.1, np.nan, np.nan, 4.4, np.nan, np.nan, 4.1],\n",
    "})\n",
    "\n",
    "print(f\"\\nDataset extremo:\")\n",
    "print(df_extreme.to_string())\n",
    "print(f\"\\nMissings por linha: {df_extreme.isna().sum(axis=1).tolist()}\")\n",
    "print(f\"Linhas completas: {(~df_extreme.isna().any(axis=1)).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer_extreme = ISCAkCore(verbose=True, use_pds=True, min_friends=2)\n",
    "result_extreme = imputer_extreme.impute(df_extreme.copy(), interactive=False)\n",
    "\n",
    "print(f\"\\n--- RESULTADO ---\")\n",
    "print(result_extreme.to_string())\n",
    "\n",
    "print(f\"\\n--- ESTATÍSTICAS ---\")\n",
    "stats = imputer_extreme.execution_stats\n",
    "print(f\"Phase 2 activada: {stats.get('phase2_activated', 'N/A')}\")\n",
    "print(f\"Phase 2 ciclos: {stats.get('phase2_cycles', 'N/A')}\")\n",
    "print(f\"Phase 2 imputados: {stats.get('phase2_imputed', 'N/A')}\")\n",
    "\n",
    "# Verificar se sobrou algum NaN\n",
    "n_remaining_nan = result_extreme.isna().sum().sum()\n",
    "print(f\"\\nNaN restantes: {n_remaining_nan}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## PARTE 4: Teste Integrado - Simular Benchmark Real\n",
    "\n",
    "Testar com condições similares aos benchmarks (MCAR, MAR, MNAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def introduce_mcar(df, rate=0.2, seed=42):\n",
    "    \"\"\"Introduz missings MCAR (completamente aleatórios)\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    df_missing = df.copy()\n",
    "    mask = np.random.random(df.shape) < rate\n",
    "    df_missing = df_missing.mask(mask)\n",
    "    return df_missing\n",
    "\n",
    "def introduce_mar(df, rate=0.2, seed=42):\n",
    "    \"\"\"Introduz missings MAR (dependentes de outras colunas)\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    df_missing = df.copy()\n",
    "    \n",
    "    # Escolher coluna causativa (numérica)\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if len(numeric_cols) < 2:\n",
    "        return introduce_mcar(df, rate, seed)\n",
    "    \n",
    "    cause_col = numeric_cols[0]\n",
    "    affect_cols = numeric_cols[1:]\n",
    "    \n",
    "    # Missings mais prováveis onde cause_col é alto\n",
    "    cause_vals = df[cause_col].values\n",
    "    prob = (cause_vals - cause_vals.min()) / (cause_vals.max() - cause_vals.min() + 1e-10)\n",
    "    prob = prob * rate * 2  # Escalar para taxa desejada\n",
    "    \n",
    "    for col in affect_cols:\n",
    "        mask = np.random.random(len(df)) < prob\n",
    "        df_missing.loc[mask, col] = np.nan\n",
    "    \n",
    "    return df_missing\n",
    "\n",
    "def introduce_mnar(df, rate=0.2, seed=42):\n",
    "    \"\"\"Introduz missings MNAR (dependentes do próprio valor)\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    df_missing = df.copy()\n",
    "    \n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        vals = df[col].values\n",
    "        # Missings mais prováveis para valores altos\n",
    "        prob = (vals - vals.min()) / (vals.max() - vals.min() + 1e-10)\n",
    "        prob = prob * rate * 2\n",
    "        mask = np.random.random(len(df)) < prob\n",
    "        df_missing.loc[mask, col] = np.nan\n",
    "    \n",
    "    return df_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TESTE INTEGRADO: Dataset tipo benchmark\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Criar dataset com estrutura conhecida\n",
    "np.random.seed(42)\n",
    "n = 150\n",
    "\n",
    "# Features correlacionadas\n",
    "F1 = np.random.normal(0, 1, n)\n",
    "F2 = F1 * 0.8 + np.random.normal(0, 0.5, n)  # Correlação forte com F1\n",
    "F3 = np.random.normal(0, 1, n)  # Independente\n",
    "F4 = F1 * 0.3 + F3 * 0.3 + np.random.normal(0, 0.7, n)  # Correlação média\n",
    "Target = F1 * 2 + F2 * 1.5 + F4 * 0.5 + np.random.normal(0, 1, n)  # Target depende de F1, F2, F4\n",
    "\n",
    "df_bench = pd.DataFrame({\n",
    "    'F1': F1, 'F2': F2, 'F3': F3, 'F4': F4, 'Target': Target\n",
    "})\n",
    "\n",
    "print(f\"Dataset: {df_bench.shape}\")\n",
    "print(f\"\\nCorrelações com Target:\")\n",
    "for col in ['F1', 'F2', 'F3', 'F4']:\n",
    "    corr = np.corrcoef(df_bench[col], df_bench['Target'])[0, 1]\n",
    "    print(f\"  {col}: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark_test(df_complete, missing_type, rate, use_pds=True):\n",
    "    \"\"\"Executa um teste de benchmark\"\"\"\n",
    "    # Introduzir missings\n",
    "    if missing_type == 'MCAR':\n",
    "        df_missing = introduce_mcar(df_complete, rate)\n",
    "    elif missing_type == 'MAR':\n",
    "        df_missing = introduce_mar(df_complete, rate)\n",
    "    else:\n",
    "        df_missing = introduce_mnar(df_complete, rate)\n",
    "    \n",
    "    # Guardar ground truth\n",
    "    missing_mask = df_missing.isna()\n",
    "    \n",
    "    # Imputar\n",
    "    imputer = ISCAkCore(verbose=False, use_pds=use_pds, min_friends=3, max_friends=10)\n",
    "    result = imputer.impute(df_missing.copy(), interactive=False)\n",
    "    \n",
    "    # Calcular métricas\n",
    "    errors = []\n",
    "    for col in df_complete.columns:\n",
    "        col_mask = missing_mask[col]\n",
    "        if col_mask.sum() > 0:\n",
    "            true_vals = df_complete.loc[col_mask, col].values\n",
    "            imputed_vals = result.loc[col_mask, col].values\n",
    "            valid = ~np.isnan(imputed_vals)\n",
    "            if valid.sum() > 0:\n",
    "                col_errors = np.abs(true_vals[valid] - imputed_vals[valid])\n",
    "                errors.extend(col_errors)\n",
    "    \n",
    "    mae = np.mean(errors) if errors else np.nan\n",
    "    n_missing = missing_mask.sum().sum()\n",
    "    n_imputed = len(errors)\n",
    "    \n",
    "    return {\n",
    "        'type': missing_type,\n",
    "        'rate': rate,\n",
    "        'pds': use_pds,\n",
    "        'n_missing': n_missing,\n",
    "        'n_imputed': n_imputed,\n",
    "        'mae': mae,\n",
    "        'phase2': imputer.execution_stats.get('phase2_activated', False)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"BENCHMARK: MCAR, MAR, MNAR x 10%, 20%, 30%, 40%\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results = []\n",
    "\n",
    "for missing_type in ['MCAR', 'MAR', 'MNAR']:\n",
    "    for rate in [0.10, 0.20, 0.30, 0.40]:\n",
    "        # COM PDS\n",
    "        r_pds = run_benchmark_test(df_bench, missing_type, rate, use_pds=True)\n",
    "        results.append(r_pds)\n",
    "        \n",
    "        # SEM PDS\n",
    "        r_no_pds = run_benchmark_test(df_bench, missing_type, rate, use_pds=False)\n",
    "        results.append(r_no_pds)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(f\"\\n{'Type':<6} {'Rate':<6} {'PDS':<5} {'Missing':<8} {'Imputed':<8} {'MAE':<10} {'Phase2'}\")\n",
    "print(\"-\" * 70)\n",
    "for r in results:\n",
    "    pds_str = \"ON\" if r['pds'] else \"OFF\"\n",
    "    phase2_str = \"YES\" if r['phase2'] else \"no\"\n",
    "    print(f\"{r['type']:<6} {r['rate']*100:.0f}%   {pds_str:<5} {r['n_missing']:<8} {r['n_imputed']:<8} {r['mae']:<10.4f} {phase2_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise comparativa PDS vs NO PDS\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"ANÁLISE: PDS vs SEM PDS\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "for missing_type in ['MCAR', 'MAR', 'MNAR']:\n",
    "    print(f\"\\n{missing_type}:\")\n",
    "    for rate in [0.10, 0.20, 0.30, 0.40]:\n",
    "        r_pds = [r for r in results if r['type'] == missing_type and r['rate'] == rate and r['pds']][0]\n",
    "        r_no = [r for r in results if r['type'] == missing_type and r['rate'] == rate and not r['pds']][0]\n",
    "        \n",
    "        diff = r_pds['mae'] - r_no['mae']\n",
    "        winner = \"PDS\" if diff < 0 else \"NO PDS\" if diff > 0 else \"EQUAL\"\n",
    "        \n",
    "        print(f\"  {rate*100:.0f}%: PDS={r_pds['mae']:.4f}, NO_PDS={r_no['mae']:.4f} → {winner} ({abs(diff):.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise de degradação com taxa de missing\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"ANÁLISE: Degradação com taxa de missing\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "for missing_type in ['MCAR', 'MAR', 'MNAR']:\n",
    "    print(f\"\\n{missing_type} (COM PDS):\")\n",
    "    maes = [r['mae'] for r in results if r['type'] == missing_type and r['pds']]\n",
    "    rates = [10, 20, 30, 40]\n",
    "    \n",
    "    for i in range(len(rates)-1):\n",
    "        if maes[i] > 0:\n",
    "            pct_increase = (maes[i+1] - maes[i]) / maes[i] * 100\n",
    "            print(f\"  {rates[i]}% → {rates[i+1]}%: MAE {maes[i]:.4f} → {maes[i+1]:.4f} ({pct_increase:+.1f}%)\")\n",
    "    \n",
    "    # Verificar colapso (aumento > 100%)\n",
    "    if len(maes) >= 2 and maes[0] > 0:\n",
    "        total_increase = (maes[-1] - maes[0]) / maes[0] * 100\n",
    "        if total_increase > 200:\n",
    "            print(f\"  ⚠️  COLAPSO: MAE aumentou {total_increase:.0f}% de 10% para 40%\")\n",
    "        else:\n",
    "            print(f\"  ✓ Degradação controlada: {total_increase:.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## PARTE 5: Diagnóstico do MI (Mutual Information)\n",
    "\n",
    "Verificar se o MI está a capturar correctamente as relações entre variáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"DIAGNÓSTICO MI: Correlação vs MI\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Usar o df_bench que tem correlações conhecidas\n",
    "print(f\"\\nCorrelações de Pearson com Target:\")\n",
    "for col in ['F1', 'F2', 'F3', 'F4']:\n",
    "    corr = np.corrcoef(df_bench[col], df_bench['Target'])[0, 1]\n",
    "    print(f\"  {col}: {corr:.4f}\")\n",
    "\n",
    "# Calcular MI\n",
    "handler = MixedDataHandler(df_bench)\n",
    "mi_matrix = calculate_mi_mixed(df_bench, handler)\n",
    "\n",
    "print(f\"\\nMutual Information com Target:\")\n",
    "for col in ['F1', 'F2', 'F3', 'F4']:\n",
    "    mi = mi_matrix.loc[col, 'Target']\n",
    "    print(f\"  {col}: {mi:.4f}\")\n",
    "\n",
    "# Verificar se ordem é consistente\n",
    "print(f\"\\n--- VERIFICAÇÃO ---\")\n",
    "corrs = {col: abs(np.corrcoef(df_bench[col], df_bench['Target'])[0, 1]) for col in ['F1', 'F2', 'F3', 'F4']}\n",
    "mis = {col: mi_matrix.loc[col, 'Target'] for col in ['F1', 'F2', 'F3', 'F4']}\n",
    "\n",
    "corr_order = sorted(corrs.keys(), key=lambda x: -corrs[x])\n",
    "mi_order = sorted(mis.keys(), key=lambda x: -mis[x])\n",
    "\n",
    "print(f\"Ordem por |correlação|: {corr_order}\")\n",
    "print(f\"Ordem por MI: {mi_order}\")\n",
    "\n",
    "if corr_order == mi_order:\n",
    "    print(f\"\\n✓ MI e correlação concordam na ordem de importância\")\n",
    "else:\n",
    "    print(f\"\\n⚠️  MI e correlação discordam na ordem!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## PARTE 6: Resumo e Conclusões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESUMO DO DIAGNÓSTICO COMPLETO\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "Execute todas as células acima e analise:\n",
    "\n",
    "1. PDS (Partial Distance Strategy)\n",
    "   - Com 1 coluna missing: PDS não actua (scale ~1)\n",
    "   - Com múltiplas colunas missing: PDS aplica scale factors\n",
    "   - Comparar MAE com/sem PDS para ver impacto\n",
    "\n",
    "2. ADAPTIVE K\n",
    "   - Vizinhos consistentes → k alto?\n",
    "   - Vizinhos inconsistentes → k baixo?\n",
    "   - Range de variação é suficiente?\n",
    "\n",
    "3. PHASE 2\n",
    "   - É activada quando há dependências circulares?\n",
    "   - Quantos ciclos são necessários?\n",
    "   - Sobram NaNs no final?\n",
    "\n",
    "4. BENCHMARK (MCAR/MAR/MNAR)\n",
    "   - Qual mecanismo tem pior desempenho?\n",
    "   - Há colapso entre 20% e 40%?\n",
    "   - PDS ajuda ou prejudica?\n",
    "\n",
    "5. MI (Mutual Information)\n",
    "   - Captura correctamente as correlações?\n",
    "   - Ordem de importância está correcta?\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}