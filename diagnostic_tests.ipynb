{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISCA-k Diagnostic Tests\n",
    "\n",
    "Este notebook contém testes diagnósticos controlados para verificar o comportamento interno do ISCA-k.\n",
    "\n",
    "**Objectivo**: Criar cenários sintéticos onde sabemos exactamente o que deveria acontecer e verificar se o algoritmo se comporta como esperado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Imports do ISCA-k\n",
    "from iscak_core import ISCAkCore\n",
    "from core.adaptive_k import adaptive_k_hybrid\n",
    "from core.mi_calculator import calculate_mi_mixed\n",
    "from core.distances import (\n",
    "    weighted_euclidean_batch, \n",
    "    mixed_distance_pds,\n",
    "    range_normalized_mixed_distance\n",
    ")\n",
    "from preprocessing.type_detection import MixedTypeHandler\n",
    "from preprocessing.scaling import get_scaled_data, compute_range_factors\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"Imports OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Funções Auxiliares de Diagnóstico\n",
    "\n",
    "Funções para ver \"debaixo do capot\" o que o ISCA-k está a fazer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnose_imputation(imputer, data, target_col, row_idx, true_value=None):\n",
    "    \"\"\"\n",
    "    Diagnóstico detalhado de uma imputação específica.\n",
    "    \n",
    "    Mostra: pesos MI, candidatos a donor, distâncias, k escolhido, vizinhos finais.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"DIAGNÓSTICO: Linha {row_idx}, Coluna '{target_col}'\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Dados da linha a imputar\n",
    "    row_data = data.loc[row_idx]\n",
    "    print(f\"\\nDados da linha {row_idx}:\")\n",
    "    print(row_data.to_string())\n",
    "    \n",
    "    if true_value is not None:\n",
    "        print(f\"\\nValor REAL (ground truth): {true_value}\")\n",
    "    \n",
    "    # Features disponíveis (não-missing)\n",
    "    feature_cols = [c for c in data.columns if c != target_col]\n",
    "    avail_features = [c for c in feature_cols if not pd.isna(row_data[c])]\n",
    "    missing_features = [c for c in feature_cols if pd.isna(row_data[c])]\n",
    "    \n",
    "    print(f\"\\nFeatures disponíveis ({len(avail_features)}): {avail_features}\")\n",
    "    print(f\"Features em falta ({len(missing_features)}): {missing_features}\")\n",
    "    \n",
    "    # Pesos MI\n",
    "    if imputer.mi_matrix is not None:\n",
    "        print(f\"\\n--- PESOS MI para prever '{target_col}' ---\")\n",
    "        mi_scores = imputer.mi_matrix.loc[feature_cols, target_col]\n",
    "        weights = mi_scores.values / mi_scores.sum() if mi_scores.sum() > 0 else np.ones(len(mi_scores)) / len(mi_scores)\n",
    "        \n",
    "        for col, mi, w in sorted(zip(feature_cols, mi_scores.values, weights), key=lambda x: -x[2]):\n",
    "            avail_marker = \"✓\" if col in avail_features else \"✗\"\n",
    "            print(f\"  {avail_marker} {col}: MI={mi:.4f}, peso={w:.4f}\")\n",
    "    \n",
    "    # Donors potenciais\n",
    "    complete_mask = ~data[target_col].isna()\n",
    "    donors = data[complete_mask]\n",
    "    print(f\"\\n--- DONORS POTENCIAIS ({len(donors)} linhas com target preenchido) ---\")\n",
    "    \n",
    "    # Calcular distâncias manualmente para diagnóstico\n",
    "    scaled_data = imputer._get_scaled_data(data)\n",
    "    range_factors = imputer._compute_range_factors(data, scaled_data)\n",
    "    \n",
    "    print(f\"\\n--- RANGE FACTORS ---\")\n",
    "    for i, col in enumerate(data.columns):\n",
    "        if col != target_col:\n",
    "            print(f\"  {col}: {range_factors[i]:.4f}\")\n",
    "    \n",
    "    # Calcular distância para cada donor\n",
    "    distances_info = []\n",
    "    sample_scaled = scaled_data.loc[row_idx, feature_cols].values.astype(np.float64)\n",
    "    \n",
    "    for donor_idx in donors.index:\n",
    "        donor_scaled = scaled_data.loc[donor_idx, feature_cols].values.astype(np.float64)\n",
    "        donor_target = donors.loc[donor_idx, target_col]\n",
    "        \n",
    "        # Contar overlap\n",
    "        overlap = sum(1 for s, d in zip(sample_scaled, donor_scaled) if not (np.isnan(s) or np.isnan(d)))\n",
    "        \n",
    "        # Calcular distância simples (euclidiana ponderada)\n",
    "        dist = 0.0\n",
    "        n_used = 0\n",
    "        for j, (s, d, w) in enumerate(zip(sample_scaled, donor_scaled, weights)):\n",
    "            if not (np.isnan(s) or np.isnan(d)):\n",
    "                diff = abs(s - d)\n",
    "                dist += (diff * w) ** 2\n",
    "                n_used += 1\n",
    "        dist = np.sqrt(dist) if n_used > 0 else np.inf\n",
    "        \n",
    "        distances_info.append({\n",
    "            'donor_idx': donor_idx,\n",
    "            'distance': dist,\n",
    "            'overlap': overlap,\n",
    "            'target': donor_target\n",
    "        })\n",
    "    \n",
    "    # Ordenar por distância\n",
    "    distances_info.sort(key=lambda x: x['distance'])\n",
    "    \n",
    "    print(f\"\\n--- TOP 10 DONORS (por distância) ---\")\n",
    "    for i, info in enumerate(distances_info[:10]):\n",
    "        print(f\"  {i+1}. Linha {info['donor_idx']}: dist={info['distance']:.4f}, overlap={info['overlap']}/{len(feature_cols)}, target={info['target']:.2f}\")\n",
    "    \n",
    "    # Simular escolha de k\n",
    "    distances_arr = np.array([d['distance'] for d in distances_info])\n",
    "    targets_arr = np.array([d['target'] for d in distances_info])\n",
    "    \n",
    "    # Verificar se é categórico\n",
    "    is_cat = (target_col in imputer.mixed_handler.nominal_cols or \n",
    "              target_col in imputer.mixed_handler.binary_cols)\n",
    "    \n",
    "    k = adaptive_k_hybrid(\n",
    "        distances_arr, targets_arr,\n",
    "        min_k=imputer.min_friends, max_k=imputer.max_friends,\n",
    "        alpha=imputer.adaptive_k_alpha, is_categorical=is_cat\n",
    "    )\n",
    "    \n",
    "    # Calcular trust components\n",
    "    k_eval = min(imputer.max_friends, len(distances_arr))\n",
    "    closest_dist = distances_arr[:k_eval]\n",
    "    closest_vals = targets_arr[:k_eval]\n",
    "    \n",
    "    mean_dist = np.mean(closest_dist[np.isfinite(closest_dist)])\n",
    "    density_trust = 1.0 / (1.0 + mean_dist) if np.isfinite(mean_dist) else 0.5\n",
    "    \n",
    "    if is_cat:\n",
    "        unique, counts = np.unique(closest_vals, return_counts=True)\n",
    "        consistency_trust = counts.max() / len(closest_vals) if len(counts) > 0 else 0.5\n",
    "    else:\n",
    "        mean_val = np.mean(closest_vals)\n",
    "        std_val = np.std(closest_vals)\n",
    "        cv = std_val / abs(mean_val) if abs(mean_val) > 1e-10 else std_val\n",
    "        consistency_trust = 1.0 / (1.0 + cv)\n",
    "    \n",
    "    print(f\"\\n--- ADAPTIVE K ---\")\n",
    "    print(f\"  density_trust: {density_trust:.4f} (mean_dist={mean_dist:.4f})\")\n",
    "    print(f\"  consistency_trust: {consistency_trust:.4f}\")\n",
    "    print(f\"  k escolhido: {k} (range [{imputer.min_friends}, {imputer.max_friends}])\")\n",
    "    \n",
    "    # Vizinhos finais\n",
    "    final_neighbors = distances_info[:k]\n",
    "    print(f\"\\n--- VIZINHOS FINAIS (k={k}) ---\")\n",
    "    for i, info in enumerate(final_neighbors):\n",
    "        print(f\"  {i+1}. Linha {info['donor_idx']}: dist={info['distance']:.4f}, target={info['target']:.2f}\")\n",
    "    \n",
    "    # Calcular valor imputado\n",
    "    neighbor_dists = np.array([d['distance'] for d in final_neighbors])\n",
    "    neighbor_vals = np.array([d['target'] for d in final_neighbors])\n",
    "    \n",
    "    if is_cat:\n",
    "        # Votação ponderada\n",
    "        weighted_votes = {}\n",
    "        for val, dist in zip(neighbor_vals, neighbor_dists):\n",
    "            w = 1 / (dist + 1e-6)\n",
    "            weighted_votes[val] = weighted_votes.get(val, 0) + w\n",
    "        imputed = max(weighted_votes.items(), key=lambda x: x[1])[0]\n",
    "    else:\n",
    "        # Média ponderada por distância inversa\n",
    "        if np.any(neighbor_dists < 1e-10):\n",
    "            imputed = np.mean(neighbor_vals[neighbor_dists < 1e-10])\n",
    "        else:\n",
    "            w = 1 / (neighbor_dists + 1e-6)\n",
    "            w = w / w.sum()\n",
    "            imputed = np.average(neighbor_vals, weights=w)\n",
    "            print(f\"\\n  Pesos IDW: {w}\")\n",
    "    \n",
    "    print(f\"\\n--- RESULTADO ---\")\n",
    "    print(f\"  Valor imputado: {imputed:.4f}\")\n",
    "    if true_value is not None:\n",
    "        error = abs(imputed - true_value)\n",
    "        print(f\"  Valor real: {true_value:.4f}\")\n",
    "        print(f\"  Erro absoluto: {error:.4f}\")\n",
    "        print(f\"  Status: {'✓ BOM' if error < 1.0 else '✗ MAU'}\")\n",
    "    \n",
    "    return imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test_with_diagnostics(data_complete, missing_indices, target_col, test_name, use_pds=True):\n",
    "    \"\"\"\n",
    "    Executa um teste completo com diagnósticos.\n",
    "    \n",
    "    Args:\n",
    "        data_complete: DataFrame completo (ground truth)\n",
    "        missing_indices: Lista de (row_idx, col_name) para tornar missing\n",
    "        target_col: Coluna target principal para diagnóstico detalhado\n",
    "        test_name: Nome do teste\n",
    "        use_pds: Se usar PDS ou não\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'#'*70}\")\n",
    "    print(f\"# TESTE: {test_name}\")\n",
    "    print(f\"{'#'*70}\")\n",
    "    \n",
    "    # Criar cópia com missings\n",
    "    data_missing = data_complete.copy()\n",
    "    true_values = {}\n",
    "    \n",
    "    for row_idx, col in missing_indices:\n",
    "        true_values[(row_idx, col)] = data_complete.loc[row_idx, col]\n",
    "        data_missing.loc[row_idx, col] = np.nan\n",
    "    \n",
    "    print(f\"\\nDataset: {data_missing.shape[0]} linhas x {data_missing.shape[1]} colunas\")\n",
    "    print(f\"Missings introduzidos: {len(missing_indices)}\")\n",
    "    print(f\"PDS: {use_pds}\")\n",
    "    \n",
    "    print(f\"\\n--- DADOS COMPLETOS (ground truth) ---\")\n",
    "    print(data_complete.to_string())\n",
    "    \n",
    "    print(f\"\\n--- DADOS COM MISSINGS ---\")\n",
    "    print(data_missing.to_string())\n",
    "    \n",
    "    # Executar ISCA-k\n",
    "    imputer = ISCAkCore(verbose=True, use_pds=use_pds, min_friends=2, max_friends=5)\n",
    "    result = imputer.impute(data_missing, interactive=False)\n",
    "    \n",
    "    print(f\"\\n--- RESULTADO IMPUTAÇÃO ---\")\n",
    "    print(result.to_string())\n",
    "    \n",
    "    # Verificar erros\n",
    "    print(f\"\\n--- VERIFICAÇÃO DE ERROS ---\")\n",
    "    total_error = 0\n",
    "    n_errors = 0\n",
    "    \n",
    "    for (row_idx, col), true_val in true_values.items():\n",
    "        imputed_val = result.loc[row_idx, col]\n",
    "        error = abs(imputed_val - true_val)\n",
    "        total_error += error\n",
    "        n_errors += 1\n",
    "        \n",
    "        status = \"✓\" if error < 1.0 else \"✗\"\n",
    "        print(f\"  {status} Linha {row_idx}, Col '{col}': imputado={imputed_val:.2f}, real={true_val:.2f}, erro={error:.2f}\")\n",
    "    \n",
    "    mae = total_error / n_errors if n_errors > 0 else 0\n",
    "    print(f\"\\nMAE total: {mae:.4f}\")\n",
    "    \n",
    "    # Diagnóstico detalhado para o primeiro missing\n",
    "    if missing_indices:\n",
    "        first_row, first_col = missing_indices[0]\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"DIAGNÓSTICO DETALHADO para linha {first_row}, coluna '{first_col}'\")\n",
    "        diagnose_imputation(imputer, data_missing, first_col, first_row, true_values[(first_row, first_col)])\n",
    "    \n",
    "    # Stats da execução\n",
    "    print(f\"\\n--- ESTATÍSTICAS DE EXECUÇÃO ---\")\n",
    "    stats = imputer.execution_stats\n",
    "    print(f\"  Fase 2 activada: {stats.get('phase2_activated', 'N/A')}\")\n",
    "    print(f\"  Fase 2 ciclos: {stats.get('phase2_cycles', 'N/A')}\")\n",
    "    print(f\"  Fase 2 imputados: {stats.get('phase2_imputed', 'N/A')}\")\n",
    "    \n",
    "    return result, imputer, mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Teste 1: Vizinhos Óbvios (Sanity Check)\n",
    "\n",
    "**Objectivo**: Verificar se o algoritmo escolhe os vizinhos correctos num cenário simples.\n",
    "\n",
    "Criamos dois clusters bem separados e verificamos se a imputação usa vizinhos do cluster correcto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste 1.1: Dataset pequeno (7 linhas, 3 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset com dois clusters óbvios\n",
    "data_1_1 = pd.DataFrame({\n",
    "    'A': [1.0, 1.1, 1.2, 5.0, 5.1, 5.2, 1.05],\n",
    "    'B': [1.0, 1.1, 1.2, 5.0, 5.1, 5.2, 1.05],\n",
    "    'C': [1.0, 1.1, 1.2, 5.0, 5.1, 5.2, 1.05],\n",
    "    'Target': [10.0, 11.0, 12.0, 50.0, 51.0, 52.0, 10.5]  # Linha 6 será missing\n",
    "})\n",
    "\n",
    "# Comportamento esperado:\n",
    "# - Vizinhos de linha 6 devem ser {0, 1, 2} (não {3, 4, 5})\n",
    "# - Valor imputado deve ser ~10.5 (não ~50)\n",
    "\n",
    "result, imputer, mae = run_test_with_diagnostics(\n",
    "    data_1_1, \n",
    "    missing_indices=[(6, 'Target')],\n",
    "    target_col='Target',\n",
    "    test_name=\"1.1 - Vizinhos Óbvios (7x4, 1 missing)\"\n",
    ")\n",
    "\n",
    "# Verificação\n",
    "imputed_val = result.loc[6, 'Target']\n",
    "expected_range = (9.5, 12.5)  # Deve estar próximo de 10-12\n",
    "assert expected_range[0] <= imputed_val <= expected_range[1], f\"FALHOU: {imputed_val} não está em {expected_range}\"\n",
    "print(f\"\\n✓ TESTE 1.1 PASSOU: valor imputado {imputed_val:.2f} está no range esperado {expected_range}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste 1.2: Dataset médio (20 linhas, 5 features, 10% missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dois clusters com mais linhas\n",
    "np.random.seed(42)\n",
    "\n",
    "# Cluster 1: valores ~1, target ~10\n",
    "cluster1 = pd.DataFrame({\n",
    "    'A': np.random.normal(1.0, 0.1, 10),\n",
    "    'B': np.random.normal(1.0, 0.1, 10),\n",
    "    'C': np.random.normal(1.0, 0.1, 10),\n",
    "    'D': np.random.normal(1.0, 0.1, 10),\n",
    "    'Target': np.random.normal(10.0, 0.5, 10)\n",
    "})\n",
    "\n",
    "# Cluster 2: valores ~5, target ~50\n",
    "cluster2 = pd.DataFrame({\n",
    "    'A': np.random.normal(5.0, 0.1, 10),\n",
    "    'B': np.random.normal(5.0, 0.1, 10),\n",
    "    'C': np.random.normal(5.0, 0.1, 10),\n",
    "    'D': np.random.normal(5.0, 0.1, 10),\n",
    "    'Target': np.random.normal(50.0, 0.5, 10)\n",
    "})\n",
    "\n",
    "data_1_2 = pd.concat([cluster1, cluster2], ignore_index=True)\n",
    "\n",
    "# Introduzir 10% missings (2 valores) - um de cada cluster\n",
    "missing_indices_1_2 = [(2, 'Target'), (15, 'Target')]  # Linha 2 cluster1, linha 15 cluster2\n",
    "\n",
    "result, imputer, mae = run_test_with_diagnostics(\n",
    "    data_1_2,\n",
    "    missing_indices=missing_indices_1_2,\n",
    "    target_col='Target',\n",
    "    test_name=\"1.2 - Dataset médio (20x5, 10% missing)\"\n",
    ")\n",
    "\n",
    "# Verificação\n",
    "val_cluster1 = result.loc[2, 'Target']\n",
    "val_cluster2 = result.loc[15, 'Target']\n",
    "\n",
    "assert 8 <= val_cluster1 <= 12, f\"FALHOU cluster1: {val_cluster1} deveria estar em [8, 12]\"\n",
    "assert 48 <= val_cluster2 <= 52, f\"FALHOU cluster2: {val_cluster2} deveria estar em [48, 52]\"\n",
    "print(f\"\\n✓ TESTE 1.2 PASSOU: cluster1={val_cluster1:.2f}, cluster2={val_cluster2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste 1.3: Dataset maior (100 linhas, 10 features, 20% missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# 3 clusters\n",
    "n_per_cluster = 33\n",
    "\n",
    "def make_cluster(center, target_center, n, n_features=10):\n",
    "    data = {f'F{i}': np.random.normal(center, 0.2, n) for i in range(n_features)}\n",
    "    data['Target'] = np.random.normal(target_center, 1.0, n)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "cluster1 = make_cluster(1.0, 10.0, n_per_cluster)\n",
    "cluster2 = make_cluster(5.0, 50.0, n_per_cluster)\n",
    "cluster3 = make_cluster(9.0, 90.0, n_per_cluster + 1)  # +1 para dar 100\n",
    "\n",
    "data_1_3 = pd.concat([cluster1, cluster2, cluster3], ignore_index=True)\n",
    "\n",
    "# 20% missings aleatórios no Target\n",
    "np.random.seed(42)\n",
    "missing_rows = np.random.choice(100, 20, replace=False)\n",
    "missing_indices_1_3 = [(row, 'Target') for row in missing_rows]\n",
    "\n",
    "result, imputer, mae = run_test_with_diagnostics(\n",
    "    data_1_3,\n",
    "    missing_indices=missing_indices_1_3,\n",
    "    target_col='Target',\n",
    "    test_name=\"1.3 - Dataset maior (100x11, 20% missing)\"\n",
    ")\n",
    "\n",
    "print(f\"\\nMAE para 20% missing: {mae:.4f}\")\n",
    "assert mae < 5.0, f\"FALHOU: MAE {mae} muito alto\"\n",
    "print(f\"✓ TESTE 1.3 PASSOU: MAE={mae:.4f} < 5.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste 1.4: Dataset maior com 40% missing (verificar colapso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mesmo dataset, mas com 40% missings\n",
    "np.random.seed(42)\n",
    "missing_rows_40 = np.random.choice(100, 40, replace=False)\n",
    "missing_indices_1_4 = [(row, 'Target') for row in missing_rows_40]\n",
    "\n",
    "result, imputer, mae = run_test_with_diagnostics(\n",
    "    data_1_3,  # Reutilizar dataset\n",
    "    missing_indices=missing_indices_1_4,\n",
    "    target_col='Target',\n",
    "    test_name=\"1.4 - Dataset maior (100x11, 40% missing)\"\n",
    ")\n",
    "\n",
    "print(f\"\\nMAE para 40% missing: {mae:.4f}\")\n",
    "print(f\"Comparação: MAE 20%={mae:.4f} vs MAE 40%={mae:.4f}\")\n",
    "\n",
    "# Verificar se a Fase 2 foi activada\n",
    "if imputer.execution_stats.get('phase2_activated', False):\n",
    "    print(f\"\\n⚠️  FASE 2 ACTIVADA com {imputer.execution_stats['phase2_cycles']} ciclos\")\n",
    "else:\n",
    "    print(f\"\\n✓ Fase 1 resolveu tudo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Teste 2: Correlação Forte vs Fraca\n",
    "\n",
    "**Objectivo**: Verificar se os pesos MI reflectem correctamente as correlações reais.\n",
    "\n",
    "Criamos um dataset onde:\n",
    "- Coluna A tem correlação forte (0.95) com Target\n",
    "- Coluna B tem correlação fraca (~0) com Target  \n",
    "- Coluna C tem correlação média (0.5) com Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n = 100\n",
    "\n",
    "# Target base\n",
    "target = np.linspace(0, 100, n)\n",
    "\n",
    "# A: correlação forte com Target (r ~ 0.95)\n",
    "A = target + np.random.normal(0, 5, n)\n",
    "\n",
    "# B: correlação fraca com Target (r ~ 0)\n",
    "B = np.random.uniform(0, 100, n)\n",
    "\n",
    "# C: correlação média com Target (r ~ 0.5)\n",
    "C = 0.5 * target + 0.5 * np.random.uniform(0, 100, n)\n",
    "\n",
    "data_2 = pd.DataFrame({\n",
    "    'A': A,\n",
    "    'B': B,\n",
    "    'C': C,\n",
    "    'Target': target\n",
    "})\n",
    "\n",
    "# Verificar correlações reais\n",
    "from scipy.stats import pearsonr\n",
    "corr_A = pearsonr(data_2['A'], data_2['Target'])[0]\n",
    "corr_B = pearsonr(data_2['B'], data_2['Target'])[0]\n",
    "corr_C = pearsonr(data_2['C'], data_2['Target'])[0]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TESTE 2: Correlação Forte vs Fraca\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nCorrelações reais com Target:\")\n",
    "print(f\"  A: {corr_A:.4f} (esperado ~0.95)\")\n",
    "print(f\"  B: {corr_B:.4f} (esperado ~0.00)\")\n",
    "print(f\"  C: {corr_C:.4f} (esperado ~0.50)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduzir alguns missings e executar\n",
    "np.random.seed(42)\n",
    "missing_rows = np.random.choice(100, 20, replace=False)\n",
    "missing_indices_2 = [(row, 'Target') for row in missing_rows]\n",
    "\n",
    "result, imputer, mae = run_test_with_diagnostics(\n",
    "    data_2,\n",
    "    missing_indices=missing_indices_2,\n",
    "    target_col='Target',\n",
    "    test_name=\"2 - Correlação Forte vs Fraca\"\n",
    ")\n",
    "\n",
    "# Verificar pesos MI\n",
    "print(f\"\\n--- VERIFICAÇÃO DOS PESOS MI ---\")\n",
    "mi_A = imputer.mi_matrix.loc['A', 'Target']\n",
    "mi_B = imputer.mi_matrix.loc['B', 'Target']\n",
    "mi_C = imputer.mi_matrix.loc['C', 'Target']\n",
    "\n",
    "print(f\"MI(A, Target) = {mi_A:.4f}\")\n",
    "print(f\"MI(B, Target) = {mi_B:.4f}\")\n",
    "print(f\"MI(C, Target) = {mi_C:.4f}\")\n",
    "\n",
    "# Pesos normalizados\n",
    "total_mi = mi_A + mi_B + mi_C\n",
    "w_A = mi_A / total_mi\n",
    "w_B = mi_B / total_mi\n",
    "w_C = mi_C / total_mi\n",
    "\n",
    "print(f\"\\nPesos normalizados:\")\n",
    "print(f\"  w(A) = {w_A:.4f} (esperado > 0.5)\")\n",
    "print(f\"  w(B) = {w_B:.4f} (esperado < 0.1)\")\n",
    "print(f\"  w(C) = {w_C:.4f} (esperado ~0.3)\")\n",
    "\n",
    "# Verificações\n",
    "assert w_A > w_C > w_B, f\"FALHOU: ordem dos pesos deveria ser A > C > B, mas foi A={w_A:.4f}, B={w_B:.4f}, C={w_C:.4f}\"\n",
    "print(f\"\\n✓ TESTE 2 PASSOU: pesos MI reflectem correlações (A > C > B)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Teste 3: Fase 2 Forçada\n",
    "\n",
    "**Objectivo**: Criar uma situação onde a Fase 1 não consegue imputar e verificar se a Fase 2 é activada correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset onde cada linha tem missings em colunas diferentes\n",
    "# Apenas uma linha está completa\n",
    "data_3 = pd.DataFrame({\n",
    "    'A': [1.0, np.nan, np.nan, np.nan, 1.0, 1.1, 1.2],\n",
    "    'B': [np.nan, 2.0, np.nan, np.nan, 2.0, 2.1, 2.2],\n",
    "    'C': [np.nan, np.nan, 3.0, np.nan, 3.0, 3.1, 3.2],\n",
    "    'D': [np.nan, np.nan, np.nan, 4.0, 4.0, 4.1, 4.2],\n",
    "    'Target': [10.0, 20.0, 30.0, 40.0, 25.0, 26.0, 27.0]  # Todos preenchidos\n",
    "})\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TESTE 3: Fase 2 Forçada\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nDataset com overlap mínimo entre linhas:\")\n",
    "print(data_3.to_string())\n",
    "print(f\"\\nLinhas 0-3: cada uma só tem 1 feature preenchida (overlap mínimo)\")\n",
    "print(f\"Linhas 4-6: completas (podem ser donors)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tentar imputar a linha 0 que só tem A preenchido\n",
    "data_3_test = data_3.copy()\n",
    "true_value = data_3_test.loc[0, 'B']\n",
    "data_3_test.loc[0, 'B'] = np.nan  # Adicionar mais um missing\n",
    "\n",
    "print(f\"\\nAdicionando missing na linha 0, coluna B\")\n",
    "print(f\"Valor real: {true_value}\")\n",
    "\n",
    "imputer = ISCAkCore(verbose=True, use_pds=True, min_friends=2)\n",
    "result = imputer.impute(data_3_test, interactive=False)\n",
    "\n",
    "print(f\"\\n--- ESTATÍSTICAS DE EXECUÇÃO ---\")\n",
    "stats = imputer.execution_stats\n",
    "print(f\"  Fase 2 activada: {stats.get('phase2_activated', 'N/A')}\")\n",
    "print(f\"  Fase 2 ciclos: {stats.get('phase2_cycles', 'N/A')}\")\n",
    "print(f\"  Fase 2 imputados: {stats.get('phase2_imputed', 'N/A')}\")\n",
    "\n",
    "print(f\"\\nResultado:\")\n",
    "print(result.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Teste 4: PDS vs Clássico\n",
    "\n",
    "**Objectivo**: Comparar o comportamento com e sem PDS (Partial Distance Strategy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset onde alguns donors têm overlap parcial\n",
    "data_4 = pd.DataFrame({\n",
    "    'A': [1.0, np.nan, 1.0, 5.0, 1.1],\n",
    "    'B': [1.0, np.nan, 1.0, 5.0, 1.1],\n",
    "    'C': [1.0, 1.0, 1.0, 5.0, 1.1],\n",
    "    'D': [np.nan, 1.0, 1.0, 5.0, 1.1],\n",
    "    'E': [np.nan, 1.0, 1.0, 5.0, np.nan],  # Linha 4 terá missing aqui\n",
    "    'Target': [10.0, 10.0, 10.0, 50.0, np.nan]  # Linha 4 terá missing\n",
    "})\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TESTE 4: PDS vs Clássico\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nDataset:\")\n",
    "print(data_4.to_string())\n",
    "print(f\"\\nLinha 0: overlap 3/5 com linha 4 (A, B, C)\")\n",
    "print(f\"Linha 1: overlap 2/5 com linha 4 (C, D)\")\n",
    "print(f\"Linha 2: overlap 4/5 com linha 4 (A, B, C, D) - MELHOR\")\n",
    "print(f\"Linha 3: overlap 4/5 com linha 4, mas distante\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar valor real\n",
    "true_target = 10.5  # Deveria ser ~10 baseado no cluster\n",
    "\n",
    "# Teste COM PDS\n",
    "print(f\"\\n{'='*30} COM PDS {'='*30}\")\n",
    "imputer_pds = ISCAkCore(verbose=True, use_pds=True, min_friends=2)\n",
    "result_pds = imputer_pds.impute(data_4.copy(), interactive=False)\n",
    "val_pds = result_pds.loc[4, 'Target']\n",
    "print(f\"\\nValor imputado COM PDS: {val_pds:.4f}\")\n",
    "\n",
    "# Teste SEM PDS\n",
    "print(f\"\\n{'='*30} SEM PDS {'='*30}\")\n",
    "imputer_no_pds = ISCAkCore(verbose=True, use_pds=False, min_friends=2)\n",
    "result_no_pds = imputer_no_pds.impute(data_4.copy(), interactive=False)\n",
    "val_no_pds = result_no_pds.loc[4, 'Target']\n",
    "print(f\"\\nValor imputado SEM PDS: {val_no_pds:.4f}\")\n",
    "\n",
    "print(f\"\\n--- COMPARAÇÃO ---\")\n",
    "print(f\"  COM PDS: {val_pds:.4f}\")\n",
    "print(f\"  SEM PDS: {val_no_pds:.4f}\")\n",
    "print(f\"  Esperado: ~10.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Teste 5: Dados Mistos (Numérico + Categórico)\n",
    "\n",
    "**Objectivo**: Verificar se a combinação de distâncias numéricas e categóricas está correcta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset misto com clusters baseados em numérico E categórico\n",
    "data_5 = pd.DataFrame({\n",
    "    'Num1': [1.0, 1.1, 1.2, 5.0, 5.1, 5.2, 1.05],\n",
    "    'Num2': [10.0, 10.1, 10.2, 50.0, 50.1, 50.2, 10.05],\n",
    "    'Cat1': ['A', 'A', 'A', 'B', 'B', 'B', 'A'],\n",
    "    'Cat2': ['X', 'X', 'X', 'Y', 'Y', 'Y', 'X'],\n",
    "    'Target': [100.0, 101.0, 102.0, 500.0, 501.0, 502.0, 100.5]  # Linha 6 será missing\n",
    "})\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TESTE 5: Dados Mistos\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nDataset com numéricos e categóricos:\")\n",
    "print(data_5.to_string())\n",
    "print(f\"\\nCluster 1 (linhas 0-2, 6): Num~1, Cat=A/X, Target~100\")\n",
    "print(f\"Cluster 2 (linhas 3-5): Num~5, Cat=B/Y, Target~500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste 5.1: Missing no Target (numérico)\n",
    "print(f\"\\n{'='*30} TESTE 5.1: Missing numérico {'='*30}\")\n",
    "result, imputer, mae = run_test_with_diagnostics(\n",
    "    data_5,\n",
    "    missing_indices=[(6, 'Target')],\n",
    "    target_col='Target',\n",
    "    test_name=\"5.1 - Dados Mistos (missing numérico)\"\n",
    ")\n",
    "\n",
    "val = result.loc[6, 'Target']\n",
    "assert 90 <= val <= 110, f\"FALHOU: {val} deveria estar em [90, 110]\"\n",
    "print(f\"\\n✓ TESTE 5.1 PASSOU: valor imputado {val:.2f} está no cluster correcto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste 5.2: Missing no Cat1 (categórico)\n",
    "print(f\"\\n{'='*30} TESTE 5.2: Missing categórico {'='*30}\")\n",
    "\n",
    "data_5_cat = data_5.copy()\n",
    "true_cat = data_5_cat.loc[6, 'Cat1']\n",
    "data_5_cat.loc[6, 'Cat1'] = np.nan\n",
    "\n",
    "imputer = ISCAkCore(verbose=True, use_pds=True, min_friends=2)\n",
    "result = imputer.impute(data_5_cat, interactive=False)\n",
    "\n",
    "val_cat = result.loc[6, 'Cat1']\n",
    "print(f\"\\nValor real: {true_cat}\")\n",
    "print(f\"Valor imputado: {val_cat}\")\n",
    "\n",
    "# Nota: o valor pode estar codificado, precisamos decodificar\n",
    "print(f\"\\nMapeamento nominal: {imputer.mixed_handler.nominal_mappings.get('Cat1', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Teste 6: Simulação de Colapso (como SONAR 40%)\n",
    "\n",
    "**Objectivo**: Tentar reproduzir o colapso que vimos no SONAR a 40% missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar dataset similar ao SONAR (muitas features)\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "n_features = 60\n",
    "\n",
    "# 3 clusters\n",
    "def make_sonar_like_cluster(center, target_center, n):\n",
    "    data = {f'F{i}': np.random.normal(center, 0.2, n) for i in range(n_features)}\n",
    "    data['Target'] = np.random.normal(target_center, 2.0, n)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "c1 = make_sonar_like_cluster(0.2, 10, 66)\n",
    "c2 = make_sonar_like_cluster(0.5, 50, 67)\n",
    "c3 = make_sonar_like_cluster(0.8, 90, 67)\n",
    "\n",
    "data_6 = pd.concat([c1, c2, c3], ignore_index=True)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TESTE 6: Simulação de Colapso (tipo SONAR)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nDataset: {data_6.shape[0]} linhas x {data_6.shape[1]} colunas\")\n",
    "print(f\"3 clusters com targets ~10, ~50, ~90\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste com 20% missing\n",
    "print(f\"\\n{'='*30} 20% MISSING {'='*30}\")\n",
    "np.random.seed(42)\n",
    "missing_20 = np.random.choice(200, 40, replace=False)\n",
    "missing_indices_20 = [(row, 'Target') for row in missing_20]\n",
    "\n",
    "data_6_20 = data_6.copy()\n",
    "true_vals_20 = {row: data_6.loc[row, 'Target'] for row in missing_20}\n",
    "for row in missing_20:\n",
    "    data_6_20.loc[row, 'Target'] = np.nan\n",
    "\n",
    "imputer_20 = ISCAkCore(verbose=True, use_pds=True)\n",
    "result_20 = imputer_20.impute(data_6_20, interactive=False)\n",
    "\n",
    "# Calcular MAE\n",
    "errors_20 = [abs(result_20.loc[row, 'Target'] - true_vals_20[row]) for row in missing_20]\n",
    "mae_20 = np.mean(errors_20)\n",
    "print(f\"\\nMAE 20%: {mae_20:.4f}\")\n",
    "print(f\"Fase 2 activada: {imputer_20.execution_stats.get('phase2_activated', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste com 40% missing\n",
    "print(f\"\\n{'='*30} 40% MISSING {'='*30}\")\n",
    "np.random.seed(42)\n",
    "missing_40 = np.random.choice(200, 80, replace=False)\n",
    "missing_indices_40 = [(row, 'Target') for row in missing_40]\n",
    "\n",
    "data_6_40 = data_6.copy()\n",
    "true_vals_40 = {row: data_6.loc[row, 'Target'] for row in missing_40}\n",
    "for row in missing_40:\n",
    "    data_6_40.loc[row, 'Target'] = np.nan\n",
    "\n",
    "imputer_40 = ISCAkCore(verbose=True, use_pds=True)\n",
    "result_40 = imputer_40.impute(data_6_40, interactive=False)\n",
    "\n",
    "# Calcular MAE\n",
    "errors_40 = [abs(result_40.loc[row, 'Target'] - true_vals_40[row]) for row in missing_40]\n",
    "mae_40 = np.mean(errors_40)\n",
    "print(f\"\\nMAE 40%: {mae_40:.4f}\")\n",
    "print(f\"Fase 2 activada: {imputer_40.execution_stats.get('phase2_activated', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparação\n",
    "print(f\"\\n{'='*30} COMPARAÇÃO {'='*30}\")\n",
    "print(f\"MAE 20% missing: {mae_20:.4f}\")\n",
    "print(f\"MAE 40% missing: {mae_40:.4f}\")\n",
    "print(f\"Degradação: {((mae_40 - mae_20) / mae_20 * 100):.1f}%\")\n",
    "\n",
    "if mae_40 > mae_20 * 3:\n",
    "    print(f\"\\n⚠️  COLAPSO DETECTADO: MAE triplicou de 20% para 40%\")\n",
    "else:\n",
    "    print(f\"\\n✓ Degradação dentro do esperado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Teste 7: Adaptive K\n",
    "\n",
    "**Objectivo**: Verificar se o adaptive k escolhe valores apropriados em diferentes cenários."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"TESTE 7: Adaptive K\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Cenário 7.1: Vizinhos CONSISTENTES (todos com target similar)\n",
    "print(f\"\\n--- Cenário 7.1: Vizinhos Consistentes ---\")\n",
    "distances_consistent = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "values_consistent = np.array([10.0, 10.1, 10.2, 9.9, 10.0, 10.1, 9.8, 10.2, 10.0, 10.1])\n",
    "\n",
    "k_consistent = adaptive_k_hybrid(distances_consistent, values_consistent, min_k=3, max_k=10, alpha=0.5)\n",
    "print(f\"  Targets dos vizinhos: {values_consistent}\")\n",
    "print(f\"  Std dos targets: {np.std(values_consistent):.4f}\")\n",
    "print(f\"  k escolhido: {k_consistent}\")\n",
    "print(f\"  Esperado: k ALTO (vizinhos concordam, seguro usar mais)\")\n",
    "\n",
    "# Cenário 7.2: Vizinhos INCONSISTENTES (targets muito diferentes)\n",
    "print(f\"\\n--- Cenário 7.2: Vizinhos Inconsistentes ---\")\n",
    "distances_inconsistent = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "values_inconsistent = np.array([10.0, 50.0, 5.0, 100.0, 20.0, 80.0, 15.0, 90.0, 30.0, 70.0])\n",
    "\n",
    "k_inconsistent = adaptive_k_hybrid(distances_inconsistent, values_inconsistent, min_k=3, max_k=10, alpha=0.5)\n",
    "print(f\"  Targets dos vizinhos: {values_inconsistent}\")\n",
    "print(f\"  Std dos targets: {np.std(values_inconsistent):.4f}\")\n",
    "print(f\"  k escolhido: {k_inconsistent}\")\n",
    "print(f\"  Esperado: k BAIXO (vizinhos discordam, usar menos)\")\n",
    "\n",
    "# Verificação\n",
    "assert k_consistent > k_inconsistent, f\"FALHOU: k_consistent ({k_consistent}) deveria ser > k_inconsistent ({k_inconsistent})\"\n",
    "print(f\"\\n✓ TESTE 7 PASSOU: k_consistent ({k_consistent}) > k_inconsistent ({k_inconsistent})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Resumo dos Testes\n",
    "\n",
    "Execute esta célula no final para ver um resumo de todos os testes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RESUMO DOS TESTES DIAGNÓSTICOS\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "Execute cada teste individualmente e verifique:\n",
    "\n",
    "1. VIZINHOS ÓBVIOS\n",
    "   - Os vizinhos seleccionados são do cluster correcto?\n",
    "   - O valor imputado está no range esperado?\n",
    "   - Há degradação excessiva com mais missings?\n",
    "\n",
    "2. CORRELAÇÃO FORTE vs FRACA\n",
    "   - Os pesos MI reflectem as correlações reais?\n",
    "   - A ordem é A > C > B?\n",
    "\n",
    "3. FASE 2 FORÇADA\n",
    "   - A Fase 2 é activada quando necessário?\n",
    "   - Quantos ciclos são necessários?\n",
    "\n",
    "4. PDS vs CLÁSSICO\n",
    "   - O PDS melhora ou piora os resultados?\n",
    "   - A escala está correcta?\n",
    "\n",
    "5. DADOS MISTOS\n",
    "   - Distâncias numéricas e categóricas estão calibradas?\n",
    "   - A votação categórica funciona?\n",
    "\n",
    "6. COLAPSO 40%\n",
    "   - Há colapso súbito entre 20% e 40%?\n",
    "   - A Fase 2 está a ser usada excessivamente?\n",
    "\n",
    "7. ADAPTIVE K\n",
    "   - k varia conforme esperado?\n",
    "   - Vizinhos consistentes → k alto?\n",
    "   - Vizinhos inconsistentes → k baixo?\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
