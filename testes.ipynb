{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03cd6c12-f06e-42de-92b9-345b33e60f7b",
   "metadata": {},
   "source": [
    "## Resultados Finais"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6bff4d-2980-43f3-8446-22209e86aaaa",
   "metadata": {},
   "source": [
    "### Setup Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6e9621-d525-4360-9acc-906091f4947c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from iscak_core import ISCAkCore\n",
    "\n",
    "# Configurações\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Configuração de runs\n",
    "N_RUNS = 10\n",
    "BASE_SEED = 42\n",
    "MISSING_RATES = [0.10, 0.20, 0.30, 0.40]\n",
    "MECHANISMS = ['MCAR', 'MAR', 'MNAR']\n",
    "METHOD_ORDER = ['ISCA-k', 'KNN', 'MICE', 'MissForest']\n",
    "\n",
    "# Gerar seeds dinamicamente\n",
    "SEEDS = [BASE_SEED + run for run in range(N_RUNS)]\n",
    "\n",
    "# Cores consistentes para métodos (Plotly)\n",
    "METHOD_COLORS = {\n",
    "    'ISCA-k': '#2E86AB',      # Azul\n",
    "    'KNN': '#A23B72',         # Roxo\n",
    "    'MICE': '#F18F01',        # Laranja\n",
    "    'MissForest': '#C73E1D'   # Vermelho\n",
    "}\n",
    "\n",
    "print(\"Setup inicial completo\")\n",
    "print(f\"N_RUNS: {N_RUNS}\")\n",
    "print(f\"Seeds: {BASE_SEED} a {BASE_SEED + N_RUNS - 1}\")\n",
    "print(f\"Missing rates: {[int(r*100) for r in MISSING_RATES]}%\")\n",
    "print(f\"Mecanismos: {MECHANISMS}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32aaccdc-3a36-46b7-afcf-027ccd2a06d5",
   "metadata": {},
   "source": [
    "### Funções Comuns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7984be14-9672-499d-aabb-f79c97c2a14e",
   "metadata": {},
   "source": [
    "##### Gerar Missings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2d69f3-311b-446b-84db-ce1a378527bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def introduce_mcar(data, missing_rate, seed):\n",
    "    \"\"\"\n",
    "    Missing Completely At Random - implementação do algoritmo estruturado\n",
    "    Seleciona posições (registro, atributo) completamente aleatórias\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.DataFrame\n",
    "        Dataset completo\n",
    "    missing_rate : float\n",
    "        Taxa de missing desejada (MDR)\n",
    "    seed : int\n",
    "        Seed para reprodutibilidade\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    data_missing = data.copy()\n",
    "    \n",
    "    # Definições\n",
    "    N = len(data)  # Total de registos\n",
    "    M = len(data.columns)  # Total de atributos\n",
    "    MDR = missing_rate  # Taxa de missing desejada\n",
    "    \n",
    "    while True:\n",
    "        # Gerar índices aleatórios\n",
    "        X = np.random.randint(0, N)  # Registo aleatório\n",
    "        Y = np.random.randint(0, M)  # Atributo aleatório\n",
    "        \n",
    "        # Se a posição ainda não é NaN, torná-la NaN\n",
    "        if not pd.isna(data_missing.iloc[X, Y]):\n",
    "            data_missing.iloc[X, Y] = np.nan\n",
    "            \n",
    "            # Calcular ratio atual de missing\n",
    "            R = data_missing.isna().sum().sum() / (N * M)\n",
    "            \n",
    "            # Se atingiu a taxa desejada, parar\n",
    "            if R >= MDR:\n",
    "                break\n",
    "    \n",
    "    return data_missing\n",
    "\n",
    "def introduce_mar(data, missing_rate, seed, na_attributes=None):\n",
    "    \"\"\"\n",
    "    Missing At Random - implementação do algoritmo estruturado\n",
    "    Os missings dependem de um atributo causativo (valores baixos causam missing)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.DataFrame\n",
    "        Dataset completo\n",
    "    missing_rate : float\n",
    "        Taxa de missing desejada (MDR)\n",
    "    seed : int\n",
    "        Seed para reprodutibilidade\n",
    "    na_attributes : int, optional\n",
    "        Número de atributos que perdem valores (NA). Se None, usa metade das colunas\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    data_missing = data.copy()\n",
    "    \n",
    "    # Definições\n",
    "    N = len(data)  # Total de registos\n",
    "    M = len(data.columns)  # Total de atributos\n",
    "    V = N * M  # Total de valores no dataset\n",
    "    \n",
    "    # NA: número de atributos que perdem valores\n",
    "    if na_attributes is None:\n",
    "        NA = max(1, M // 2)  # Default: metade dos atributos\n",
    "    else:\n",
    "        NA = min(na_attributes, M)  # Não pode exceder o total de atributos\n",
    "    \n",
    "    MDR = missing_rate * 100  # Taxa de missing em percentagem\n",
    "    \n",
    "    # Selecionar atributo causativo aleatório\n",
    "    Causative = np.random.randint(0, M)\n",
    "    \n",
    "    # Selecionar NA atributos aleatórios que perderão valores (diferentes do causativo)\n",
    "    MDAttributes = []\n",
    "    while len(MDAttributes) < NA:\n",
    "        Y = np.random.randint(0, M)\n",
    "        if Y not in MDAttributes and Y != Causative:\n",
    "            MDAttributes.append(Y)\n",
    "    \n",
    "    # Se não conseguimos NA atributos diferentes do causativo (dataset muito pequeno)\n",
    "    if len(MDAttributes) == 0:\n",
    "        MDAttributes = [i for i in range(M) if i != Causative][:NA]\n",
    "    \n",
    "    # Obter vetor de valores do atributo causativo (converter para float para aceitar np.inf)\n",
    "    Aux = data.iloc[:, Causative].values.astype(float).copy()\n",
    "    \n",
    "    # Selecionar registos com valores MÍNIMOS no atributo causativo\n",
    "    MDRecords = []\n",
    "    MV = 0  # Contador de valores missing\n",
    "    MaxInt = np.inf  # Usar infinito (agora Aux é float)\n",
    "    \n",
    "    while True:\n",
    "        # Encontrar índice do valor mínimo em Aux\n",
    "        MinIndex = np.argmin(Aux)\n",
    "        MDRecords.append(MinIndex)\n",
    "        \n",
    "        # Marcar este índice como inelegível\n",
    "        Aux[MinIndex] = MaxInt\n",
    "        \n",
    "        MV += len(MDAttributes)  # Incrementar por NA (número de atributos que perdem valores)\n",
    "        R = (MV / V) * 100  # Calcular ratio atual\n",
    "        \n",
    "        if R >= MDR:\n",
    "            break\n",
    "    \n",
    "    # Aplicar missing nos registos e atributos selecionados\n",
    "    for i in MDRecords:\n",
    "        for j in MDAttributes:\n",
    "            data_missing.iloc[i, j] = np.nan\n",
    "    \n",
    "    return data_missing\n",
    "\n",
    "def introduce_mnar(data, missing_rate, seed, na_attributes=None):\n",
    "    \"\"\"\n",
    "    Missing Not At Random - implementação estatisticamente correcta.\n",
    "    Os missings dependem do PRÓPRIO VALOR que vai faltar.\n",
    "    Valores mais ALTOS têm maior probabilidade de ser missing.\n",
    "    \n",
    "    Simula cenários reais como:\n",
    "    - Pessoas com rendimentos altos não reportam rendimento\n",
    "    - Pacientes mais graves faltam aos check-ups\n",
    "    - Valores extremos são omitidos por vergonha/receio\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.DataFrame\n",
    "        Dataset completo\n",
    "    missing_rate : float\n",
    "        Taxa de missing desejada (MDR)\n",
    "    seed : int\n",
    "        Seed para reprodutibilidade\n",
    "    na_attributes : int, optional\n",
    "        Número de atributos que perdem valores (NA). Se None, usa metade das colunas\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    data_missing = data.copy()\n",
    "    \n",
    "    N = len(data)\n",
    "    M = len(data.columns)\n",
    "    V = N * M\n",
    "    \n",
    "    if na_attributes is None:\n",
    "        NA = max(1, M // 2)\n",
    "    else:\n",
    "        NA = min(na_attributes, M)\n",
    "    \n",
    "    target_missing = int(missing_rate * V)\n",
    "    \n",
    "    # Selecionar NA atributos aleatórios\n",
    "    all_cols = list(range(M))\n",
    "    np.random.shuffle(all_cols)\n",
    "    MDAttributes = all_cols[:NA]\n",
    "    \n",
    "    # Distribuir missings pelos atributos selecionados\n",
    "    missing_per_attr = target_missing // NA\n",
    "    extra = target_missing % NA\n",
    "    \n",
    "    for i, col_idx in enumerate(MDAttributes):\n",
    "        # Quantos missings para este atributo\n",
    "        n_missing = missing_per_attr + (1 if i < extra else 0)\n",
    "        n_missing = min(n_missing, N)\n",
    "        \n",
    "        if n_missing <= 0:\n",
    "            continue\n",
    "        \n",
    "        # Valores da coluna\n",
    "        values = data_missing.iloc[:, col_idx].values.astype(float)\n",
    "        \n",
    "        # MNAR: probabilidade de missing PROPORCIONAL AO VALOR\n",
    "        # Ranking percentual: valor mais alto → rank mais alto → maior prob\n",
    "        ranks = pd.Series(values).rank(pct=True, na_option='bottom').values\n",
    "        \n",
    "        # Adicionar epsilon para evitar prob=0 (valores mais baixos ainda têm pequena chance)\n",
    "        probs = ranks + 0.01\n",
    "        probs = probs / probs.sum()\n",
    "        \n",
    "        # Índices ainda não missing nesta coluna\n",
    "        valid_idx = np.where(~pd.isna(data_missing.iloc[:, col_idx]))[0]\n",
    "        \n",
    "        if len(valid_idx) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Ajustar probabilidades para apenas índices válidos\n",
    "        valid_probs = probs[valid_idx]\n",
    "        valid_probs = valid_probs / valid_probs.sum()\n",
    "        \n",
    "        # Selecionar registos baseado na probabilidade (sem repetição)\n",
    "        n_to_select = min(n_missing, len(valid_idx))\n",
    "        selected = np.random.choice(valid_idx, size=n_to_select, replace=False, p=valid_probs)\n",
    "        \n",
    "        # Aplicar missing\n",
    "        for idx in selected:\n",
    "            data_missing.iloc[idx, col_idx] = np.nan\n",
    "    \n",
    "    return data_missing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390a1374-0f56-4137-9576-7d222a0698b6",
   "metadata": {},
   "source": [
    "##### Métricas de Avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47663006-af36-4d30-b082-a56c0890967e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(data_original: pd.DataFrame, \n",
    "                     data_imputed: pd.DataFrame, \n",
    "                     missing_mask: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Calcula métricas de avaliação para imputação.\n",
    "    Retorna tanto métricas agregadas quanto métricas por coluna.\n",
    "    \n",
    "    Args:\n",
    "        data_original: DataFrame original (ground truth)\n",
    "        data_imputed: DataFrame após imputação\n",
    "        missing_mask: Máscara booleana dos valores que eram missing\n",
    "    \n",
    "    Returns:\n",
    "        Dict com:\n",
    "        - R2, Pearson, NRMSE (numéricas) e Accuracy (categóricas) - AGREGADOS\n",
    "        - column_metrics: dict com métricas individuais por coluna\n",
    "    \"\"\"\n",
    "    numeric_cols = data_original.select_dtypes(include=[np.number]).columns\n",
    "    categorical_cols = data_original.select_dtypes(exclude=[np.number]).columns\n",
    "    \n",
    "    metrics = {\n",
    "        'R2': np.nan,\n",
    "        'Pearson': np.nan,\n",
    "        'NRMSE': np.nan,\n",
    "        'Accuracy': np.nan,\n",
    "        'column_metrics': {}\n",
    "    }\n",
    "    \n",
    "    # ===== MÉTRICAS NUMÉRICAS =====\n",
    "    r2_scores = []\n",
    "    pearson_scores = []\n",
    "    nrmse_scores = []\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        col_mask = missing_mask[col]\n",
    "        \n",
    "        if not col_mask.any():\n",
    "            continue\n",
    "        \n",
    "        orig_vals = data_original.loc[col_mask, col].values\n",
    "        imp_vals = data_imputed.loc[col_mask, col].values\n",
    "        \n",
    "        # Remover possíveis NaN remanescentes\n",
    "        valid = ~(pd.isna(orig_vals) | pd.isna(imp_vals))\n",
    "        orig_vals = orig_vals[valid]\n",
    "        imp_vals = imp_vals[valid]\n",
    "        \n",
    "        if len(orig_vals) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Inicializar métricas desta coluna\n",
    "        col_metrics = {\n",
    "            'type': 'numeric',\n",
    "            'R2': np.nan,\n",
    "            'NRMSE': np.nan,\n",
    "            'Accuracy': np.nan\n",
    "        }\n",
    "        \n",
    "        # R²\n",
    "        if np.var(orig_vals) > 1e-10:\n",
    "            col_r2 = r2_score(orig_vals, imp_vals)\n",
    "            r2_scores.append(col_r2)\n",
    "            col_metrics['R2'] = col_r2\n",
    "        \n",
    "        # Pearson\n",
    "        if len(orig_vals) >= 3 and np.var(orig_vals) > 1e-10 and np.var(imp_vals) > 1e-10:\n",
    "            pearson_scores.append(pearsonr(orig_vals, imp_vals)[0])\n",
    "        \n",
    "        # NRMSE\n",
    "        rmse = np.sqrt(mean_squared_error(orig_vals, imp_vals))\n",
    "        value_range = orig_vals.max() - orig_vals.min()\n",
    "        if value_range > 1e-10:\n",
    "            col_nrmse = rmse / value_range\n",
    "            nrmse_scores.append(col_nrmse)\n",
    "            col_metrics['NRMSE'] = col_nrmse\n",
    "        \n",
    "        metrics['column_metrics'][col] = col_metrics\n",
    "    \n",
    "    if r2_scores:\n",
    "        metrics['R2'] = np.mean(r2_scores)\n",
    "    if pearson_scores:\n",
    "        metrics['Pearson'] = np.mean(pearson_scores)\n",
    "    if nrmse_scores:\n",
    "        metrics['NRMSE'] = np.mean(nrmse_scores)\n",
    "    \n",
    "    # ===== MÉTRICAS CATEGÓRICAS =====\n",
    "    accuracy_scores = []\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        col_mask = missing_mask[col]\n",
    "        \n",
    "        if not col_mask.any():\n",
    "            continue\n",
    "        \n",
    "        orig_vals = data_original.loc[col_mask, col]\n",
    "        imp_vals = data_imputed.loc[col_mask, col]\n",
    "        \n",
    "        # Remover possíveis NaN remanescentes\n",
    "        valid = ~(orig_vals.isna() | imp_vals.isna())\n",
    "        if valid.sum() < 1:\n",
    "            continue\n",
    "        \n",
    "        orig_vals = orig_vals[valid]\n",
    "        imp_vals = imp_vals[valid]\n",
    "        \n",
    "        col_accuracy = (orig_vals == imp_vals).mean()\n",
    "        accuracy_scores.append(col_accuracy)\n",
    "        \n",
    "        # Guardar métricas desta coluna\n",
    "        metrics['column_metrics'][col] = {\n",
    "            'type': 'categorical',\n",
    "            'R2': np.nan,\n",
    "            'NRMSE': np.nan,\n",
    "            'Accuracy': col_accuracy\n",
    "        }\n",
    "    \n",
    "    if accuracy_scores:\n",
    "        metrics['Accuracy'] = np.mean(accuracy_scores)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "print(\"Função de cálculo de métricas definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47980dc-9a45-4a7d-89b7-bf74a3073a33",
   "metadata": {},
   "source": [
    "##### Testes Estatisticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d21ab9-80d0-4e7a-83ee-1015f174d102",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wilcoxon, friedmanchisquare\n",
    "from itertools import combinations\n",
    "\n",
    "def pairwise_wilcoxon(results_dict: dict, mechanism: str, missing_rate: float, \n",
    "                      metric: str = 'NRMSE', reference_method: str = 'ISCA-k') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Realiza testes de Wilcoxon signed-rank entre o método de referência e os outros.\n",
    "    \n",
    "    Args:\n",
    "        results_dict: Dicionário com resultados do benchmark\n",
    "        mechanism: 'MCAR', 'MAR' ou 'MNAR'\n",
    "        missing_rate: Missing rate específico (e.g., 0.30)\n",
    "        metric: Métrica a comparar ('R2', 'NRMSE', 'Pearson', 'Accuracy')\n",
    "        reference_method: Método de referência (default: 'ISCA-k')\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame com comparações e p-values\n",
    "    \"\"\"\n",
    "    comparisons = []\n",
    "    \n",
    "    # Obter scores do método de referência\n",
    "    key_ref = (mechanism, missing_rate, reference_method)\n",
    "    if key_ref not in results_dict:\n",
    "        print(f\"Método de referência {reference_method} não encontrado\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    ref_scores = [run[metric] for run in results_dict[key_ref] if not np.isnan(run[metric])]\n",
    "    \n",
    "    if len(ref_scores) < 2:\n",
    "        print(f\"Dados insuficientes para {reference_method}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Comparar com outros métodos\n",
    "    for method in METHOD_ORDER:\n",
    "        if method == reference_method:\n",
    "            continue\n",
    "        \n",
    "        key = (mechanism, missing_rate, method)\n",
    "        if key not in results_dict:\n",
    "            continue\n",
    "        \n",
    "        method_scores = [run[metric] for run in results_dict[key] if not np.isnan(run[metric])]\n",
    "        \n",
    "        if len(method_scores) < 2 or len(ref_scores) != len(method_scores):\n",
    "            continue\n",
    "        \n",
    "        # Wilcoxon signed-rank test\n",
    "        try:\n",
    "            statistic, p_value = wilcoxon(ref_scores, method_scores)\n",
    "            \n",
    "            # Determinar significância\n",
    "            if p_value < 0.001:\n",
    "                significance = \"***\"\n",
    "            elif p_value < 0.01:\n",
    "                significance = \"**\"\n",
    "            elif p_value < 0.05:\n",
    "                significance = \"*\"\n",
    "            else:\n",
    "                significance = \"n.s.\"\n",
    "            \n",
    "            # Calcular diferença média\n",
    "            diff = np.mean(ref_scores) - np.mean(method_scores)\n",
    "            \n",
    "            # Para NRMSE, menor é melhor (inverter interpretação)\n",
    "            if metric == 'NRMSE':\n",
    "                better = reference_method if diff < 0 else method\n",
    "            else:\n",
    "                better = reference_method if diff > 0 else method\n",
    "            \n",
    "            comparisons.append({\n",
    "                'Comparison': f\"{reference_method} vs {method}\",\n",
    "                'Reference_Mean': f\"{np.mean(ref_scores):.4f}\",\n",
    "                'Other_Mean': f\"{np.mean(method_scores):.4f}\",\n",
    "                'Difference': f\"{diff:.4f}\",\n",
    "                'p-value': f\"{p_value:.4f}\",\n",
    "                'Significance': significance,\n",
    "                'Better': better\n",
    "            })\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Erro no teste {reference_method} vs {method}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(comparisons)\n",
    "\n",
    "\n",
    "def display_statistical_tests(results_dict: dict, mechanism: str, missing_rate: float,\n",
    "                              dataset_name: str, metric: str = 'NRMSE'):\n",
    "    \"\"\"\n",
    "    Exibe tabela de testes estatísticos com Plotly.\n",
    "    \n",
    "    Args:\n",
    "        results_dict: Dicionário com resultados\n",
    "        mechanism: 'MCAR', 'MAR' ou 'MNAR'\n",
    "        missing_rate: Missing rate específico\n",
    "        dataset_name: Nome do dataset\n",
    "        metric: Métrica a analisar\n",
    "    \"\"\"\n",
    "    df = pairwise_wilcoxon(results_dict, mechanism, missing_rate, metric, reference_method='ISCA-k')\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        print(f\"Sem dados para testes estatísticos - {mechanism} {int(missing_rate*100)}%\")\n",
    "        return\n",
    "    \n",
    "    # Criar tabela Plotly\n",
    "    fig = go.Figure(data=[go.Table(\n",
    "        header=dict(\n",
    "            values=list(df.columns),\n",
    "            fill_color='#2E86AB',\n",
    "            font=dict(color='white', size=12, family='Arial'),\n",
    "            align='center',\n",
    "            height=35,\n",
    "            line_color='white'\n",
    "        ),\n",
    "        cells=dict(\n",
    "            values=[df[col] for col in df.columns],\n",
    "            fill_color=[['#f8f9fa', 'white'] * len(df)],\n",
    "            font=dict(size=11, family='Arial'),\n",
    "            align='center',\n",
    "            height=28,\n",
    "            line_color='#e0e0e0'\n",
    "        )\n",
    "    )])\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f\"{dataset_name} - {mechanism} {int(missing_rate*100)}% - Statistical Tests ({metric})\",\n",
    "        title_font_size=14,\n",
    "        height=min(400, 150 + len(df) * 30),\n",
    "        margin=dict(l=20, r=20, t=60, b=20)\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Interpretação textual\n",
    "    print(f\"\\nInterpretação dos resultados ({metric}):\")\n",
    "    print(\"Significância: *** p<0.001, ** p<0.01, * p<0.05, n.s. = não significativo\")\n",
    "    \n",
    "    significant_wins = df[df['Significance'] != 'n.s.']\n",
    "    if len(significant_wins) > 0:\n",
    "        print(f\"\\nISCA-k apresenta diferenças significativas em {len(significant_wins)}/{len(df)} comparações:\")\n",
    "        for _, row in significant_wins.iterrows():\n",
    "            print(f\"  - vs {row['Comparison'].split('vs')[1].strip()}: p={row['p-value']} {row['Significance']}\")\n",
    "    else:\n",
    "        print(\"\\nNenhuma diferença estatisticamente significativa detectada.\")\n",
    "\n",
    "\n",
    "def friedman_test_all_methods(results_dict: dict, mechanism: str, missing_rate: float,\n",
    "                               metric: str = 'NRMSE') -> dict:\n",
    "    \"\"\"\n",
    "    Realiza teste de Friedman para comparar múltiplos métodos.\n",
    "    \n",
    "    Args:\n",
    "        results_dict: Dicionário com resultados\n",
    "        mechanism: 'MCAR', 'MAR' ou 'MNAR'\n",
    "        missing_rate: Missing rate específico\n",
    "        metric: Métrica a analisar\n",
    "    \n",
    "    Returns:\n",
    "        Dict com estatística e p-value\n",
    "    \"\"\"\n",
    "    # Coletar scores de todos os métodos\n",
    "    all_scores = []\n",
    "    methods_with_data = []\n",
    "    \n",
    "    for method in METHOD_ORDER:\n",
    "        key = (mechanism, missing_rate, method)\n",
    "        if key not in results_dict:\n",
    "            continue\n",
    "        \n",
    "        scores = [run[metric] for run in results_dict[key] if not np.isnan(run[metric])]\n",
    "        \n",
    "        if len(scores) >= 2:\n",
    "            all_scores.append(scores)\n",
    "            methods_with_data.append(method)\n",
    "    \n",
    "    if len(all_scores) < 3:\n",
    "        return {'error': 'Dados insuficientes (necessário ≥3 métodos)'}\n",
    "    \n",
    "    # Verificar se todos têm o mesmo número de runs\n",
    "    if len(set(len(s) for s in all_scores)) > 1:\n",
    "        return {'error': 'Número diferente de runs entre métodos'}\n",
    "    \n",
    "    try:\n",
    "        statistic, p_value = friedmanchisquare(*all_scores)\n",
    "        \n",
    "        # Calcular rankings médios\n",
    "        # Transformar para array (n_runs x n_methods)\n",
    "        data_array = np.array(all_scores).T\n",
    "        \n",
    "        # Calcular ranks: menor valor = rank 1 para NRMSE, maior valor = rank 1 para R²\n",
    "        if metric == 'NRMSE':\n",
    "            # Menor é melhor: rank 1 para o menor valor\n",
    "            ranks = np.apply_along_axis(lambda x: np.argsort(np.argsort(x)) + 1, 1, data_array)\n",
    "        else:\n",
    "            # Maior é melhor (R², Pearson, Accuracy): rank 1 para o maior valor\n",
    "            ranks = np.apply_along_axis(lambda x: np.argsort(np.argsort(-x)) + 1, 1, data_array)\n",
    "        \n",
    "        mean_ranks = ranks.mean(axis=0)\n",
    "        \n",
    "        return {\n",
    "            'statistic': statistic,\n",
    "            'p_value': p_value,\n",
    "            'methods': methods_with_data,\n",
    "            'mean_ranks': mean_ranks,\n",
    "            'significant': p_value < 0.05\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {'error': str(e)}\n",
    "\n",
    "\n",
    "def display_friedman_results(results_dict: dict, mechanism: str, missing_rate: float,\n",
    "                             dataset_name: str, metric: str = 'NRMSE'):\n",
    "    \"\"\"\n",
    "    Exibe resultados do teste de Friedman.\n",
    "    \n",
    "    Args:\n",
    "        results_dict: Dicionário com resultados\n",
    "        mechanism: 'MCAR', 'MAR' ou 'MNAR'\n",
    "        missing_rate: Missing rate específico\n",
    "        dataset_name: Nome do dataset\n",
    "        metric: Métrica a analisar\n",
    "    \"\"\"\n",
    "    result = friedman_test_all_methods(results_dict, mechanism, missing_rate, metric)\n",
    "    \n",
    "    if 'error' in result:\n",
    "        print(f\"Teste de Friedman: {result['error']}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TESTE DE FRIEDMAN - {dataset_name}\")\n",
    "    print(f\"{mechanism} {int(missing_rate*100)}% - {metric}\")\n",
    "    print('='*80)\n",
    "    \n",
    "    print(f\"\\nEstatística: {result['statistic']:.4f}\")\n",
    "    print(f\"p-value: {result['p_value']:.4f}\")\n",
    "    \n",
    "    if result['significant']:\n",
    "        print(\"Resultado: DIFERENÇAS SIGNIFICATIVAS entre métodos (p < 0.05)\")\n",
    "    else:\n",
    "        print(\"Resultado: Sem diferenças significativas entre métodos\")\n",
    "    \n",
    "    print(\"\\nRanking médio (1=melhor):\")\n",
    "    for method, rank in zip(result['methods'], result['mean_ranks']):\n",
    "        print(f\"  {method}: {rank:.2f}\")\n",
    "    \n",
    "    # Ordenar por ranking\n",
    "    sorted_methods = sorted(zip(result['methods'], result['mean_ranks']), key=lambda x: x[1])\n",
    "    print(f\"\\nOrdem: {' > '.join([m for m, r in sorted_methods])}\")\n",
    "\n",
    "\n",
    "print(\"Funções de testes estatísticos carregadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2605e2-da75-4417-920e-773f3b8a3fb6",
   "metadata": {},
   "source": [
    "##### Tabelas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff8321e-ed4a-44ee-9830-865754ddc9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results_table(results_dict: dict, mechanism: str, dataset_name: str):\n",
    "    table_data = []\n",
    "\n",
    "    for missing_rate in MISSING_RATES:\n",
    "        for method in METHOD_ORDER:\n",
    "            key = (mechanism, missing_rate, method)\n",
    "            if key not in results_dict:\n",
    "                continue\n",
    "\n",
    "            runs = results_dict[key]\n",
    "\n",
    "            r2_vals = [r['R2'] for r in runs if not np.isnan(r['R2'])]\n",
    "            pearson_vals = [r['Pearson'] for r in runs if not np.isnan(r['Pearson'])]\n",
    "            nrmse_vals = [r['NRMSE'] for r in runs if not np.isnan(r['NRMSE'])]\n",
    "            accuracy_vals = [r['Accuracy'] for r in runs if not np.isnan(r['Accuracy'])]\n",
    "            time_vals = [r['Time'] for r in runs]\n",
    "\n",
    "            row = {\n",
    "                'Missing Rate': f\"{int(missing_rate*100)}%\",\n",
    "                'Method': method,\n",
    "                'R2_mean': np.mean(r2_vals) if r2_vals else np.nan,\n",
    "                'R2_std': np.std(r2_vals) if r2_vals else np.nan,\n",
    "                'Pearson_mean': np.mean(pearson_vals) if pearson_vals else np.nan,\n",
    "                'Pearson_std': np.std(pearson_vals) if pearson_vals else np.nan,\n",
    "                'NRMSE_mean': np.mean(nrmse_vals) if nrmse_vals else np.nan,\n",
    "                'NRMSE_std': np.std(nrmse_vals) if nrmse_vals else np.nan,\n",
    "                'Accuracy_mean': np.mean(accuracy_vals) if accuracy_vals else np.nan,\n",
    "                'Accuracy_std': np.std(accuracy_vals) if accuracy_vals else np.nan,\n",
    "                'Time_mean': np.mean(time_vals),\n",
    "                'Time_std': np.std(time_vals)\n",
    "            }\n",
    "\n",
    "            table_data.append(row)\n",
    "\n",
    "    if not table_data:\n",
    "        print(f\"Sem dados para {mechanism}\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(table_data)\n",
    "\n",
    "    # Criar coluna formatada final\n",
    "    df_display = pd.DataFrame({\n",
    "        'Missing Rate': df['Missing Rate'],\n",
    "        'Method': df['Method'],\n",
    "        'R²': df.apply(lambda r: f\"{r.R2_mean:.3f}±{r.R2_std:.3f}\" if not np.isnan(r.R2_mean) else \"N/A\", axis=1),\n",
    "        'Pearson': df.apply(lambda r: f\"{r.Pearson_mean:.3f}±{r.Pearson_std:.3f}\" if not np.isnan(r.Pearson_mean) else \"N/A\", axis=1),\n",
    "        'NRMSE': df.apply(lambda r: f\"{r.NRMSE_mean:.4f}±{r.NRMSE_std:.4f}\" if not np.isnan(r.NRMSE_mean) else \"N/A\", axis=1),\n",
    "        'Accuracy': df.apply(lambda r: f\"{r.Accuracy_mean:.3f}±{r.Accuracy_std:.3f}\" if not np.isnan(r.Accuracy_mean) else \"N/A\", axis=1),\n",
    "        'Time': df.apply(lambda r: f\"{r.Time_mean:.2f}±{r.Time_std:.2f}\", axis=1)\n",
    "    })\n",
    "\n",
    "    # Encontrar melhores valores\n",
    "    best = {}\n",
    "    for rate in df[\"Missing Rate\"].unique():\n",
    "        subset = df[df[\"Missing Rate\"] == rate]\n",
    "        best[rate] = {\n",
    "            \"R2\": subset[\"R2_mean\"].max(),\n",
    "            \"Pearson\": subset[\"Pearson_mean\"].max(),\n",
    "            \"NRMSE\": subset[\"NRMSE_mean\"].min(),\n",
    "            \"Accuracy\": subset[\"Accuracy_mean\"].max(),\n",
    "            \"Time\": subset[\"Time_mean\"].min()\n",
    "        }\n",
    "\n",
    "    # Styler: bold e alternância de cor\n",
    "    def highlight_best(row):\n",
    "        rate = row[\"Missing Rate\"]\n",
    "        b = best[rate]\n",
    "\n",
    "        styles = [''] * len(df_display.columns)\n",
    "\n",
    "        if df.loc[row.name, \"R2_mean\"] == b[\"R2\"]:\n",
    "            styles[2] = 'font-weight: bold'\n",
    "        if df.loc[row.name, \"Pearson_mean\"] == b[\"Pearson\"]:\n",
    "            styles[3] = 'font-weight: bold'\n",
    "        if df.loc[row.name, \"NRMSE_mean\"] == b[\"NRMSE\"]:\n",
    "            styles[4] = 'font-weight: bold'\n",
    "        if df.loc[row.name, \"Accuracy_mean\"] == b[\"Accuracy\"]:\n",
    "            styles[5] = 'font-weight: bold'\n",
    "        if df.loc[row.name, \"Time_mean\"] == b[\"Time\"]:\n",
    "            styles[6] = 'font-weight: bold'\n",
    "\n",
    "        return styles\n",
    "\n",
    "    def alternate(row):\n",
    "        # cor escura para 10% e 30%\n",
    "        if row[\"Missing Rate\"] in [\"10%\", \"30%\"]:\n",
    "            return ['background-color: #e6e6e6'] * len(df_display.columns)\n",
    "        else:\n",
    "            return [''] * len(df_display.columns)\n",
    "\n",
    "    styled = (\n",
    "        df_display.style\n",
    "        .apply(alternate, axis=1)\n",
    "        .apply(highlight_best, axis=1)\n",
    "    )\n",
    "\n",
    "    display(styled)\n",
    "\n",
    "    # Exportar para Excel\n",
    "    df_display.to_excel(f\"{dataset_name}_{mechanism}_agregada.xlsx\", index=False)\n",
    "\n",
    "def display_column_table(results_dict: dict, mechanism: str, dataset_name: str, missing_rate: float = 0.30):\n",
    "    table_data = []\n",
    "\n",
    "    for method in METHOD_ORDER:\n",
    "        key = (mechanism, missing_rate, method)\n",
    "        if key not in results_dict:\n",
    "            continue\n",
    "\n",
    "        runs = results_dict[key]\n",
    "        if \"column_metrics\" not in runs[0]:\n",
    "            continue\n",
    "\n",
    "        for col, info in runs[0][\"column_metrics\"].items():\n",
    "            r2_vals, nrmse_vals, acc_vals = [], [], []\n",
    "\n",
    "            for run in runs:\n",
    "                if col not in run[\"column_metrics\"]:\n",
    "                    continue\n",
    "                cm = run[\"column_metrics\"][col]\n",
    "                if not np.isnan(cm[\"R2\"]): r2_vals.append(cm[\"R2\"])\n",
    "                if not np.isnan(cm[\"NRMSE\"]): nrmse_vals.append(cm[\"NRMSE\"])\n",
    "                if not np.isnan(cm[\"Accuracy\"]): acc_vals.append(cm[\"Accuracy\"])\n",
    "\n",
    "            table_data.append({\n",
    "                \"Coluna\": col,\n",
    "                \"Tipo\": info[\"type\"],\n",
    "                \"Method\": method,\n",
    "                \"R2_mean\": np.mean(r2_vals) if r2_vals else np.nan,\n",
    "                \"R2_std\": np.std(r2_vals) if r2_vals else np.nan,\n",
    "                \"NRMSE_mean\": np.mean(nrmse_vals) if nrmse_vals else np.nan,\n",
    "                \"NRMSE_std\": np.std(nrmse_vals) if nrmse_vals else np.nan,\n",
    "                \"Accuracy_mean\": np.mean(acc_vals) if acc_vals else np.nan,\n",
    "                \"Accuracy_std\": np.std(acc_vals) if acc_vals else np.nan\n",
    "            })\n",
    "\n",
    "    if not table_data:\n",
    "        print(f\"Sem dados para {mechanism} - {int(missing_rate*100)}%\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(table_data)\n",
    "\n",
    "    df_display = pd.DataFrame({\n",
    "        \"Coluna\": df[\"Coluna\"],\n",
    "        \"Tipo\": df[\"Tipo\"],\n",
    "        \"Method\": df[\"Method\"],\n",
    "        \"R²\": df.apply(lambda r: f\"{r.R2_mean:.3f}±{r.R2_std:.3f}\" if not np.isnan(r.R2_mean) else \"N/A\", axis=1),\n",
    "        \"NRMSE\": df.apply(lambda r: f\"{r.NRMSE_mean:.4f}±{r.NRMSE_std:.4f}\" if not np.isnan(r.NRMSE_mean) else \"N/A\", axis=1),\n",
    "        \"Accuracy\": df.apply(lambda r: f\"{r.Accuracy_mean:.3f}±{r.Accuracy_std:.3f}\" if not np.isnan(r.Accuracy_mean) else \"N/A\", axis=1)\n",
    "    })\n",
    "\n",
    "    # melhores valores por coluna\n",
    "    best = {}\n",
    "    for col in df[\"Coluna\"].unique():\n",
    "        sub = df[df[\"Coluna\"] == col]\n",
    "        best[col] = {\n",
    "            \"R2\": sub[\"R2_mean\"].max(),\n",
    "            \"NRMSE\": sub[\"NRMSE_mean\"].min(),\n",
    "            \"Accuracy\": sub[\"Accuracy_mean\"].max()\n",
    "        }\n",
    "\n",
    "    def highlight(row):\n",
    "        col = row[\"Coluna\"]\n",
    "        b = best[col]\n",
    "\n",
    "        styles = [''] * len(df_display.columns)\n",
    "        if df.loc[row.name, \"R2_mean\"] == b[\"R2\"]:\n",
    "            styles[3] = \"font-weight: bold\"\n",
    "        if df.loc[row.name, \"NRMSE_mean\"] == b[\"NRMSE\"]:\n",
    "            styles[4] = \"font-weight: bold\"\n",
    "        if df.loc[row.name, \"Accuracy_mean\"] == b[\"Accuracy\"]:\n",
    "            styles[5] = \"font-weight: bold\"\n",
    "        return styles\n",
    "\n",
    "    def alternate_columns(row):\n",
    "        idx = list(df_display[\"Coluna\"].unique()).index(row[\"Coluna\"])\n",
    "        if idx % 2 == 0:\n",
    "            return [\"background-color: #e6e6e6\"] * len(df_display.columns)\n",
    "        return [\"\"] * len(df_display.columns)\n",
    "\n",
    "    styled = df_display.style.apply(alternate_columns, axis=1).apply(highlight, axis=1)\n",
    "    display(styled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4267ae-d03c-4f71-86f1-2bc9ffbbf247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results_table2(results_dict: dict, mechanism: str, dataset_name: str):\n",
    "    \"\"\"\n",
    "    Gera e renderiza tabela de resultados agregados com Plotly.\n",
    "    \n",
    "    Args:\n",
    "        results_dict: Dicionário com resultados do benchmark\n",
    "        mechanism: 'MCAR', 'MAR' ou 'MNAR'\n",
    "        dataset_name: Nome do dataset\n",
    "    \"\"\"\n",
    "    # Preparar dados\n",
    "    table_data = []\n",
    "    \n",
    "    for missing_rate in MISSING_RATES:\n",
    "        for method in METHOD_ORDER:\n",
    "            key = (mechanism, missing_rate, method)\n",
    "            \n",
    "            if key not in results_dict:\n",
    "                continue\n",
    "            \n",
    "            runs = results_dict[key]\n",
    "            \n",
    "            # Agregar as 3 runs\n",
    "            r2_vals = [r['R2'] for r in runs if not np.isnan(r['R2'])]\n",
    "            pearson_vals = [r['Pearson'] for r in runs if not np.isnan(r['Pearson'])]\n",
    "            nrmse_vals = [r['NRMSE'] for r in runs if not np.isnan(r['NRMSE'])]\n",
    "            accuracy_vals = [r['Accuracy'] for r in runs if not np.isnan(r['Accuracy'])]\n",
    "            time_vals = [r['Time'] for r in runs]\n",
    "            \n",
    "            row = {\n",
    "                'Missing Rate': f\"{int(missing_rate*100)}%\",\n",
    "                'Method': method,\n",
    "                'R²_mean': np.mean(r2_vals) if r2_vals else np.nan,\n",
    "                'R²_std': np.std(r2_vals) if r2_vals else np.nan,\n",
    "                'Pearson_mean': np.mean(pearson_vals) if pearson_vals else np.nan,\n",
    "                'Pearson_std': np.std(pearson_vals) if pearson_vals else np.nan,\n",
    "                'NRMSE_mean': np.mean(nrmse_vals) if nrmse_vals else np.nan,\n",
    "                'NRMSE_std': np.std(nrmse_vals) if nrmse_vals else np.nan,\n",
    "                'Accuracy_mean': np.mean(accuracy_vals) if accuracy_vals else np.nan,\n",
    "                'Accuracy_std': np.std(accuracy_vals) if accuracy_vals else np.nan,\n",
    "                'Time_mean': np.mean(time_vals),\n",
    "                'Time_std': np.std(time_vals)\n",
    "            }\n",
    "            \n",
    "            table_data.append(row)\n",
    "    \n",
    "    if len(table_data) == 0:\n",
    "        print(f\"Sem dados para {mechanism}\")\n",
    "        return\n",
    "    \n",
    "    df = pd.DataFrame(table_data)\n",
    "    \n",
    "    # Formatar valores para exibição\n",
    "    display_data = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        display_row = {\n",
    "            'Missing Rate': row['Missing Rate'],\n",
    "            'Method': row['Method'],\n",
    "            'R²': f\"{row['R²_mean']:.3f}±{row['R²_std']:.3f}\" if not np.isnan(row['R²_mean']) else \"N/A\",\n",
    "            'Pearson': f\"{row['Pearson_mean']:.3f}±{row['Pearson_std']:.3f}\" if not np.isnan(row['Pearson_mean']) else \"N/A\",\n",
    "            'NRMSE': f\"{row['NRMSE_mean']:.4f}±{row['NRMSE_std']:.4f}\" if not np.isnan(row['NRMSE_mean']) else \"N/A\",\n",
    "            'Accuracy': f\"{row['Accuracy_mean']:.3f}±{row['Accuracy_std']:.3f}\" if not np.isnan(row['Accuracy_mean']) else \"N/A\",\n",
    "            'Time': f\"{row['Time_mean']:.2f}±{row['Time_std']:.2f}\"\n",
    "        }\n",
    "        display_data.append(display_row)\n",
    "    \n",
    "    df_display = pd.DataFrame(display_data)\n",
    "    \n",
    "    # Identificar melhores valores por missing rate\n",
    "    best_values = {}\n",
    "    for missing_rate in df['Missing Rate'].unique():\n",
    "        df_rate = df[df['Missing Rate'] == missing_rate]\n",
    "        best_values[missing_rate] = {\n",
    "            'R²': df_rate['R²_mean'].max() if not df_rate['R²_mean'].isna().all() else None,\n",
    "            'Pearson': df_rate['Pearson_mean'].max() if not df_rate['Pearson_mean'].isna().all() else None,\n",
    "            'NRMSE': df_rate['NRMSE_mean'].min() if not df_rate['NRMSE_mean'].isna().all() else None,\n",
    "            'Accuracy': df_rate['Accuracy_mean'].max() if not df_rate['Accuracy_mean'].isna().all() else None,\n",
    "            'Time': df_rate['Time_mean'].min()\n",
    "        }\n",
    "    \n",
    "    # Aplicar bold aos melhores valores\n",
    "    for idx, row in df.iterrows():\n",
    "        missing_rate = row['Missing Rate']\n",
    "        \n",
    "        if not np.isnan(row['R²_mean']) and best_values[missing_rate]['R²'] is not None:\n",
    "            if abs(row['R²_mean'] - best_values[missing_rate]['R²']) < 1e-6:\n",
    "                df_display.at[idx, 'R²'] = f\"<b>{df_display.at[idx, 'R²']}</b>\"\n",
    "        \n",
    "        if not np.isnan(row['Pearson_mean']) and best_values[missing_rate]['Pearson'] is not None:\n",
    "            if abs(row['Pearson_mean'] - best_values[missing_rate]['Pearson']) < 1e-6:\n",
    "                df_display.at[idx, 'Pearson'] = f\"<b>{df_display.at[idx, 'Pearson']}</b>\"\n",
    "        \n",
    "        if not np.isnan(row['NRMSE_mean']) and best_values[missing_rate]['NRMSE'] is not None:\n",
    "            if abs(row['NRMSE_mean'] - best_values[missing_rate]['NRMSE']) < 1e-6:\n",
    "                df_display.at[idx, 'NRMSE'] = f\"<b>{df_display.at[idx, 'NRMSE']}</b>\"\n",
    "        \n",
    "        if not np.isnan(row['Accuracy_mean']) and best_values[missing_rate]['Accuracy'] is not None:\n",
    "            if abs(row['Accuracy_mean'] - best_values[missing_rate]['Accuracy']) < 1e-6:\n",
    "                df_display.at[idx, 'Accuracy'] = f\"<b>{df_display.at[idx, 'Accuracy']}</b>\"\n",
    "        \n",
    "        if abs(row['Time_mean'] - best_values[missing_rate]['Time']) < 1e-6:\n",
    "            df_display.at[idx, 'Time'] = f\"<b>{df_display.at[idx, 'Time']}</b>\"\n",
    "    \n",
    "    # Criar cores alternadas por missing rate\n",
    "    fill_colors = []\n",
    "    for _, row in df_display.iterrows():\n",
    "        if row['Missing Rate'] in ['10%', '30%']:\n",
    "            fill_colors.append('#d9d9d9')  # Cinzento escuro\n",
    "        else:\n",
    "            fill_colors.append('white')\n",
    "    \n",
    "    # Criar tabela Plotly\n",
    "    fig = go.Figure(data=[go.Table(\n",
    "        header=dict(\n",
    "            values=list(df_display.columns),\n",
    "            fill_color='#2E86AB',\n",
    "            font=dict(color='white', size=12, family='Arial'),\n",
    "            align='center',\n",
    "            height=35,\n",
    "            line_color='white'\n",
    "        ),\n",
    "        cells=dict(\n",
    "            values=[df_display[col] for col in df_display.columns],\n",
    "            fill_color=[fill_colors],  # Usa lista de cores por missing rate\n",
    "            font=dict(size=11, family='Arial'),\n",
    "            align='center',\n",
    "            height=28,\n",
    "            line_color='#808080',  # Linha cinzenta entre células\n",
    "            line_width=1\n",
    "        )\n",
    "    )])\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f\"{dataset_name} - {mechanism}\",\n",
    "        title_font_size=14,\n",
    "        height=min(400, 150 + len(df_display) * 30),\n",
    "        margin=dict(l=20, r=20, t=60, b=20)\n",
    "    )\n",
    "    \n",
    "    df_display.to_excel(f'{dataset_name}_{mechanism}_agregada.xlsx', index=False)\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def display_column_table2(results_dict: dict, mechanism: str, dataset_name: str, missing_rate: float = 0.30):\n",
    "    \"\"\"\n",
    "    Gera e renderiza tabela detalhada por coluna com Plotly.\n",
    "    \n",
    "    Args:\n",
    "        results_dict: Dicionário com resultados do benchmark\n",
    "        mechanism: 'MCAR', 'MAR' ou 'MNAR'\n",
    "        dataset_name: Nome do dataset\n",
    "        missing_rate: Missing rate específico\n",
    "    \"\"\"\n",
    "    # Preparar dados\n",
    "    table_data = []\n",
    "    \n",
    "    for method in METHOD_ORDER:\n",
    "        key = (mechanism, missing_rate, method)\n",
    "        \n",
    "        if key not in results_dict:\n",
    "            continue\n",
    "        \n",
    "        runs = results_dict[key]\n",
    "        \n",
    "        if 'column_metrics' not in runs[0]:\n",
    "            continue\n",
    "        \n",
    "        column_metrics = runs[0]['column_metrics']\n",
    "        \n",
    "        for col_name, col_info in column_metrics.items():\n",
    "            col_type = col_info['type']\n",
    "            \n",
    "            r2_vals = []\n",
    "            nrmse_vals = []\n",
    "            accuracy_vals = []\n",
    "            \n",
    "            for run in runs:\n",
    "                if col_name in run['column_metrics']:\n",
    "                    cm = run['column_metrics'][col_name]\n",
    "                    if not np.isnan(cm['R2']):\n",
    "                        r2_vals.append(cm['R2'])\n",
    "                    if not np.isnan(cm['NRMSE']):\n",
    "                        nrmse_vals.append(cm['NRMSE'])\n",
    "                    if not np.isnan(cm['Accuracy']):\n",
    "                        accuracy_vals.append(cm['Accuracy'])\n",
    "            \n",
    "            row = {\n",
    "                'Coluna': col_name,\n",
    "                'Tipo': col_type,\n",
    "                'Method': method,\n",
    "                'R²_mean': np.mean(r2_vals) if r2_vals else np.nan,\n",
    "                'R²_std': np.std(r2_vals) if r2_vals else np.nan,\n",
    "                'NRMSE_mean': np.mean(nrmse_vals) if nrmse_vals else np.nan,\n",
    "                'NRMSE_std': np.std(nrmse_vals) if nrmse_vals else np.nan,\n",
    "                'Accuracy_mean': np.mean(accuracy_vals) if accuracy_vals else np.nan,\n",
    "                'Accuracy_std': np.std(accuracy_vals) if accuracy_vals else np.nan\n",
    "            }\n",
    "            \n",
    "            table_data.append(row)\n",
    "    \n",
    "    if len(table_data) == 0:\n",
    "        print(f\"Sem dados para {mechanism} - {int(missing_rate*100)}%\")\n",
    "        return\n",
    "    \n",
    "    df = pd.DataFrame(table_data)\n",
    "    \n",
    "    # Formatar valores para exibição\n",
    "    display_data = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        display_row = {\n",
    "            'Coluna': row['Coluna'],\n",
    "            'Tipo': row['Tipo'],\n",
    "            'Method': row['Method'],\n",
    "            'R²': f\"{row['R²_mean']:.3f}±{row['R²_std']:.3f}\" if not np.isnan(row['R²_mean']) else \"N/A\",\n",
    "            'NRMSE': f\"{row['NRMSE_mean']:.4f}±{row['NRMSE_std']:.4f}\" if not np.isnan(row['NRMSE_mean']) else \"N/A\",\n",
    "            'Accuracy': f\"{row['Accuracy_mean']:.3f}±{row['Accuracy_std']:.3f}\" if not np.isnan(row['Accuracy_mean']) else \"N/A\"\n",
    "        }\n",
    "        display_data.append(display_row)\n",
    "    \n",
    "    df_display = pd.DataFrame(display_data)\n",
    "    \n",
    "    # Identificar melhores valores por coluna\n",
    "    best_values = {}\n",
    "    for col_name in df['Coluna'].unique():\n",
    "        df_col = df[df['Coluna'] == col_name]\n",
    "        best_values[col_name] = {\n",
    "            'R²': df_col['R²_mean'].max() if not df_col['R²_mean'].isna().all() else None,\n",
    "            'NRMSE': df_col['NRMSE_mean'].min() if not df_col['NRMSE_mean'].isna().all() else None,\n",
    "            'Accuracy': df_col['Accuracy_mean'].max() if not df_col['Accuracy_mean'].isna().all() else None\n",
    "        }\n",
    "    \n",
    "    # Aplicar bold aos melhores valores\n",
    "    for idx, row in df.iterrows():\n",
    "        col_name = row['Coluna']\n",
    "        \n",
    "        if not np.isnan(row['R²_mean']) and best_values[col_name]['R²'] is not None:\n",
    "            if abs(row['R²_mean'] - best_values[col_name]['R²']) < 1e-6:\n",
    "                df_display.at[idx, 'R²'] = f\"<b>{df_display.at[idx, 'R²']}</b>\"\n",
    "        \n",
    "        if not np.isnan(row['NRMSE_mean']) and best_values[col_name]['NRMSE'] is not None:\n",
    "            if abs(row['NRMSE_mean'] - best_values[col_name]['NRMSE']) < 1e-6:\n",
    "                df_display.at[idx, 'NRMSE'] = f\"<b>{df_display.at[idx, 'NRMSE']}</b>\"\n",
    "        \n",
    "        if not np.isnan(row['Accuracy_mean']) and best_values[col_name]['Accuracy'] is not None:\n",
    "            if abs(row['Accuracy_mean'] - best_values[col_name]['Accuracy']) < 1e-6:\n",
    "                df_display.at[idx, 'Accuracy'] = f\"<b>{df_display.at[idx, 'Accuracy']}</b>\"\n",
    "    \n",
    "    # Criar tabela Plotly\n",
    "    unique_columns = df_display['Coluna'].unique()\n",
    "    \n",
    "    # Criar cores alternadas por coluna\n",
    "    fill_colors = []\n",
    "    for _, row in df_display.iterrows():\n",
    "        col_idx = list(unique_columns).index(row['Coluna'])\n",
    "        if col_idx % 2 == 0:\n",
    "            fill_colors.append('#d9d9d9')  # Cinzento escuro para colunas pares\n",
    "        else:\n",
    "            fill_colors.append('white')  # Branco para colunas ímpares\n",
    "    \n",
    "    # Criar tabela Plotly\n",
    "    fig = go.Figure(data=[go.Table(\n",
    "        header=dict(\n",
    "            values=list(df_display.columns),\n",
    "            fill_color='#2E86AB',\n",
    "            font=dict(color='white', size=12, family='Arial'),\n",
    "            align='center',\n",
    "            height=35,\n",
    "            line_color='white'\n",
    "        ),\n",
    "        cells=dict(\n",
    "            values=[df_display[col] for col in df_display.columns],\n",
    "            fill_color=[fill_colors],  # Usa lista de cores por missing rate\n",
    "            font=dict(size=11, family='Arial'),\n",
    "            align='center',\n",
    "            height=28,\n",
    "            line_color='#808080',  # Linha cinzenta entre células\n",
    "            line_width=1\n",
    "        )\n",
    "    )])\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f\"{dataset_name} - {mechanism} - {int(missing_rate*100)}% (Detalhes por Coluna)\",\n",
    "        title_font_size=14,\n",
    "        height=min(600, 150 + len(df_display) * 30),\n",
    "        margin=dict(l=20, r=20, t=60, b=20)\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "\n",
    "print(\"Funções de geração e exibição de tabelas definidas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3d0313-37eb-47a6-9f1b-d0dca2c54b0e",
   "metadata": {},
   "source": [
    "##### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67265f46-5830-4da3-9e20-5beb0d68e423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_degradation_curves(results_dict: dict, dataset_name: str, metric: str = 'NRMSE'):\n",
    "    \"\"\"\n",
    "    Plota curvas de degradação para os 3 mecanismos.\n",
    "    \n",
    "    Args:\n",
    "        results_dict: Dicionário com resultados do benchmark\n",
    "        dataset_name: Nome do dataset\n",
    "        metric: Métrica a plotar ('NRMSE', 'R2', 'Pearson', 'Accuracy')\n",
    "    \"\"\"\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=3,\n",
    "        subplot_titles=('MCAR', 'MAR', 'MNAR'),\n",
    "        horizontal_spacing=0.1\n",
    "    )\n",
    "    \n",
    "    for idx, mechanism in enumerate(MECHANISMS, 1):\n",
    "        for method in METHOD_ORDER:\n",
    "            x_vals = []\n",
    "            y_means = []\n",
    "            y_stds = []\n",
    "            \n",
    "            for missing_rate in MISSING_RATES:\n",
    "                key = (mechanism, missing_rate, method)\n",
    "                \n",
    "                if key not in results_dict:\n",
    "                    continue\n",
    "                \n",
    "                runs = results_dict[key]\n",
    "                metric_vals = [r[metric] for r in runs if not np.isnan(r[metric])]\n",
    "                \n",
    "                if not metric_vals:\n",
    "                    continue\n",
    "                \n",
    "                x_vals.append(missing_rate * 100)\n",
    "                y_means.append(np.mean(metric_vals))\n",
    "                y_stds.append(np.std(metric_vals))\n",
    "            \n",
    "            if not x_vals:\n",
    "                continue\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=x_vals,\n",
    "                    y=y_means,\n",
    "                    error_y=dict(\n",
    "                        type='data',\n",
    "                        array=y_stds,\n",
    "                        visible=True,\n",
    "                        thickness=1.5,\n",
    "                        width=3\n",
    "                    ),\n",
    "                    mode='lines+markers',\n",
    "                    name=method,\n",
    "                    line=dict(width=2.5, color=METHOD_COLORS[method]),\n",
    "                    marker=dict(size=8),\n",
    "                    showlegend=(idx == 1),\n",
    "                    legendgroup=method\n",
    "                ),\n",
    "                row=1, col=idx\n",
    "            )\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Missing Rate (%)\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Missing Rate (%)\", row=1, col=2)\n",
    "    fig.update_xaxes(title_text=\"Missing Rate (%)\", row=1, col=3)\n",
    "    fig.update_yaxes(title_text=metric, row=1, col=1)\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title_text=f\"Curvas de Degradação - {dataset_name}\",\n",
    "        title_font_size=16,\n",
    "        height=400,\n",
    "        hovermode='x unified',\n",
    "        template='plotly_white',\n",
    "        font=dict(size=11),\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=-0.25,\n",
    "            xanchor=\"center\",\n",
    "            x=0.5\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def plot_method_comparison(results_dict: dict, dataset_name: str):\n",
    "    \"\"\"\n",
    "    Plota comparação de NRMSE médio entre métodos.\n",
    "    \n",
    "    Args:\n",
    "        results_dict: Dicionário com resultados do benchmark\n",
    "        dataset_name: Nome do dataset\n",
    "    \"\"\"\n",
    "    method_means = []\n",
    "    method_stds = []\n",
    "    \n",
    "    for method in METHOD_ORDER:\n",
    "        all_nrmse = []\n",
    "        \n",
    "        for mechanism in MECHANISMS:\n",
    "            for missing_rate in MISSING_RATES:\n",
    "                key = (mechanism, missing_rate, method)\n",
    "                \n",
    "                if key not in results_dict:\n",
    "                    continue\n",
    "                \n",
    "                runs = results_dict[key]\n",
    "                nrmse_vals = [r['NRMSE'] for r in runs if not np.isnan(r['NRMSE'])]\n",
    "                all_nrmse.extend(nrmse_vals)\n",
    "        \n",
    "        if all_nrmse:\n",
    "            method_means.append(np.mean(all_nrmse))\n",
    "            method_stds.append(np.std(all_nrmse))\n",
    "        else:\n",
    "            method_means.append(np.nan)\n",
    "            method_stds.append(0)\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(go.Bar(\n",
    "        x=METHOD_ORDER,\n",
    "        y=method_means,\n",
    "        error_y=dict(\n",
    "            type='data',\n",
    "            array=method_stds,\n",
    "            visible=True,\n",
    "            thickness=1.5,\n",
    "            width=5\n",
    "        ),\n",
    "        marker_color=[METHOD_COLORS[m] for m in METHOD_ORDER],\n",
    "        text=[f\"{v:.4f}\" if not np.isnan(v) else \"N/A\" for v in method_means],\n",
    "        textposition='outside',\n",
    "        hovertemplate='%{x}<br>NRMSE: %{y:.4f}±%{error_y.array:.4f}<extra></extra>'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f\"Comparação de Métodos - {dataset_name} (NRMSE Médio)\",\n",
    "        title_font_size=16,\n",
    "        xaxis_title=\"Método\",\n",
    "        yaxis_title=\"NRMSE\",\n",
    "        height=450,\n",
    "        template='plotly_white',\n",
    "        font=dict(size=12),\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "\n",
    "print(\"Funções de visualização definidas\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PARTE 1 COMPLETA - Setup e Funções Comuns\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4368b9ae-ab13-45b3-9136-31ee1baacf78",
   "metadata": {},
   "source": [
    "### Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed565a18-060e-41ca-9019-e1ba2741736a",
   "metadata": {},
   "source": [
    "#### Datasets Numéricos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ffafea-25a6-4a2b-aa17-dc277d055ec4",
   "metadata": {},
   "source": [
    "##### IRIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ddad15-69f3-4a4f-8986-41af425df1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dataset\n",
    "iris_data = load_iris()\n",
    "data_iris = pd.DataFrame(iris_data.data, columns=iris_data.feature_names)\n",
    "\n",
    "print(f\"\\nShape original: {data_iris.shape}\")\n",
    "print(f\"Tipos de dados:\\n{data_iris.dtypes}\")\n",
    "print(f\"\\nMissings nativos: {data_iris.isna().sum().sum()}\")\n",
    "\n",
    "# Verificar que não há missings nativos\n",
    "assert data_iris.isna().sum().sum() == 0, \"Dataset tem missings nativos!\"\n",
    "\n",
    "print(\"\\nDataset completo e limpo\")\n",
    "print(f\"Shape final: {data_iris.shape}\")\n",
    "print(f\"\\nPrimeiras linhas:\")\n",
    "print(data_iris.head())\n",
    "print(f\"\\nEstatísticas descritivas:\")\n",
    "print(data_iris.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95a5066-9223-4a8e-a7c4-4d99f5ab4408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_iscak_iris(data_missing):\n",
    "    \"\"\"ISCA-k para IRIS (numérico puro).\"\"\"\n",
    "    imputer = ISCAkCore(verbose=True)\n",
    "    result = imputer.impute(\n",
    "        data_missing,\n",
    "        force_categorical=None,\n",
    "        force_ordinal=None,\n",
    "        interactive=False\n",
    "    )\n",
    "    return result\n",
    "\n",
    "def impute_knn_iris(data_missing, k=5):\n",
    "    \"\"\"KNN com standardização para IRIS.\"\"\"\n",
    "    # Calcular mean e std dos valores observados\n",
    "    means = data_missing.mean()\n",
    "    stds = data_missing.std()\n",
    "    \n",
    "    # Standardizar (mantém NaN)\n",
    "    data_scaled = (data_missing - means) / stds\n",
    "    \n",
    "    # Imputa\n",
    "    imputer = KNNImputer(n_neighbors=k)\n",
    "    data_imputed_scaled = pd.DataFrame(\n",
    "        imputer.fit_transform(data_scaled),\n",
    "        columns=data_scaled.columns,\n",
    "        index=data_scaled.index\n",
    "    )\n",
    "    \n",
    "    # Inverse transform\n",
    "    data_imputed = (data_imputed_scaled * stds) + means\n",
    "    \n",
    "    return data_imputed\n",
    "\n",
    "def impute_mice_iris(data_missing, random_state=42):\n",
    "    \"\"\"MICE para IRIS.\"\"\"\n",
    "    imputer = IterativeImputer(max_iter=10, random_state=random_state)\n",
    "    data_imputed = pd.DataFrame(\n",
    "        imputer.fit_transform(data_missing),\n",
    "        columns=data_missing.columns,\n",
    "        index=data_missing.index\n",
    "    )\n",
    "    return data_imputed\n",
    "\n",
    "def impute_missforest_iris(data_missing, random_state=42):\n",
    "    \"\"\"MissForest para IRIS.\"\"\"\n",
    "    imputer = IterativeImputer(\n",
    "        estimator=RandomForestRegressor(n_estimators=10, random_state=random_state),\n",
    "        max_iter=10,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    data_imputed = pd.DataFrame(\n",
    "        imputer.fit_transform(data_missing),\n",
    "        columns=data_missing.columns,\n",
    "        index=data_missing.index\n",
    "    )\n",
    "    return data_imputed\n",
    "\n",
    "print(\"\\nISCA-k: sem configurações especiais\")\n",
    "print(\"KNN: k=[3,5,7,9,11], com standardização (mean/std dos observados)\")\n",
    "print(\"MICE: max_iter=10\")\n",
    "print(\"MissForest: n_estimators=10, max_iter=10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18764a42-2890-438d-bd9a-1f5baf834527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicionário para armazenar resultados\n",
    "results_iris = {}\n",
    "\n",
    "# Contadores para progresso\n",
    "total_runs = len(MECHANISMS) * len(MISSING_RATES) * len(SEEDS) * len(METHOD_ORDER)\n",
    "current_run = 0\n",
    "\n",
    "for mechanism in MECHANISMS:\n",
    "    print(f\"\\n{mechanism}:\")\n",
    "    \n",
    "    for missing_rate in MISSING_RATES:\n",
    "        print(f\"  Missing rate {int(missing_rate*100)}%: \", end=\"\")\n",
    "        \n",
    "        for run_idx, seed in enumerate(SEEDS):\n",
    "            \n",
    "            # Introduzir missings\n",
    "            if mechanism == 'MCAR':\n",
    "                data_missing = introduce_mcar(data_iris, missing_rate, seed)\n",
    "            elif mechanism == 'MAR':\n",
    "                data_missing = introduce_mar(data_iris, missing_rate, seed)\n",
    "            else:\n",
    "                data_missing = introduce_mnar(data_iris, missing_rate, seed)\n",
    "            \n",
    "            missing_mask = data_missing.isna()\n",
    "            \n",
    "            # Testar cada método\n",
    "            for method in METHOD_ORDER:\n",
    "                current_run += 1\n",
    "                \n",
    "                try:\n",
    "                    start_time = time.time()\n",
    "                    \n",
    "                    if method == 'ISCA-k':\n",
    "                        data_imputed = impute_iscak_iris(data_missing)\n",
    "                    \n",
    "                    elif method == 'KNN':\n",
    "                        # Grid search: testar todos os k\n",
    "                        best_result = None\n",
    "                        best_metrics = None\n",
    "                        best_k = None\n",
    "                        \n",
    "                        for k in [3, 5, 7, 9, 11]:\n",
    "                            data_imputed_k = impute_knn_iris(data_missing, k=k)\n",
    "                            metrics_k = calculate_metrics(data_iris, data_imputed_k, missing_mask)\n",
    "                            \n",
    "                            # Escolher k com melhor R²\n",
    "                            if best_metrics is None or (not np.isnan(metrics_k['R2']) and metrics_k['R2'] > best_metrics['R2']):\n",
    "                                best_result = data_imputed_k\n",
    "                                best_metrics = metrics_k\n",
    "                                best_k = k\n",
    "                        \n",
    "                        data_imputed = best_result\n",
    "                        elapsed_time = time.time() - start_time\n",
    "                        \n",
    "                        # Adicionar info do melhor k\n",
    "                        best_metrics['Time'] = elapsed_time\n",
    "                        best_metrics['Best_k'] = best_k\n",
    "                        \n",
    "                        # Guardar resultado\n",
    "                        key = (mechanism, missing_rate, method)\n",
    "                        if key not in results_iris:\n",
    "                            results_iris[key] = []\n",
    "                        results_iris[key].append(best_metrics)\n",
    "                        \n",
    "                        continue  # Pular o código abaixo (já guardámos)\n",
    "                    \n",
    "                    elif method == 'MICE':\n",
    "                        data_imputed = impute_mice_iris(data_missing, random_state=seed)\n",
    "                    \n",
    "                    else:  # MissForest\n",
    "                        data_imputed = impute_missforest_iris(data_missing, random_state=seed)\n",
    "                    \n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    \n",
    "                    # Calcular métricas\n",
    "                    metrics = calculate_metrics(data_iris, data_imputed, missing_mask)\n",
    "                    metrics['Time'] = elapsed_time\n",
    "                    \n",
    "                    # Guardar resultado\n",
    "                    key = (mechanism, missing_rate, method)\n",
    "                    if key not in results_iris:\n",
    "                        results_iris[key] = []\n",
    "                    results_iris[key].append(metrics)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"\\nERRO em {method}: {e}\")\n",
    "                    import traceback\n",
    "                    traceback.print_exc()\n",
    "                    continue\n",
    "            \n",
    "            print(\".\", end=\"\", flush=True)\n",
    "        \n",
    "        print(\" OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb97103-5444-4c45-858a-b01b5e9ee34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mechanism in MECHANISMS:\n",
    "    display_results_table(results_iris, mechanism, \"IRIS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a891734-3100-4c60-aaa5-18b564f5efc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "def introduce_mcar1(data, missing_rate, seed):\n",
    "    \"\"\"\n",
    "    Missing Completely At Random - implementação do algoritmo estruturado\n",
    "    Seleciona posições (registro, atributo) completamente aleatórias\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.DataFrame\n",
    "        Dataset completo\n",
    "    missing_rate : float\n",
    "        Taxa de missing desejada (MDR)\n",
    "    seed : int\n",
    "        Seed para reprodutibilidade\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    data_missing = data.copy()\n",
    "    \n",
    "    # Definições\n",
    "    N = len(data)  # Total de registos\n",
    "    M = len(data.columns)  # Total de atributos\n",
    "    MDR = missing_rate  # Taxa de missing desejada\n",
    "    \n",
    "    while True:\n",
    "        # Gerar índices aleatórios\n",
    "        X = np.random.randint(0, N)  # Registo aleatório\n",
    "        Y = np.random.randint(0, M)  # Atributo aleatório\n",
    "        \n",
    "        # Se a posição ainda não é NaN, torná-la NaN\n",
    "        if not pd.isna(data_missing.iloc[X, Y]):\n",
    "            data_missing.iloc[X, Y] = np.nan\n",
    "            \n",
    "            # Calcular ratio atual de missing\n",
    "            R = data_missing.isna().sum().sum() / (N * M)\n",
    "            \n",
    "            # Se atingiu a taxa desejada, parar\n",
    "            if R >= MDR:\n",
    "                break\n",
    "    \n",
    "    return data_missing\n",
    "\n",
    "def introduce_mar1(data, missing_rate, seed, na_attributes=None):\n",
    "    \"\"\"\n",
    "    Missing At Random - implementação do algoritmo estruturado\n",
    "    Os missings dependem de um atributo causativo (valores baixos causam missing)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.DataFrame\n",
    "        Dataset completo\n",
    "    missing_rate : float\n",
    "        Taxa de missing desejada (MDR)\n",
    "    seed : int\n",
    "        Seed para reprodutibilidade\n",
    "    na_attributes : int, optional\n",
    "        Número de atributos que perdem valores (NA). Se None, usa metade das colunas\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    data_missing = data.copy()\n",
    "    \n",
    "    # Definições\n",
    "    N = len(data)  # Total de registos\n",
    "    M = len(data.columns)  # Total de atributos\n",
    "    V = N * M  # Total de valores no dataset\n",
    "    \n",
    "    # NA: número de atributos que perdem valores\n",
    "    if na_attributes is None:\n",
    "        NA = max(1, M // 2)  # Default: metade dos atributos\n",
    "    else:\n",
    "        NA = min(na_attributes, M)  # Não pode exceder o total de atributos\n",
    "    \n",
    "    MDR = missing_rate * 100  # Taxa de missing em percentagem\n",
    "    \n",
    "    # Selecionar atributo causativo aleatório\n",
    "    Causative = np.random.randint(0, M)\n",
    "    \n",
    "    # Selecionar NA atributos aleatórios que perderão valores (diferentes do causativo)\n",
    "    MDAttributes = []\n",
    "    while len(MDAttributes) < NA:\n",
    "        Y = np.random.randint(0, M)\n",
    "        if Y not in MDAttributes and Y != Causative:\n",
    "            MDAttributes.append(Y)\n",
    "    \n",
    "    # Se não conseguimos NA atributos diferentes do causativo (dataset muito pequeno)\n",
    "    if len(MDAttributes) == 0:\n",
    "        MDAttributes = [i for i in range(M) if i != Causative][:NA]\n",
    "    \n",
    "    # Obter vetor de valores do atributo causativo\n",
    "    Aux = data.iloc[:, Causative].values.copy()\n",
    "    \n",
    "    # Selecionar registos com valores MÍNIMOS no atributo causativo\n",
    "    MDRecords = []\n",
    "    MV = 0  # Contador de valores missing\n",
    "    MaxInt = np.inf  # Valor máximo para marcar índices já usados\n",
    "    \n",
    "    while True:\n",
    "        # Encontrar índice do valor mínimo em Aux\n",
    "        MinIndex = np.argmin(Aux)\n",
    "        MDRecords.append(MinIndex)\n",
    "        \n",
    "        # Marcar este índice como inelegível\n",
    "        Aux[MinIndex] = MaxInt\n",
    "        \n",
    "        MV += len(MDAttributes)  # Incrementar por NA (número de atributos que perdem valores)\n",
    "        R = (MV / V) * 100  # Calcular ratio atual\n",
    "        \n",
    "        if R >= MDR:\n",
    "            break\n",
    "    \n",
    "    # Aplicar missing nos registos e atributos selecionados\n",
    "    for i in MDRecords:\n",
    "        for j in MDAttributes:\n",
    "            data_missing.iloc[i, j] = np.nan\n",
    "    \n",
    "    return data_missing\n",
    "\n",
    "def introduce_mnar1(data, missing_rate, seed, na_attributes=None):\n",
    "    \"\"\"\n",
    "    Missing Completely At Random - implementação do algoritmo estruturado\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.DataFrame\n",
    "        Dataset completo\n",
    "    missing_rate : float\n",
    "        Taxa de missing desejada (MDR)\n",
    "    seed : int\n",
    "        Seed para reprodutibilidade\n",
    "    na_attributes : int, optional\n",
    "        Número de atributos que perdem valores (NA). Se None, usa metade das colunas\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    data_missing = data.copy()\n",
    "    \n",
    "    # Definições\n",
    "    N = len(data)  # Total de registos\n",
    "    M = len(data.columns)  # Total de atributos\n",
    "    V = N * M  # Total de valores no dataset\n",
    "    \n",
    "    # NA: número de atributos que perdem valores\n",
    "    if na_attributes is None:\n",
    "        NA = max(1, M // 2)  # Default: metade dos atributos\n",
    "    else:\n",
    "        NA = min(na_attributes, M)  # Não pode exceder o total de atributos\n",
    "    \n",
    "    MDR = missing_rate * 100  # Taxa de missing em percentagem\n",
    "    \n",
    "    # Selecionar NA atributos aleatórios que perderão valores\n",
    "    MDAttributes = []\n",
    "    while len(MDAttributes) < NA:\n",
    "        Y = np.random.randint(0, M)\n",
    "        if Y not in MDAttributes:\n",
    "            MDAttributes.append(Y)\n",
    "    \n",
    "    # Selecionar registos que perderão valores\n",
    "    MDRecords = []\n",
    "    MV = 0  # Contador de valores missing\n",
    "    \n",
    "    while True:\n",
    "        X = np.random.randint(0, N)\n",
    "        if X not in MDRecords:\n",
    "            MDRecords.append(X)\n",
    "            MV += NA  # Incrementar por NA (número de atributos que perdem valores)\n",
    "            R = (MV / V) * 100  # Calcular ratio atual\n",
    "            \n",
    "            if R >= MDR:\n",
    "                break\n",
    "    \n",
    "    # Aplicar missing nos registos e atributos selecionados\n",
    "    for i in MDRecords:\n",
    "        for j in MDAttributes:\n",
    "            data_missing.iloc[i, j] = np.nan\n",
    "    \n",
    "    return data_missing\n",
    "    \n",
    "def plot_missing_pattern(data, mechanism='MCAR', figsize=(10, 8)):\n",
    "    \"\"\"\n",
    "    Cria visualização de padrões de missing data similar à imagem fornecida.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.DataFrame\n",
    "        Dataset com missing values\n",
    "    mechanism : str\n",
    "        Tipo de mecanismo ('MCAR', 'MAR', 'MNAR')\n",
    "    figsize : tuple\n",
    "        Tamanho da figura\n",
    "    \"\"\"\n",
    "    # Criar máscara de missing (True = missing, False = observado)\n",
    "    missing_mask = data.isna()\n",
    "    \n",
    "    # Calcular percentagem de missing\n",
    "    missing_rate = (missing_mask.sum().sum() / (data.shape[0] * data.shape[1])) * 100\n",
    "    \n",
    "    # Criar figura com dois subplots (agora na vertical)\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=figsize, \n",
    "                                     gridspec_kw={'height_ratios': [20, 1]})\n",
    "    \n",
    "    # Plot principal: padrão de missing data\n",
    "    # Agora as variáveis ficam nas colunas e observações nas linhas\n",
    "    missing_matrix = missing_mask.values\n",
    "    \n",
    "    # Criar imagem: cinza claro para dados, branco para missing\n",
    "    img = np.where(missing_matrix, 1, 0.55)  # 1 = branco (missing), 0.55 = cinza claro (observado)\n",
    "    \n",
    "    ax1.imshow(img, cmap='gray', aspect='auto', interpolation='nearest')\n",
    "    \n",
    "    # Configurar eixos\n",
    "    ax1.set_xticks(range(len(data.columns)))\n",
    "    ax1.set_xticklabels(data.columns, fontsize=10, rotation=45, ha='right')\n",
    "    ax1.set_ylabel('Observações', fontsize=11)\n",
    "    ax1.set_yticks([0, len(data)-1])\n",
    "    ax1.set_yticklabels(['1', str(len(data))], fontsize=10)\n",
    "    \n",
    "    # Adicionar grid subtil\n",
    "    ax1.set_xticks(np.arange(-0.5, len(data.columns), 1), minor=True)\n",
    "    ax1.set_yticks(np.arange(-0.5, len(data), 1), minor=True)\n",
    "    ax1.grid(which='minor', color='white', linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "    \n",
    "    # Plot inferior: contagem de missing por variável\n",
    "    missing_counts = missing_mask.sum(axis=0).values\n",
    "    \n",
    "    ax2.bar(range(len(data.columns)), missing_counts, color='#666666', alpha=0.7)\n",
    "    ax2.set_xlim(ax1.get_xlim())\n",
    "    ax2.set_xticks([])\n",
    "    ax2.set_ylabel('N Missing', fontsize=9)\n",
    "    ax2.invert_yaxis()\n",
    "    \n",
    "    # Adicionar valores nas barras\n",
    "    for i, count in enumerate(missing_counts):\n",
    "        if count > 0:\n",
    "            ax2.text(i, count/2, str(int(count)), \n",
    "                    ha='center', va='center', fontsize=8, color='white', fontweight='bold')\n",
    "    \n",
    "    # Título\n",
    "    plt.suptitle(f'Visualização de missing data - Mecanismo {mechanism} - '\n",
    "                 f'{missing_rate:.1f}% missing', \n",
    "                 fontsize=13, fontweight='bold', y=0.99)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# ======================\n",
    "# EXEMPLO DE USO\n",
    "# ======================\n",
    "\n",
    "# Carregar dados Sonar\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "sonar_data = fetch_openml('sonar', version=1, parser='auto', as_frame=True)\n",
    "data_sonar = pd.DataFrame(sonar_data.data).select_dtypes(include=[np.number])\n",
    "\n",
    "iris_data = load_iris()\n",
    "data_iris = pd.DataFrame(iris_data.data, columns=iris_data.feature_names)\n",
    "\n",
    "data_wine = pd.read_csv(\"dataset.csv\", sep=\";\")\n",
    "\n",
    "dataset = data_wine\n",
    "\n",
    "# Configurações\n",
    "MECHANISM = 'MCAR'      # 'MCAR' | 'MAR' | 'MNAR'\n",
    "MISSING_RATE = 0.10    # percentagem de missing\n",
    "SEED = 41\n",
    "\n",
    "# Gerar dataset com missing\n",
    "if MECHANISM == 'MCAR':\n",
    "    data_missing = introduce_mcar1(dataset, MISSING_RATE, SEED)\n",
    "elif MECHANISM == 'MAR':\n",
    "    data_missing = introduce_mar1(dataset, MISSING_RATE, SEED)\n",
    "elif MECHANISM == 'MNAR':\n",
    "    data_missing = introduce_mnar1(dataset, MISSING_RATE, SEED)\n",
    "else:\n",
    "    raise ValueError(\"MECHANISM inválido\")\n",
    "\n",
    "# Criar visualização\n",
    "fig = plot_missing_pattern(data_missing, mechanism=MECHANISM)\n",
    "plt.show()\n",
    "\n",
    "linhas_completamente_vazias = data_missing.isna().all(axis=1).sum()\n",
    "\n",
    "# Estatísticas adicionais\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ESTATÍSTICAS DE MISSING DATA\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nMecanismo: {MECHANISM}\")\n",
    "print(f\"Taxa de missing configurada: {MISSING_RATE*100:.1f}%\")\n",
    "print(f\"Taxa de missing real: {(data_missing.isna().sum().sum() / data_missing.size)*100:.1f}%\")\n",
    "print(f\"\\nMissing por variável:\")\n",
    "print(data_missing.isna().sum())\n",
    "print(f\"\\nTotal de observações: {len(data_missing)}\")\n",
    "print(f\"Total de valores missing: {data_missing.isna().sum().sum()}\")\n",
    "print(f\"Linhas completamente vazias: {linhas_completamente_vazias}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c536854-e1f1-4b10-8b3d-7e84d511ff89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TESTES ESTATÍSTICOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Wilcoxon para 30% em cada mecanismo\n",
    "for mechanism in MECHANISMS:\n",
    "    print(f\"\\n{mechanism} - 30%:\")\n",
    "    display_statistical_tests(results_iris, mechanism, 0.30, \"IRIS\", metric='NRMSE')\n",
    "\n",
    "# Friedman test para 30% MCAR\n",
    "print(\"\\n\")\n",
    "display_friedman_results(results_iris, 'MCAR', 0.30, \"IRIS\", metric='NRMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca0f088-0f57-40c0-ba8a-2b8033eba00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curvas de degradação\n",
    "print(\"\\nGerando curvas de degradação...\")\n",
    "plot_degradation_curves(results_iris, \"IRIS\", metric='NRMSE')\n",
    "\n",
    "# Comparação de métodos\n",
    "print(\"Gerando comparação de métodos...\")\n",
    "plot_method_comparison(results_iris, \"IRIS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e5bb56-754d-4369-adf8-7030c6350249",
   "metadata": {},
   "source": [
    "##### WINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03dbdc5-fda2-4021-b1e0-c9fafc07a34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a0ce33-f8d4-46da-aa13-532abdfd80d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_wine = pd.read_csv(\"dataset.csv\", sep=\";\")\n",
    "\n",
    "print(f\"\\nShape original: {data_wine.shape}\")\n",
    "print(f\"Tipos de dados:\\n{data_wine.dtypes}\")\n",
    "print(f\"\\nMissings nativos: {data_wine.isna().sum().sum()}\")\n",
    "\n",
    "# Verificar que não há missings nativos\n",
    "assert data_wine.isna().sum().sum() == 0, \"Dataset tem missings nativos!\"\n",
    "\n",
    "print(\"\\nDataset completo e limpo\")\n",
    "print(f\"Shape final: {data_wine.shape}\")\n",
    "print(f\"\\nPrimeiras linhas:\")\n",
    "print(data_wine.head())\n",
    "print(f\"\\nEstatísticas descritivas:\")\n",
    "print(data_wine.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284661fb-bc4f-4d2a-8e6e-417e660922f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_iscak_wine(data_missing):\n",
    "    \"\"\"ISCA-k para WINE (numérico puro).\"\"\"\n",
    "    imputer = ISCAkCore(verbose=False)\n",
    "    result = imputer.impute(\n",
    "        data_missing,\n",
    "        force_categorical=None,\n",
    "        force_ordinal=None,\n",
    "        interactive=False\n",
    "    )\n",
    "    return result\n",
    "\n",
    "def impute_knn_wine(data_missing, k=5):\n",
    "    \"\"\"KNN com standardização para WINE.\"\"\"\n",
    "    # Calcular mean e std dos valores observados\n",
    "    means = data_missing.mean()\n",
    "    stds = data_missing.std()\n",
    "    \n",
    "    # Standardizar (mantém NaN)\n",
    "    data_scaled = (data_missing - means) / stds\n",
    "    \n",
    "    # Imputa\n",
    "    imputer = KNNImputer(n_neighbors=k)\n",
    "    data_imputed_scaled = pd.DataFrame(\n",
    "        imputer.fit_transform(data_scaled),\n",
    "        columns=data_scaled.columns,\n",
    "        index=data_scaled.index\n",
    "    )\n",
    "    \n",
    "    # Inverse transform\n",
    "    data_imputed = (data_imputed_scaled * stds) + means\n",
    "    \n",
    "    return data_imputed\n",
    "\n",
    "def impute_mice_wine(data_missing, random_state=42):\n",
    "    \"\"\"MICE para WINE.\"\"\"\n",
    "    imputer = IterativeImputer(max_iter=10, random_state=random_state)\n",
    "    data_imputed = pd.DataFrame(\n",
    "        imputer.fit_transform(data_missing),\n",
    "        columns=data_missing.columns,\n",
    "        index=data_missing.index\n",
    "    )\n",
    "    return data_imputed\n",
    "\n",
    "def impute_missforest_wine(data_missing, random_state=42):\n",
    "    \"\"\"MissForest para WINE.\"\"\"\n",
    "    imputer = IterativeImputer(\n",
    "        estimator=RandomForestRegressor(n_estimators=10, random_state=random_state),\n",
    "        max_iter=10,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    data_imputed = pd.DataFrame(\n",
    "        imputer.fit_transform(data_missing),\n",
    "        columns=data_missing.columns,\n",
    "        index=data_missing.index\n",
    "    )\n",
    "    return data_imputed\n",
    "\n",
    "print(\"\\nISCA-k: sem configurações especiais\")\n",
    "print(\"KNN: k=[3,5,7,9,11], com standardização (mean/std dos observados)\")\n",
    "print(\"MICE: max_iter=10\")\n",
    "print(\"MissForest: n_estimators=10, max_iter=10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17716e36-277a-4af8-a1ca-2893353b745f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicionário para armazenar resultados\n",
    "results_wine = {}\n",
    "\n",
    "for mechanism in MECHANISMS:\n",
    "    print(f\"\\n{mechanism}:\")\n",
    "    \n",
    "    for missing_rate in MISSING_RATES:\n",
    "        print(f\"  Missing rate {int(missing_rate*100)}%: \", end=\"\")\n",
    "        \n",
    "        for run_idx, seed in enumerate(SEEDS):\n",
    "            \n",
    "            # Introduzir missings\n",
    "            if mechanism == 'MCAR':\n",
    "                data_missing = introduce_mcar(data_wine, missing_rate, seed)\n",
    "            elif mechanism == 'MAR':\n",
    "                data_missing = introduce_mar(data_wine, missing_rate, seed)\n",
    "            else:\n",
    "                data_missing = introduce_mnar(data_wine, missing_rate, seed)\n",
    "            \n",
    "            missing_mask = data_missing.isna()\n",
    "            \n",
    "            # Testar cada método\n",
    "            for method in METHOD_ORDER:\n",
    "                \n",
    "                try:\n",
    "                    start_time = time.time()\n",
    "                    \n",
    "                    if method == 'ISCA-k':\n",
    "                        data_imputed = impute_iscak_wine(data_missing)\n",
    "                    \n",
    "                    elif method == 'KNN':\n",
    "                        # Grid search: testar todos os k\n",
    "                        best_result = None\n",
    "                        best_metrics = None\n",
    "                        best_k = None\n",
    "                        \n",
    "                        for k in [3, 5, 7, 9, 11]:\n",
    "                            data_imputed_k = impute_knn_wine(data_missing, k=k)\n",
    "                            metrics_k = calculate_metrics(data_wine, data_imputed_k, missing_mask)\n",
    "                            \n",
    "                            # Escolher k com melhor R²\n",
    "                            if best_metrics is None or (not np.isnan(metrics_k['R2']) and metrics_k['R2'] > best_metrics['R2']):\n",
    "                                best_result = data_imputed_k\n",
    "                                best_metrics = metrics_k\n",
    "                                best_k = k\n",
    "                        \n",
    "                        data_imputed = best_result\n",
    "                        elapsed_time = time.time() - start_time\n",
    "                        \n",
    "                        # Adicionar info do melhor k\n",
    "                        best_metrics['Time'] = elapsed_time\n",
    "                        best_metrics['Best_k'] = best_k\n",
    "                        \n",
    "                        # Guardar resultado\n",
    "                        key = (mechanism, missing_rate, method)\n",
    "                        if key not in results_wine:\n",
    "                            results_wine[key] = []\n",
    "                        results_wine[key].append(best_metrics)\n",
    "                        \n",
    "                        continue  # Pular o código abaixo (já guardámos)\n",
    "                    \n",
    "                    elif method == 'MICE':\n",
    "                        data_imputed = impute_mice_wine(data_missing, random_state=seed)\n",
    "                    \n",
    "                    else:  # MissForest\n",
    "                        data_imputed = impute_missforest_wine(data_missing, random_state=seed)\n",
    "                    \n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    \n",
    "                    # Calcular métricas\n",
    "                    metrics = calculate_metrics(data_wine, data_imputed, missing_mask)\n",
    "                    metrics['Time'] = elapsed_time\n",
    "                    \n",
    "                    # Guardar resultado\n",
    "                    key = (mechanism, missing_rate, method)\n",
    "                    if key not in results_wine:\n",
    "                        results_wine[key] = []\n",
    "                    results_wine[key].append(metrics)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"\\nERRO em {method}: {e}\")\n",
    "                    import traceback\n",
    "                    traceback.print_exc()\n",
    "                    continue\n",
    "            \n",
    "            print(\".\", end=\"\", flush=True)\n",
    "        \n",
    "        print(\" OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a4652a-0304-48d8-9c53-2ddb1c29404d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mechanism in MECHANISMS:\n",
    "    display_results_table(results_wine, mechanism, \"WINE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c24921-45a5-4cfd-99cf-8330183fb318",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TESTES ESTATÍSTICOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Wilcoxon para 30% em cada mecanismo\n",
    "for mechanism in MECHANISMS:\n",
    "    print(f\"\\n{mechanism} - 30%:\")\n",
    "    display_statistical_tests(results_wine, mechanism, 0.30, \"WINE\", metric='NRMSE')\n",
    "\n",
    "# Friedman test para 30% MCAR\n",
    "print(\"\\n\")\n",
    "display_friedman_results(results_wine, 'MCAR', 0.30, \"WINE\", metric='NRMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344e4f25-d27f-4c7d-b1ba-49cc10c4d7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curvas de degradação\n",
    "print(\"\\nGerando curvas de degradação...\")\n",
    "plot_degradation_curves(results_wine, \"WINE\", metric='NRMSE')\n",
    "\n",
    "# Comparação de métodos\n",
    "print(\"Gerando comparação de métodos...\")\n",
    "plot_method_comparison(results_wine, \"WINE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2655f7-046b-4087-8fe2-372c9049c490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63185e70-1666-482c-b273-79fb9dd17023",
   "metadata": {},
   "source": [
    "##### SONAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b31493c-c467-4da5-873d-a652a0c4b3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "print(\"Carregando SONAR do OpenML...\")\n",
    "sonar_data = fetch_openml('sonar', version=1, parser='auto', as_frame=True)\n",
    "data_sonar = pd.DataFrame(sonar_data.data).select_dtypes(include=[np.number])\n",
    "\n",
    "print(f\"\\nShape original: {data_sonar.shape}\")\n",
    "print(f\"Tipos de dados:\\n{data_sonar.dtypes.value_counts()}\")\n",
    "print(f\"\\nMissings nativos: {data_sonar.isna().sum().sum()}\")\n",
    "\n",
    "# Remover missings nativos se existirem\n",
    "if data_sonar.isna().sum().sum() > 0:\n",
    "    print(f\"Removendo {data_sonar.isna().sum().sum()} missings nativos...\")\n",
    "    data_sonar = data_sonar.dropna()\n",
    "    data_sonar = data_sonar.reset_index(drop=True)\n",
    "\n",
    "# Verificar que não há missings nativos\n",
    "assert data_sonar.isna().sum().sum() == 0, \"Dataset tem missings nativos!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c542ba97-1c8a-40cb-97ff-c34a1f338b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_iscak_sonar(data_missing):\n",
    "    \"\"\"ISCA-k para SONAR (numérico puro).\"\"\"\n",
    "    imputer = ISCAkCore(verbose=False)\n",
    "    result = imputer.impute(\n",
    "        data_missing,\n",
    "        force_categorical=None,\n",
    "        force_ordinal=None,\n",
    "        interactive=False\n",
    "    )\n",
    "    return result\n",
    "\n",
    "def impute_knn_sonar(data_missing, k=5):\n",
    "    \"\"\"KNN com standardização para SONAR.\"\"\"\n",
    "    # Calcular mean e std dos valores observados\n",
    "    means = data_missing.mean()\n",
    "    stds = data_missing.std()\n",
    "    \n",
    "    # Standardizar (mantém NaN)\n",
    "    data_scaled = (data_missing - means) / stds\n",
    "    \n",
    "    # Imputa\n",
    "    imputer = KNNImputer(n_neighbors=k)\n",
    "    data_imputed_scaled = pd.DataFrame(\n",
    "        imputer.fit_transform(data_scaled),\n",
    "        columns=data_scaled.columns,\n",
    "        index=data_scaled.index\n",
    "    )\n",
    "    \n",
    "    # Inverse transform\n",
    "    data_imputed = (data_imputed_scaled * stds) + means\n",
    "    \n",
    "    return data_imputed\n",
    "\n",
    "def impute_mice_sonar(data_missing, random_state=42):\n",
    "    \"\"\"MICE para SONAR.\"\"\"\n",
    "    imputer = IterativeImputer(max_iter=10, random_state=random_state)\n",
    "    data_imputed = pd.DataFrame(\n",
    "        imputer.fit_transform(data_missing),\n",
    "        columns=data_missing.columns,\n",
    "        index=data_missing.index\n",
    "    )\n",
    "    return data_imputed\n",
    "\n",
    "def impute_missforest_sonar(data_missing, random_state=42):\n",
    "    \"\"\"MissForest para SONAR.\"\"\"\n",
    "    imputer = IterativeImputer(\n",
    "        estimator=RandomForestRegressor(n_estimators=10, random_state=random_state),\n",
    "        max_iter=10,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    data_imputed = pd.DataFrame(\n",
    "        imputer.fit_transform(data_missing),\n",
    "        columns=data_missing.columns,\n",
    "        index=data_missing.index\n",
    "    )\n",
    "    return data_imputed\n",
    "\n",
    "print(\"\\nISCA-k: sem configurações especiais\")\n",
    "print(\"KNN: k=[3,5,7,9,11], com standardização (mean/std dos observados)\")\n",
    "print(\"MICE: max_iter=10\")\n",
    "print(\"MissForest: n_estimators=10, max_iter=10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2243362a-8a07-49c1-8679-db2806c77af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicionário para armazenar resultados\n",
    "results_sonar = {}\n",
    "\n",
    "for mechanism in MECHANISMS:\n",
    "    print(f\"\\n{mechanism}:\")\n",
    "    \n",
    "    for missing_rate in MISSING_RATES:\n",
    "        print(f\"  Missing rate {int(missing_rate*100)}%: \", end=\"\")\n",
    "        \n",
    "        for run_idx, seed in enumerate(SEEDS):\n",
    "            \n",
    "            # Introduzir missings\n",
    "            if mechanism == 'MCAR':\n",
    "                data_missing = introduce_mcar(data_sonar, missing_rate, seed)\n",
    "            elif mechanism == 'MAR':\n",
    "                data_missing = introduce_mar(data_sonar, missing_rate, seed)\n",
    "            else:\n",
    "                data_missing = introduce_mnar(data_sonar, missing_rate, seed)\n",
    "            \n",
    "            missing_mask = data_missing.isna()\n",
    "            \n",
    "            # Testar cada método\n",
    "            for method in METHOD_ORDER:\n",
    "                \n",
    "                try:\n",
    "                    start_time = time.time()\n",
    "                    \n",
    "                    if method == 'ISCA-k':\n",
    "                        data_imputed = impute_iscak_sonar(data_missing)\n",
    "                    \n",
    "                    elif method == 'KNN':\n",
    "                        # Grid search: testar todos os k\n",
    "                        best_result = None\n",
    "                        best_metrics = None\n",
    "                        best_k = None\n",
    "                        \n",
    "                        for k in [3, 5, 7, 9, 11]:\n",
    "                            data_imputed_k = impute_knn_sonar(data_missing, k=k)\n",
    "                            metrics_k = calculate_metrics(data_sonar, data_imputed_k, missing_mask)\n",
    "                            \n",
    "                            # Escolher k com melhor R²\n",
    "                            if best_metrics is None or (not np.isnan(metrics_k['R2']) and metrics_k['R2'] > best_metrics['R2']):\n",
    "                                best_result = data_imputed_k\n",
    "                                best_metrics = metrics_k\n",
    "                                best_k = k\n",
    "                        \n",
    "                        data_imputed = best_result\n",
    "                        elapsed_time = time.time() - start_time\n",
    "                        \n",
    "                        # Adicionar info do melhor k\n",
    "                        best_metrics['Time'] = elapsed_time\n",
    "                        best_metrics['Best_k'] = best_k\n",
    "                        \n",
    "                        # Guardar resultado\n",
    "                        key = (mechanism, missing_rate, method)\n",
    "                        if key not in results_sonar:\n",
    "                            results_sonar[key] = []\n",
    "                        results_sonar[key].append(best_metrics)\n",
    "                        \n",
    "                        continue  # Pular o código abaixo (já guardámos)\n",
    "                    \n",
    "                    elif method == 'MICE':\n",
    "                        data_imputed = impute_mice_sonar(data_missing, random_state=seed)\n",
    "                    \n",
    "                    else:  # MissForest\n",
    "                        data_imputed = impute_missforest_sonar(data_missing, random_state=seed)\n",
    "                    \n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    \n",
    "                    # Calcular métricas\n",
    "                    metrics = calculate_metrics(data_sonar, data_imputed, missing_mask)\n",
    "                    metrics['Time'] = elapsed_time\n",
    "                    \n",
    "                    # Guardar resultado\n",
    "                    key = (mechanism, missing_rate, method)\n",
    "                    if key not in results_sonar:\n",
    "                        results_sonar[key] = []\n",
    "                    results_sonar[key].append(metrics)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"\\nERRO em {method}: {e}\")\n",
    "                    import traceback\n",
    "                    traceback.print_exc()\n",
    "                    continue\n",
    "            \n",
    "            print(\".\", end=\"\", flush=True)\n",
    "        \n",
    "        print(\" OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d998d168-7be3-4441-b6bb-053f1eaba6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mechanism in MECHANISMS:\n",
    "    display_results_table(results_sonar, mechanism, \"SONAR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2dea37-a71e-4fe3-afe1-b66055a1e452",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TESTES ESTATÍSTICOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Wilcoxon para 30% em cada mecanismo\n",
    "for mechanism in MECHANISMS:\n",
    "    print(f\"\\n{mechanism} - 30%:\")\n",
    "    display_statistical_tests(results_sonar, mechanism, 0.30, \"SONAR\", metric='NRMSE')\n",
    "\n",
    "# Friedman test para 30% MCAR\n",
    "print(\"\\n\")\n",
    "display_friedman_results(results_sonar, 'MCAR', 0.30, \"SONAR\", metric='NRMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c08aa6-9a56-4758-8da5-6b2d65c100c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c02c19ef-5771-41b8-9875-ee36a839eec4",
   "metadata": {},
   "source": [
    "##### DIABETES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c443fe11-22d7-47eb-9da7-a8b0f857eeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Importar o teu método\n",
    "from iscak_core import ISCAkCore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf35c4c-f79b-4eeb-9e66-e03ff392d246",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_data = load_diabetes()\n",
    "data_diabetes = pd.DataFrame(diabetes_data.data, columns=diabetes_data.feature_names)\n",
    "\n",
    "print(f\"\\nShape original: {data_diabetes.shape}\")\n",
    "print(f\"Tipos de dados:\\n{data_diabetes.dtypes}\")\n",
    "print(f\"\\nMissings nativos: {data_diabetes.isna().sum().sum()}\")\n",
    "\n",
    "# Verificar que não há missings nativos\n",
    "assert data_diabetes.isna().sum().sum() == 0, \"Dataset tem missings nativos!\"\n",
    "\n",
    "print(\"\\nDataset completo e limpo\")\n",
    "print(f\"Shape final: {data_diabetes.shape}\")\n",
    "print(f\"\\nPrimeiras linhas:\")\n",
    "print(data_diabetes.head())\n",
    "print(f\"\\nEstatísticas descritivas:\")\n",
    "print(data_diabetes.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79802d0f-b619-4a39-a41a-d5506e3a372c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_iscak_diabetes(data_missing):\n",
    "    \"\"\"ISCA-k para DIABETES (numérico puro).\"\"\"\n",
    "    imputer = ISCAkCore(verbose=True)\n",
    "    result = imputer.impute(\n",
    "        data_missing,\n",
    "        force_categorical=None,\n",
    "        force_ordinal=None,\n",
    "        interactive=False\n",
    "    )\n",
    "    return result\n",
    "\n",
    "def impute_knn_diabetes(data_missing, k=5):\n",
    "    \"\"\"KNN com standardização para DIABETES.\"\"\"\n",
    "    # Calcular mean e std dos valores observados\n",
    "    means = data_missing.mean()\n",
    "    stds = data_missing.std()\n",
    "    \n",
    "    # Standardizar (mantém NaN)\n",
    "    data_scaled = (data_missing - means) / stds\n",
    "    \n",
    "    # Imputa\n",
    "    imputer = KNNImputer(n_neighbors=k)\n",
    "    data_imputed_scaled = pd.DataFrame(\n",
    "        imputer.fit_transform(data_scaled),\n",
    "        columns=data_scaled.columns,\n",
    "        index=data_scaled.index\n",
    "    )\n",
    "    \n",
    "    # Inverse transform\n",
    "    data_imputed = (data_imputed_scaled * stds) + means\n",
    "    \n",
    "    return data_imputed\n",
    "\n",
    "def impute_mice_diabetes(data_missing, random_state=42):\n",
    "    \"\"\"MICE para DIABETES.\"\"\"\n",
    "    imputer = IterativeImputer(max_iter=10, random_state=random_state)\n",
    "    data_imputed = pd.DataFrame(\n",
    "        imputer.fit_transform(data_missing),\n",
    "        columns=data_missing.columns,\n",
    "        index=data_missing.index\n",
    "    )\n",
    "    return data_imputed\n",
    "\n",
    "def impute_missforest_diabetes(data_missing, random_state=42):\n",
    "    \"\"\"MissForest para DIABETES.\"\"\"\n",
    "    imputer = IterativeImputer(\n",
    "        estimator=RandomForestRegressor(n_estimators=10, random_state=random_state),\n",
    "        max_iter=10,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    data_imputed = pd.DataFrame(\n",
    "        imputer.fit_transform(data_missing),\n",
    "        columns=data_missing.columns,\n",
    "        index=data_missing.index\n",
    "    )\n",
    "    return data_imputed\n",
    "\n",
    "print(\"\\nISCA-k: sem configurações especiais\")\n",
    "print(\"KNN: k=[3,5,7,9,11], com standardização (mean/std dos observados)\")\n",
    "print(\"MICE: max_iter=10\")\n",
    "print(\"MissForest: n_estimators=10, max_iter=10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a7ccff-87bb-4a2c-8a98-7eb8e06c3296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicionário para armazenar resultados\n",
    "results_diabetes = {}\n",
    "\n",
    "for mechanism in MECHANISMS:\n",
    "    print(f\"\\n{mechanism}:\")\n",
    "    \n",
    "    for missing_rate in MISSING_RATES:\n",
    "        print(f\"  Missing rate {int(missing_rate*100)}%: \", end=\"\")\n",
    "        \n",
    "        for run_idx, seed in enumerate(SEEDS):\n",
    "            \n",
    "            # Introduzir missings\n",
    "            if mechanism == 'MCAR':\n",
    "                data_missing = introduce_mcar(data_diabetes, missing_rate, seed)\n",
    "            elif mechanism == 'MAR':\n",
    "                data_missing = introduce_mar(data_diabetes, missing_rate, seed)\n",
    "            else:\n",
    "                data_missing = introduce_mnar(data_diabetes, missing_rate, seed)\n",
    "            \n",
    "            missing_mask = data_missing.isna()\n",
    "            \n",
    "            # Testar cada método\n",
    "            for method in METHOD_ORDER:\n",
    "                \n",
    "                try:\n",
    "                    start_time = time.time()\n",
    "                    \n",
    "                    if method == 'ISCA-k':\n",
    "                        data_imputed = impute_iscak_diabetes(data_missing)\n",
    "                    \n",
    "                    elif method == 'KNN':\n",
    "                        # Grid search: testar todos os k\n",
    "                        best_result = None\n",
    "                        best_metrics = None\n",
    "                        best_k = None\n",
    "                        \n",
    "                        for k in [3, 5, 7, 9, 11]:\n",
    "                            data_imputed_k = impute_knn_diabetes(data_missing, k=k)\n",
    "                            metrics_k = calculate_metrics(data_diabetes, data_imputed_k, missing_mask)\n",
    "                            \n",
    "                            # Escolher k com melhor R²\n",
    "                            if best_metrics is None or (not np.isnan(metrics_k['R2']) and metrics_k['R2'] > best_metrics['R2']):\n",
    "                                best_result = data_imputed_k\n",
    "                                best_metrics = metrics_k\n",
    "                                best_k = k\n",
    "                        \n",
    "                        data_imputed = best_result\n",
    "                        elapsed_time = time.time() - start_time\n",
    "                        \n",
    "                        # Adicionar info do melhor k\n",
    "                        best_metrics['Time'] = elapsed_time\n",
    "                        best_metrics['Best_k'] = best_k\n",
    "                        \n",
    "                        # Guardar resultado\n",
    "                        key = (mechanism, missing_rate, method)\n",
    "                        if key not in results_diabetes:\n",
    "                            results_diabetes[key] = []\n",
    "                        results_diabetes[key].append(best_metrics)\n",
    "                        \n",
    "                        continue  # Pular o código abaixo (já guardámos)\n",
    "                    \n",
    "                    elif method == 'MICE':\n",
    "                        data_imputed = impute_mice_diabetes(data_missing, random_state=seed)\n",
    "                    \n",
    "                    else:  # MissForest\n",
    "                        data_imputed = impute_missforest_diabetes(data_missing, random_state=seed)\n",
    "                    \n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    \n",
    "                    # Calcular métricas\n",
    "                    metrics = calculate_metrics(data_diabetes, data_imputed, missing_mask)\n",
    "                    metrics['Time'] = elapsed_time\n",
    "                    \n",
    "                    # Guardar resultado\n",
    "                    key = (mechanism, missing_rate, method)\n",
    "                    if key not in results_diabetes:\n",
    "                        results_diabetes[key] = []\n",
    "                    results_diabetes[key].append(metrics)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"\\nERRO em {method}: {e}\")\n",
    "                    import traceback\n",
    "                    traceback.print_exc()\n",
    "                    continue\n",
    "            \n",
    "            print(\".\", end=\"\", flush=True)\n",
    "        \n",
    "        print(\" OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e916db54-30c7-415a-9199-373be5f4aa42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for mechanism in MECHANISMS:\n",
    "    display_results_table(results_diabetes, mechanism, \"DIABETES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ff9456-717a-47b8-9876-e00e13165699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wilcoxon para 30% em cada mecanismo\n",
    "for mechanism in MECHANISMS:\n",
    "    print(f\"\\n{mechanism} - 30%:\")\n",
    "    display_statistical_tests(results_diabetes, mechanism, 0.30, \"DIABETES\", metric='NRMSE')\n",
    "\n",
    "# Friedman test para 30% MCAR\n",
    "print(\"\\n\")\n",
    "display_friedman_results(results_diabetes, 'MCAR', 0.30, \"DIABETES\", metric='NRMSE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb92f18-fc2f-430d-802c-e28f5f92239e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curvas de degradação\n",
    "print(\"\\nGerando curvas de degradação...\")\n",
    "plot_degradation_curves(results_diabetes, \"DIABETES\", metric='NRMSE')\n",
    "\n",
    "# Comparação de métodos\n",
    "print(\"Gerando comparação de métodos...\")\n",
    "plot_method_comparison(results_diabetes, \"DIABETES\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3a21e1-67b2-4ab1-be34-06d00120213b",
   "metadata": {},
   "source": [
    "#### Datasets Mistos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548138a4-6a37-470c-b473-7c1a12346c67",
   "metadata": {},
   "source": [
    "##### TITANIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbb5939-43b5-4313-b049-79e99ac62cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.impute import KNNImputer, IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ffa782-1d62-4070-83cd-f2a2988f12a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "data_titanic_raw = pd.read_csv(url)\n",
    "\n",
    "print(f\"\\nShape original: {data_titanic_raw.shape}\")\n",
    "print(f\"\\nColunas: {list(data_titanic_raw.columns)}\")\n",
    "print(f\"\\nTipos originais:\\n{data_titanic_raw.dtypes}\")\n",
    "\n",
    "# Selecionar features relevantes e remover linhas com missings nativos\n",
    "# Features a usar: Pclass, Sex, Age, SibSp, Parch, Fare, Embarked\n",
    "features_to_use = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
    "\n",
    "data_titanic = data_titanic_raw[features_to_use].copy()\n",
    "\n",
    "print(f\"\\nMissings nativos por coluna:\")\n",
    "print(data_titanic.isna().sum())\n",
    "\n",
    "# Remover linhas com missings nativos (Age, Fare, Embarked)\n",
    "# Para ter dataset completo para introduzir missings artificiais\n",
    "data_titanic_complete = data_titanic.dropna()\n",
    "\n",
    "print(f\"\\nShape após remover missings nativos: {data_titanic_complete.shape}\")\n",
    "print(f\"Missings restantes: {data_titanic_complete.isna().sum().sum()}\")\n",
    "\n",
    "# Verificar\n",
    "assert data_titanic_complete.isna().sum().sum() == 0, \"Dataset ainda tem missings!\"\n",
    "\n",
    "print(\"\\n✓ Dataset completo e limpo\")\n",
    "print(f\"Shape final: {data_titanic_complete.shape}\")\n",
    "print(f\"\\nTipos finais:\")\n",
    "print(data_titanic_complete.dtypes)\n",
    "print(f\"\\nPrimeiras linhas:\")\n",
    "print(data_titanic_complete.head())\n",
    "print(f\"\\nEstatísticas descritivas (numéricas):\")\n",
    "print(data_titanic_complete.describe())\n",
    "print(f\"\\nDistribuição categóricas:\")\n",
    "print(f\"  Sex: {data_titanic_complete['Sex'].value_counts().to_dict()}\")\n",
    "print(f\"  Embarked: {data_titanic_complete['Embarked'].value_counts().to_dict()}\")\n",
    "print(f\"  Pclass: {data_titanic_complete['Pclass'].value_counts().to_dict()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91b7cfc-a9e2-4fdf-8244-660ce76bbee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "force_categorical_titanic = None  # Sex e Embarked já são object, serão detectadas\n",
    "\n",
    "force_ordinal_titanic = {\n",
    "    'Pclass': [1, 2, 3]  # Ordem: 1ª classe > 2ª > 3ª (em termos de qualidade)\n",
    "}\n",
    "\n",
    "print(\"\\nforce_categorical:\", force_categorical_titanic)\n",
    "print(\"force_ordinal:\", force_ordinal_titanic)\n",
    "print(\"\\nVariáveis detectadas automaticamente:\")\n",
    "print(\"  - Sex: nominal (binária)\")\n",
    "print(\"  - Embarked: nominal (C/Q/S)\")\n",
    "print(\"  - Age, SibSp, Parch, Fare: numéricas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e144d002-0d70-45d8-93c3-9600a00c8747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_iscak_titanic(data_missing):\n",
    "    \"\"\"ISCA-k para TITANIC (misto).\"\"\"\n",
    "    imputer = ISCAkCore(verbose=False)\n",
    "    result = imputer.impute(\n",
    "        data_missing,\n",
    "        force_categorical=force_categorical_titanic,\n",
    "        force_ordinal=force_ordinal_titanic,\n",
    "        interactive=False\n",
    "    )\n",
    "    return result\n",
    "\n",
    "def impute_knn_titanic(data_missing, k=5):\n",
    "    \"\"\"\n",
    "    KNN para TITANIC (misto).\n",
    "    \n",
    "    Estratégia: \n",
    "    1. Label encode categóricas\n",
    "    2. One-hot encode para KNN  \n",
    "    3. Imputa\n",
    "    4. Reverte para tipos ORIGINAIS (Sex/Embarked=string, Pclass=int)\n",
    "    \"\"\"\n",
    "    # Definir colunas por tipo (conforme dataset original)\n",
    "    numeric_cols = ['Age', 'SibSp', 'Parch', 'Fare']\n",
    "    ordinal_cols = ['Pclass']  # int ordinal\n",
    "    nominal_cols = ['Sex', 'Embarked']  # string nominal\n",
    "    \n",
    "    # Label encode TODAS as categóricas\n",
    "    data_encoded = data_missing.copy()\n",
    "    label_mappings = {}\n",
    "    \n",
    "    for col in ordinal_cols + nominal_cols:\n",
    "        unique_vals = data_encoded[col].dropna().unique()\n",
    "        mapping = {val: idx for idx, val in enumerate(sorted(unique_vals))}\n",
    "        reverse_mapping = {idx: val for val, idx in mapping.items()}\n",
    "        label_mappings[col] = mapping\n",
    "        label_mappings[f'{col}_reverse'] = reverse_mapping\n",
    "        \n",
    "        # Encode\n",
    "        data_encoded[col] = data_encoded[col].map(mapping)\n",
    "    \n",
    "    # One-hot encode para KNN\n",
    "    encoded_dfs = []\n",
    "    \n",
    "    for col in ordinal_cols + nominal_cols:\n",
    "        dummies = pd.get_dummies(data_encoded[col], prefix=col, drop_first=False, dtype=float)\n",
    "        # Propagar NaN\n",
    "        mask = data_encoded[col].isna()\n",
    "        dummies.loc[mask, :] = np.nan\n",
    "        encoded_dfs.append(dummies)\n",
    "    \n",
    "    # Combinar numéricas + dummies\n",
    "    data_for_knn = pd.concat(\n",
    "        [data_encoded[numeric_cols]] + encoded_dfs,\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Standardizar APENAS numéricas\n",
    "    means = data_for_knn[numeric_cols].mean()\n",
    "    stds = data_for_knn[numeric_cols].std().replace(0, 1)\n",
    "    data_for_knn[numeric_cols] = (data_for_knn[numeric_cols] - means) / stds\n",
    "    \n",
    "    # Imputar\n",
    "    imputer = KNNImputer(n_neighbors=k, weights='distance')\n",
    "    data_imputed_array = imputer.fit_transform(data_for_knn)\n",
    "    data_imputed_knn = pd.DataFrame(\n",
    "        data_imputed_array,\n",
    "        columns=data_for_knn.columns,\n",
    "        index=data_for_knn.index\n",
    "    )\n",
    "    \n",
    "    # Inverse transform numéricas\n",
    "    data_imputed_knn[numeric_cols] = (data_imputed_knn[numeric_cols] * stds) + means\n",
    "    \n",
    "    # Construir resultado\n",
    "    result = pd.DataFrame(index=data_missing.index)\n",
    "    \n",
    "    # Copiar numéricas\n",
    "    result[numeric_cols] = data_imputed_knn[numeric_cols]\n",
    "    \n",
    "    # Reverter categóricas para valores ORIGINAIS\n",
    "    for col in ordinal_cols + nominal_cols:\n",
    "        dummy_cols = [c for c in data_imputed_knn.columns if c.startswith(f\"{col}_\")]\n",
    "        if len(dummy_cols) > 0:\n",
    "            # Índice da dummy com maior valor\n",
    "            dummy_values = data_imputed_knn[dummy_cols].values\n",
    "            max_indices = dummy_values.argmax(axis=1)\n",
    "            \n",
    "            # Reverter para valores originais\n",
    "            reverse_map = label_mappings[f'{col}_reverse']\n",
    "            result[col] = [reverse_map[idx] for idx in max_indices]\n",
    "    \n",
    "    # Garantir ordem e tipos corretos\n",
    "    result = result[data_missing.columns]\n",
    "    \n",
    "    # Pclass deve ser int\n",
    "    if 'Pclass' in result.columns:\n",
    "        result['Pclass'] = result['Pclass'].astype(int)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def impute_mice_titanic(data_missing, random_state=42):\n",
    "    \"\"\"MICE para TITANIC - BayesianRidge.\"\"\"\n",
    "    data_encoded = data_missing.copy()\n",
    "    label_mappings = {}\n",
    "    categorical_cols = ['Sex', 'Embarked', 'Pclass']\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        unique_vals = data_encoded[col].dropna().unique()\n",
    "        mapping = {val: idx for idx, val in enumerate(sorted(unique_vals))}\n",
    "        reverse_mapping = {idx: val for val, idx in mapping.items()}\n",
    "        label_mappings[col] = mapping\n",
    "        label_mappings[f'{col}_reverse'] = reverse_mapping\n",
    "        data_encoded[col] = data_encoded[col].map(mapping)\n",
    "    \n",
    "    imputer = IterativeImputer(max_iter=10, random_state=random_state)\n",
    "    data_imputed_array = imputer.fit_transform(data_encoded)\n",
    "    data_imputed = pd.DataFrame(data_imputed_array, columns=data_encoded.columns, index=data_encoded.index)\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        n_classes = len(label_mappings[col])\n",
    "        data_imputed[col] = data_imputed[col].round().clip(0, n_classes - 1).astype(int)\n",
    "        reverse_map = label_mappings[f'{col}_reverse']\n",
    "        data_imputed[col] = data_imputed[col].map(reverse_map)\n",
    "    \n",
    "    if 'Pclass' in data_imputed.columns:\n",
    "        data_imputed['Pclass'] = data_imputed['Pclass'].astype(int)\n",
    "    \n",
    "    return data_imputed\n",
    "\n",
    "def impute_missforest_titanic(data_missing, random_state=42):\n",
    "    \"\"\"MissForest para TITANIC - RandomForest.\"\"\"\n",
    "    data_encoded = data_missing.copy()\n",
    "    label_mappings = {}\n",
    "    categorical_cols = ['Sex', 'Embarked', 'Pclass']\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        unique_vals = data_encoded[col].dropna().unique()\n",
    "        mapping = {val: idx for idx, val in enumerate(sorted(unique_vals))}\n",
    "        reverse_mapping = {idx: val for val, idx in mapping.items()}\n",
    "        label_mappings[col] = mapping\n",
    "        label_mappings[f'{col}_reverse'] = reverse_mapping\n",
    "        data_encoded[col] = data_encoded[col].map(mapping)\n",
    "    \n",
    "    imputer = IterativeImputer(\n",
    "        estimator=RandomForestRegressor(n_estimators=10, random_state=random_state),\n",
    "        max_iter=10, sample_posterior=False, random_state=random_state\n",
    "    )\n",
    "    data_imputed_array = imputer.fit_transform(data_encoded)\n",
    "    data_imputed = pd.DataFrame(data_imputed_array, columns=data_encoded.columns, index=data_encoded.index)\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        n_classes = len(label_mappings[col])\n",
    "        data_imputed[col] = data_imputed[col].round().clip(0, n_classes - 1).astype(int)\n",
    "        reverse_map = label_mappings[f'{col}_reverse']\n",
    "        data_imputed[col] = data_imputed[col].map(reverse_map)\n",
    "    \n",
    "    if 'Pclass' in data_imputed.columns:\n",
    "        data_imputed['Pclass'] = data_imputed['Pclass'].astype(int)\n",
    "    \n",
    "    return data_imputed\n",
    "\n",
    "print(\"\\n✓ Funções de imputação definidas:\")\n",
    "print(\"  - ISCA-k: force_ordinal={'Pclass': [1,2,3]}\")\n",
    "print(\"  - KNN: one-hot encode → standardize → impute → reverse\")\n",
    "print(\"  - MICE: label encode → RandomForest → impute → reverse\")\n",
    "print(\"  - MissForest: aproximado com IterativeImputer + RF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76c8694-48aa-48e3-8376-8dd75de4fbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetar o dataset para garantir consistência\n",
    "data_titanic = data_titanic_complete.copy()\n",
    "\n",
    "# Dicionário para armazenar resultados\n",
    "results_titanic = {}\n",
    "\n",
    "# TESTE: Limitar a 2 runs\n",
    "TEST_MODE = False\n",
    "MAX_TEST_RUNS = 2\n",
    "\n",
    "# Contadores\n",
    "if TEST_MODE:\n",
    "    print(f\"\\n⚠️  MODO TESTE: Limitado a {MAX_TEST_RUNS} runs\")\n",
    "    total_runs = MAX_TEST_RUNS\n",
    "else:\n",
    "    total_runs = len(MECHANISMS) * len(MISSING_RATES) * len(SEEDS) * len(METHOD_ORDER)\n",
    "\n",
    "current_run = 0\n",
    "\n",
    "for mechanism in MECHANISMS:\n",
    "    print(f\"\\n{mechanism}:\")\n",
    "    \n",
    "    for missing_rate in MISSING_RATES:\n",
    "        print(f\"  Missing rate {int(missing_rate*100)}%: \", end=\"\")\n",
    "        \n",
    "        for run_idx, seed in enumerate(SEEDS):\n",
    "            \n",
    "            # TESTE: Parar após MAX_TEST_RUNS\n",
    "            if TEST_MODE and current_run >= MAX_TEST_RUNS:\n",
    "                print(\"\\n⚠️  Limite de teste atingido, parando...\")\n",
    "                break\n",
    "            \n",
    "            # Introduzir missings\n",
    "            if mechanism == 'MCAR':\n",
    "                data_missing = introduce_mcar(data_titanic, missing_rate, seed)\n",
    "            elif mechanism == 'MAR':\n",
    "                data_missing = introduce_mar(data_titanic, missing_rate, seed)\n",
    "            else:\n",
    "                data_missing = introduce_mnar(data_titanic, missing_rate, seed)\n",
    "            \n",
    "            missing_mask = data_missing.isna()\n",
    "            \n",
    "            # Testar cada método\n",
    "            for method in METHOD_ORDER:\n",
    "                current_run += 1\n",
    "                \n",
    "                try:\n",
    "                    start_time = time.time()\n",
    "                    \n",
    "                    if method == 'ISCA-k':\n",
    "                        data_imputed = impute_iscak_titanic(data_missing)\n",
    "                    \n",
    "                    elif method == 'KNN':\n",
    "                        # Grid search: testar k=[3,5,7,9,11]\n",
    "                        best_result = None\n",
    "                        best_metrics = None\n",
    "                        best_k = None\n",
    "                        \n",
    "                        for k in [3, 5, 7, 9, 11]:\n",
    "                            data_imputed_k = impute_knn_titanic(data_missing, k=k)\n",
    "                            metrics_k = calculate_metrics(data_titanic, data_imputed_k, missing_mask)\n",
    "                            \n",
    "                            # Escolher k com melhor NRMSE (ou accuracy para categóricas)\n",
    "                            if best_metrics is None or (\n",
    "                                not np.isnan(metrics_k.get('NRMSE', np.inf)) and \n",
    "                                metrics_k.get('NRMSE', np.inf) < best_metrics.get('NRMSE', np.inf)\n",
    "                            ):\n",
    "                                best_result = data_imputed_k\n",
    "                                best_metrics = metrics_k\n",
    "                                best_k = k\n",
    "                        \n",
    "                        data_imputed = best_result\n",
    "                        elapsed_time = time.time() - start_time\n",
    "                        \n",
    "                        best_metrics['Time'] = elapsed_time\n",
    "                        best_metrics['Best_k'] = best_k\n",
    "                        \n",
    "                        # Guardar\n",
    "                        key = (mechanism, missing_rate, method)\n",
    "                        if key not in results_titanic:\n",
    "                            results_titanic[key] = []\n",
    "                        results_titanic[key].append(best_metrics)\n",
    "                        \n",
    "                        continue\n",
    "                    \n",
    "                    elif method == 'MICE':\n",
    "                        data_imputed = impute_mice_titanic(data_missing, random_state=seed)\n",
    "                    \n",
    "                    else:  # MissForest\n",
    "                        data_imputed = impute_missforest_titanic(data_missing, random_state=seed)\n",
    "                    \n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    \n",
    "                    # Calcular métricas\n",
    "                    metrics = calculate_metrics(data_titanic, data_imputed, missing_mask)\n",
    "                    metrics['Time'] = elapsed_time\n",
    "                    \n",
    "                    # Guardar\n",
    "                    key = (mechanism, missing_rate, method)\n",
    "                    if key not in results_titanic:\n",
    "                        results_titanic[key] = []\n",
    "                    results_titanic[key].append(metrics)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"\\nERRO em {method}: {e}\")\n",
    "                    import traceback\n",
    "                    traceback.print_exc()\n",
    "                    continue\n",
    "            \n",
    "            print(\".\", end=\"\", flush=True)\n",
    "        \n",
    "        # TESTE: Parar após MAX_TEST_RUNS\n",
    "        if TEST_MODE and current_run >= MAX_TEST_RUNS:\n",
    "            break\n",
    "        \n",
    "        print(\" OK\")\n",
    "    \n",
    "    # TESTE: Parar após MAX_TEST_RUNS\n",
    "    if TEST_MODE and current_run >= MAX_TEST_RUNS:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa65b210-e7dd-48b7-8b42-7ea6136e1d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mechanism in MECHANISMS:\n",
    "    display_results_table(results_titanic, mechanism, \"TITANIC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251dfdef-1810-4fba-a60d-68099c004110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wilcoxon para 30% em cada mecanismo\n",
    "for mechanism in MECHANISMS:\n",
    "    print(f\"\\n{mechanism} - 30%:\")\n",
    "    display_statistical_tests(results_titanic, mechanism, 0.30, \"TITANIC\", metric='NRMSE')\n",
    "\n",
    "# Friedman test para 30% MCAR\n",
    "print(\"\\n\")\n",
    "display_friedman_results(results_titanic, 'MCAR', 0.30, \"TITANIC\", metric='NRMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caab07a-510e-4048-9ea7-5f5d763e918e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nGerando curvas de degradação...\")\n",
    "plot_degradation_curves(results_titanic, \"TITANIC\", metric='NRMSE')\n",
    "\n",
    "print(\"Gerando comparação de métodos...\")\n",
    "plot_method_comparison(results_titanic, \"TITANIC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4624c67b-cf5e-4698-ba97-ed04223e8aab",
   "metadata": {},
   "source": [
    "##### CREDIT APPROVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb73eae0-9f73-45ca-b1c0-9466563115b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.impute import KNNImputer, IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17ae399-9124-41ac-9ec4-93e3905e1033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dataset Credit Approval do UCI\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.data\"\n",
    "\n",
    "# Nomes das colunas conforme UCI repository\n",
    "column_names = ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', \n",
    "                'A10', 'A11', 'A12', 'A13', 'A14', 'A15', 'A16']\n",
    "\n",
    "data_credit_raw = pd.read_csv(url, header=None, names=column_names, na_values='?')\n",
    "\n",
    "print(f\"\\nShape original: {data_credit_raw.shape}\")\n",
    "print(f\"\\nColunas: {list(data_credit_raw.columns)}\")\n",
    "print(f\"\\nTipos originais:\\n{data_credit_raw.dtypes}\")\n",
    "\n",
    "# Tipos conforme UCI:\n",
    "# A1: b, a (nominal)\n",
    "# A2: continuous (numérico)\n",
    "# A3: continuous (numérico)\n",
    "# A4: u, y, l, t (nominal)\n",
    "# A5: g, p, gg (nominal)\n",
    "# A6: c, d, cc, i, j, k, m, r, q, w, x, e, aa, ff (nominal)\n",
    "# A7: v, h, bb, j, n, z, dd, ff, o (nominal)\n",
    "# A8: continuous (numérico)\n",
    "# A9: t, f (binária)\n",
    "# A10: t, f (binária)\n",
    "# A11: continuous (numérico)\n",
    "# A12: t, f (binária)\n",
    "# A13: g, p, s (nominal)\n",
    "# A14: continuous (numérico)\n",
    "# A15: continuous (numérico)\n",
    "# A16: +, - (target - NÃO USAR)\n",
    "\n",
    "# Remover coluna target (A16)\n",
    "features_to_use = ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', \n",
    "                   'A10', 'A11', 'A12', 'A13', 'A14', 'A15']\n",
    "\n",
    "data_credit = data_credit_raw[features_to_use].copy()\n",
    "\n",
    "print(f\"\\nMissings nativos por coluna:\")\n",
    "print(data_credit.isna().sum())\n",
    "\n",
    "# Converter tipos corretos\n",
    "# Numéricas\n",
    "numeric_cols = ['A2', 'A3', 'A8', 'A11', 'A14', 'A15']\n",
    "for col in numeric_cols:\n",
    "    data_credit[col] = pd.to_numeric(data_credit[col], errors='coerce')\n",
    "\n",
    "print(f\"\\nTipos após conversão:\")\n",
    "print(data_credit.dtypes)\n",
    "\n",
    "# Remover linhas com missings nativos\n",
    "data_credit_complete = data_credit.dropna()\n",
    "\n",
    "print(f\"\\nShape após remover missings nativos: {data_credit_complete.shape}\")\n",
    "print(f\"Missings restantes: {data_credit_complete.isna().sum().sum()}\")\n",
    "\n",
    "# Verificar\n",
    "assert data_credit_complete.isna().sum().sum() == 0, \"Dataset ainda tem missings!\"\n",
    "\n",
    "print(\"\\n✓ Dataset completo e limpo\")\n",
    "print(f\"Shape final: {data_credit_complete.shape}\")\n",
    "print(f\"\\nPrimeiras linhas:\")\n",
    "print(data_credit_complete.head())\n",
    "print(f\"\\nEstatísticas descritivas (numéricas):\")\n",
    "print(data_credit_complete[numeric_cols].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fde07b0-2da7-4fc6-8090-f727fb5f7a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANTE: Credit Approval NÃO tem variáveis ordinais claras\n",
    "# Todas as categóricas são NOMINAIS\n",
    "\n",
    "force_categorical_credit = None  # Detectar automaticamente\n",
    "force_ordinal_credit = None  # Não há ordinais neste dataset\n",
    "\n",
    "print(\"\\nforce_categorical:\", force_categorical_credit)\n",
    "print(\"force_ordinal:\", force_ordinal_credit)\n",
    "print(\"\\nVariáveis detectadas automaticamente:\")\n",
    "print(f\"  - Numéricas: {numeric_cols}\")\n",
    "print(f\"  - Nominais: A1, A4, A5, A6, A7, A13 (categóricas)\")\n",
    "print(f\"  - Binárias: A9, A10, A12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594fb845-2f3b-4512-9a9a-2750578536cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_iscak_credit(data_missing):\n",
    "    \"\"\"ISCA-k para CREDIT (misto).\"\"\"\n",
    "    imputer = ISCAkCore(verbose=False)\n",
    "    result = imputer.impute(\n",
    "        data_missing,\n",
    "        force_categorical=force_categorical_credit,\n",
    "        force_ordinal=force_ordinal_credit,\n",
    "        interactive=False\n",
    "    )\n",
    "    return result\n",
    "\n",
    "def impute_knn_credit(data_missing, k=5):\n",
    "    \"\"\"\n",
    "    KNN para CREDIT (misto).\n",
    "    \n",
    "    Estratégia: Label encode categóricas, one-hot, imputa, reverte tipos originais.\n",
    "    \"\"\"\n",
    "    # Separar por tipo\n",
    "    numeric_cols_knn = ['A2', 'A3', 'A8', 'A11', 'A14', 'A15']\n",
    "    categorical_cols_knn = ['A1', 'A4', 'A5', 'A6', 'A7', 'A9', 'A10', 'A12', 'A13']\n",
    "    \n",
    "    # Label encode\n",
    "    data_encoded = data_missing.copy()\n",
    "    label_mappings = {}\n",
    "    \n",
    "    for col in categorical_cols_knn:\n",
    "        unique_vals = data_encoded[col].dropna().unique()\n",
    "        mapping = {val: idx for idx, val in enumerate(sorted(unique_vals))}\n",
    "        reverse_mapping = {idx: val for val, idx in mapping.items()}\n",
    "        label_mappings[col] = mapping\n",
    "        label_mappings[f'{col}_reverse'] = reverse_mapping\n",
    "        data_encoded[col] = data_encoded[col].map(mapping)\n",
    "    \n",
    "    # One-hot encode\n",
    "    encoded_dfs = []\n",
    "    for col in categorical_cols_knn:\n",
    "        dummies = pd.get_dummies(data_encoded[col], prefix=col, drop_first=False, dtype=float)\n",
    "        mask = data_encoded[col].isna()\n",
    "        dummies.loc[mask, :] = np.nan\n",
    "        encoded_dfs.append(dummies)\n",
    "    \n",
    "    # Combinar\n",
    "    data_for_knn = pd.concat([data_encoded[numeric_cols_knn]] + encoded_dfs, axis=1)\n",
    "    \n",
    "    # Standardizar numéricas\n",
    "    means = data_for_knn[numeric_cols_knn].mean()\n",
    "    stds = data_for_knn[numeric_cols_knn].std().replace(0, 1)\n",
    "    data_for_knn[numeric_cols_knn] = (data_for_knn[numeric_cols_knn] - means) / stds\n",
    "    \n",
    "    # Imputar\n",
    "    imputer = KNNImputer(n_neighbors=k, weights='distance')\n",
    "    data_imputed_array = imputer.fit_transform(data_for_knn)\n",
    "    data_imputed_knn = pd.DataFrame(\n",
    "        data_imputed_array,\n",
    "        columns=data_for_knn.columns,\n",
    "        index=data_for_knn.index\n",
    "    )\n",
    "    \n",
    "    # Inverse transform\n",
    "    data_imputed_knn[numeric_cols_knn] = (data_imputed_knn[numeric_cols_knn] * stds) + means\n",
    "    \n",
    "    # Resultado\n",
    "    result = pd.DataFrame(index=data_missing.index)\n",
    "    result[numeric_cols_knn] = data_imputed_knn[numeric_cols_knn]\n",
    "    \n",
    "    # Reverter categóricas\n",
    "    for col in categorical_cols_knn:\n",
    "        dummy_cols = [c for c in data_imputed_knn.columns if c.startswith(f\"{col}_\")]\n",
    "        if len(dummy_cols) > 0:\n",
    "            dummy_values = data_imputed_knn[dummy_cols].values\n",
    "            max_indices = dummy_values.argmax(axis=1)\n",
    "            reverse_map = label_mappings[f'{col}_reverse']\n",
    "            result[col] = [reverse_map[idx] for idx in max_indices]\n",
    "    \n",
    "    # Ordem original\n",
    "    result = result[data_missing.columns]\n",
    "    \n",
    "    return result\n",
    "\n",
    "def impute_mice_credit(data_missing, random_state=42):\n",
    "    \"\"\"MICE para CREDIT - BayesianRidge.\"\"\"\n",
    "    data_encoded = data_missing.copy()\n",
    "    label_mappings = {}\n",
    "    categorical_cols = ['A1', 'A4', 'A5', 'A6', 'A7', 'A9', 'A10', 'A12', 'A13']\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        unique_vals = data_encoded[col].dropna().unique()\n",
    "        mapping = {val: idx for idx, val in enumerate(sorted(unique_vals))}\n",
    "        reverse_mapping = {idx: val for val, idx in mapping.items()}\n",
    "        label_mappings[col] = mapping\n",
    "        label_mappings[f'{col}_reverse'] = reverse_mapping\n",
    "        data_encoded[col] = data_encoded[col].map(mapping)\n",
    "    \n",
    "    imputer = IterativeImputer(max_iter=10, random_state=random_state)\n",
    "    data_imputed_array = imputer.fit_transform(data_encoded)\n",
    "    data_imputed = pd.DataFrame(data_imputed_array, columns=data_encoded.columns, index=data_encoded.index)\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        n_classes = len(label_mappings[col])\n",
    "        data_imputed[col] = data_imputed[col].round().clip(0, n_classes - 1).astype(int)\n",
    "        reverse_map = label_mappings[f'{col}_reverse']\n",
    "        data_imputed[col] = data_imputed[col].map(reverse_map)\n",
    "    \n",
    "    return data_imputed\n",
    "\n",
    "def impute_missforest_credit(data_missing, random_state=42):\n",
    "    \"\"\"MissForest para CREDIT - RandomForest.\"\"\"\n",
    "    data_encoded = data_missing.copy()\n",
    "    label_mappings = {}\n",
    "    categorical_cols = ['A1', 'A4', 'A5', 'A6', 'A7', 'A9', 'A10', 'A12', 'A13']\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        unique_vals = data_encoded[col].dropna().unique()\n",
    "        mapping = {val: idx for idx, val in enumerate(sorted(unique_vals))}\n",
    "        reverse_mapping = {idx: val for val, idx in mapping.items()}\n",
    "        label_mappings[col] = mapping\n",
    "        label_mappings[f'{col}_reverse'] = reverse_mapping\n",
    "        data_encoded[col] = data_encoded[col].map(mapping)\n",
    "    \n",
    "    imputer = IterativeImputer(\n",
    "        estimator=RandomForestRegressor(n_estimators=10, random_state=random_state),\n",
    "        max_iter=10, sample_posterior=False, random_state=random_state\n",
    "    )\n",
    "    data_imputed_array = imputer.fit_transform(data_encoded)\n",
    "    data_imputed = pd.DataFrame(data_imputed_array, columns=data_encoded.columns, index=data_encoded.index)\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        n_classes = len(label_mappings[col])\n",
    "        data_imputed[col] = data_imputed[col].round().clip(0, n_classes - 1).astype(int)\n",
    "        reverse_map = label_mappings[f'{col}_reverse']\n",
    "        data_imputed[col] = data_imputed[col].map(reverse_map)\n",
    "    \n",
    "    return data_imputed\n",
    "\n",
    "print(\"\\n✓ Funções de imputação definidas:\")\n",
    "print(\"  - ISCA-k: sem force_ordinal (só nominais)\")\n",
    "print(\"  - KNN: one-hot → standardize → impute → reverse\")\n",
    "print(\"  - MICE: label encode → RandomForest → reverse\")\n",
    "print(\"  - MissForest: aproximado com MICE+RF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0127a4b3-a246-4212-abc7-156b76b7742b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset dataset\n",
    "data_credit = data_credit_complete.copy()\n",
    "\n",
    "# Resultados\n",
    "results_credit = {}\n",
    "\n",
    "# TESTE: Limitar a 2 runs\n",
    "TEST_MODE = False\n",
    "MAX_TEST_RUNS = 2\n",
    "\n",
    "if TEST_MODE:\n",
    "    print(f\"\\n⚠️  MODO TESTE: Limitado a {MAX_TEST_RUNS} runs\")\n",
    "    total_runs = MAX_TEST_RUNS\n",
    "else:\n",
    "    total_runs = len(MECHANISMS) * len(MISSING_RATES) * len(SEEDS) * len(METHOD_ORDER)\n",
    "\n",
    "current_run = 0\n",
    "\n",
    "for mechanism in MECHANISMS:\n",
    "    print(f\"\\n{mechanism}:\")\n",
    "    \n",
    "    for missing_rate in MISSING_RATES:\n",
    "        print(f\"  Missing rate {int(missing_rate*100)}%: \", end=\"\")\n",
    "        \n",
    "        for run_idx, seed in enumerate(SEEDS):\n",
    "            \n",
    "            # TESTE: Parar\n",
    "            if TEST_MODE and current_run >= MAX_TEST_RUNS:\n",
    "                print(\"\\n⚠️  Limite de teste atingido, parando...\")\n",
    "                break\n",
    "            \n",
    "            # Introduzir missings\n",
    "            if mechanism == 'MCAR':\n",
    "                data_missing = introduce_mcar(data_credit, missing_rate, seed)\n",
    "            elif mechanism == 'MAR':\n",
    "                data_missing = introduce_mar(data_credit, missing_rate, seed)\n",
    "            else:\n",
    "                data_missing = introduce_mnar(data_credit, missing_rate, seed)\n",
    "            \n",
    "            missing_mask = data_missing.isna()\n",
    "            \n",
    "            # Testar cada método\n",
    "            for method in METHOD_ORDER:\n",
    "                current_run += 1\n",
    "                \n",
    "                try:\n",
    "                    start_time = time.time()\n",
    "                    \n",
    "                    if method == 'ISCA-k':\n",
    "                        data_imputed = impute_iscak_credit(data_missing)\n",
    "                    \n",
    "                    elif method == 'KNN':\n",
    "                        # Grid search\n",
    "                        best_result = None\n",
    "                        best_metrics = None\n",
    "                        best_k = None\n",
    "                        \n",
    "                        for k in [3, 5, 7, 9, 11]:\n",
    "                            data_imputed_k = impute_knn_credit(data_missing, k=k)\n",
    "                            metrics_k = calculate_metrics(data_credit, data_imputed_k, missing_mask)\n",
    "                            \n",
    "                            if best_metrics is None or (\n",
    "                                not np.isnan(metrics_k.get('NRMSE', np.inf)) and \n",
    "                                metrics_k.get('NRMSE', np.inf) < best_metrics.get('NRMSE', np.inf)\n",
    "                            ):\n",
    "                                best_result = data_imputed_k\n",
    "                                best_metrics = metrics_k\n",
    "                                best_k = k\n",
    "                        \n",
    "                        data_imputed = best_result\n",
    "                        elapsed_time = time.time() - start_time\n",
    "                        \n",
    "                        best_metrics['Time'] = elapsed_time\n",
    "                        best_metrics['Best_k'] = best_k\n",
    "                        \n",
    "                        key = (mechanism, missing_rate, method)\n",
    "                        if key not in results_credit:\n",
    "                            results_credit[key] = []\n",
    "                        results_credit[key].append(best_metrics)\n",
    "                        \n",
    "                        continue\n",
    "                    \n",
    "                    elif method == 'MICE':\n",
    "                        data_imputed = impute_mice_credit(data_missing, random_state=seed)\n",
    "                    \n",
    "                    else:  # MissForest\n",
    "                        data_imputed = impute_missforest_credit(data_missing, random_state=seed)\n",
    "                    \n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    \n",
    "                    # Métricas\n",
    "                    metrics = calculate_metrics(data_credit, data_imputed, missing_mask)\n",
    "                    metrics['Time'] = elapsed_time\n",
    "                    \n",
    "                    # Guardar\n",
    "                    key = (mechanism, missing_rate, method)\n",
    "                    if key not in results_credit:\n",
    "                        results_credit[key] = []\n",
    "                    results_credit[key].append(metrics)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"\\nERRO em {method}: {e}\")\n",
    "                    import traceback\n",
    "                    traceback.print_exc()\n",
    "                    continue\n",
    "            \n",
    "            print(\".\", end=\"\", flush=True)\n",
    "        \n",
    "        # TESTE: Parar\n",
    "        if TEST_MODE and current_run >= MAX_TEST_RUNS:\n",
    "            break\n",
    "        \n",
    "        print(\" OK\")\n",
    "    \n",
    "    # TESTE: Parar\n",
    "    if TEST_MODE and current_run >= MAX_TEST_RUNS:\n",
    "        break\n",
    "\n",
    "print(\"\\n✓ Benchmark completo!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e62db7-b3a6-48b0-8331-daac8395e5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mechanism in MECHANISMS:\n",
    "    display_results_table(results_credit, mechanism, \"CREDIT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9877a4f8-c092-49af-b913-4819e6a1463f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mechanism in MECHANISMS:\n",
    "    print(f\"\\n{mechanism} - 30%:\")\n",
    "    display_statistical_tests(results_credit, mechanism, 0.30, \"CREDIT\", metric='NRMSE')\n",
    "\n",
    "print(\"\\n\")\n",
    "display_friedman_results(results_credit, 'MCAR', 0.30, \"CREDIT\", metric='NRMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5752bb5-6d62-4212-8569-090c46160eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nGerando curvas de degradação...\")\n",
    "plot_degradation_curves(results_credit, \"CREDIT\", metric='NRMSE')\n",
    "\n",
    "print(\"Gerando comparação de métodos...\")\n",
    "plot_method_comparison(results_credit, \"CREDIT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e246cf-da5c-47a4-a4e2-b3ed3f4f2b1b",
   "metadata": {},
   "source": [
    "#### Datasets Categóricos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed124c64-040c-48d6-a0f4-30dd92d3c538",
   "metadata": {},
   "source": [
    "##### MUSHROOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e77adf1-d025-460c-b451-5b7f0170f0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.impute import KNNImputer, IterativeImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60442ba-510b-4854-86ae-25a672cc5afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dataset Mushroom do UCI\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data\"\n",
    "\n",
    "# Nomes das colunas conforme UCI\n",
    "column_names = [\n",
    "    'class',  # target (edible=e, poisonous=p) - NÃO USAR\n",
    "    'cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor',\n",
    "    'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color',\n",
    "    'stalk-shape', 'stalk-root', 'stalk-surface-above-ring',\n",
    "    'stalk-surface-below-ring', 'stalk-color-above-ring',\n",
    "    'stalk-color-below-ring', 'veil-type', 'veil-color',\n",
    "    'ring-number', 'ring-type', 'spore-print-color',\n",
    "    'population', 'habitat'\n",
    "]\n",
    "\n",
    "data_mushroom_raw = pd.read_csv(url, header=None, names=column_names, na_values='?')\n",
    "\n",
    "print(f\"\\nShape original: {data_mushroom_raw.shape}\")\n",
    "print(f\"\\nColunas: {list(data_mushroom_raw.columns)}\")\n",
    "\n",
    "# Remover coluna target (class)\n",
    "features_to_use = [col for col in column_names if col != 'class']\n",
    "\n",
    "data_mushroom = data_mushroom_raw[features_to_use].copy()\n",
    "\n",
    "print(f\"\\nMissings nativos por coluna:\")\n",
    "missing_counts = data_mushroom.isna().sum()\n",
    "print(missing_counts[missing_counts > 0])\n",
    "\n",
    "# Remover coluna 'stalk-root' que tem muitos missings (2480)\n",
    "# E remover 'veil-type' que é constante (sempre 'p')\n",
    "data_mushroom = data_mushroom.drop(['stalk-root', 'veil-type'], axis=1)\n",
    "\n",
    "print(f\"\\nColunas após remover stalk-root e veil-type:\")\n",
    "print(list(data_mushroom.columns))\n",
    "\n",
    "# Verificar e remover linhas com missings restantes (se houver)\n",
    "data_mushroom_complete = data_mushroom.dropna()\n",
    "\n",
    "print(f\"\\nShape após remover missings nativos: {data_mushroom_complete.shape}\")\n",
    "print(f\"Missings restantes: {data_mushroom_complete.isna().sum().sum()}\")\n",
    "\n",
    "# Verificar\n",
    "assert data_mushroom_complete.isna().sum().sum() == 0, \"Dataset ainda tem missings!\"\n",
    "\n",
    "print(\"\\n✓ Dataset completo e limpo\")\n",
    "print(f\"Shape final: {data_mushroom_complete.shape}\")\n",
    "print(f\"Total features: {data_mushroom_complete.shape[1]} (todas categóricas nominais)\")\n",
    "\n",
    "print(f\"\\nPrimeiras linhas:\")\n",
    "print(data_mushroom_complete.head())\n",
    "\n",
    "print(f\"\\nTipos de dados:\")\n",
    "print(data_mushroom_complete.dtypes)\n",
    "\n",
    "print(f\"\\nNúmero de categorias únicas por feature:\")\n",
    "for col in data_mushroom_complete.columns:\n",
    "    print(f\"  {col}: {data_mushroom_complete[col].nunique()} categorias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ac82a5-4856-4842-bcbe-ccc5477baa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "force_categorical_mushroom = None  # Auto-detect (todas são object)\n",
    "force_ordinal_mushroom = None      # Nenhuma ordinal\n",
    "\n",
    "print(\"\\nforce_categorical:\", force_categorical_mushroom)\n",
    "print(\"force_ordinal:\", force_ordinal_mushroom)\n",
    "print(\"\\nVariáveis: TODAS nominais (20 features categóricas)\")\n",
    "print(\"  - Nenhuma numérica\")\n",
    "print(\"  - Nenhuma ordinal\")\n",
    "print(\"  - Todas nominais/binárias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3114a008-fa69-453c-a40a-bd7db4029e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_iscak_mushroom(data_missing):\n",
    "    \"\"\"ISCA-k para MUSHROOM (categórico puro).\"\"\"\n",
    "    imputer = ISCAkCore(verbose=False)\n",
    "    result = imputer.impute(\n",
    "        data_missing,\n",
    "        force_categorical=force_categorical_mushroom,\n",
    "        force_ordinal=force_ordinal_mushroom,\n",
    "        interactive=False\n",
    "    )\n",
    "    return result\n",
    "\n",
    "def impute_knn_mushroom(data_missing, k=5):\n",
    "    \"\"\"\n",
    "    KNN para MUSHROOM (categórico puro).\n",
    "    \n",
    "    Estratégia: ONE-HOT ENCODING\n",
    "    - 20 features categóricas → ~100+ features binárias\n",
    "    - KNN imputa\n",
    "    - Reverte para categóricas originais\n",
    "    \"\"\"\n",
    "    # Label encode TODAS as features\n",
    "    data_encoded = data_missing.copy()\n",
    "    label_mappings = {}\n",
    "    all_cols = data_missing.columns.tolist()\n",
    "    \n",
    "    for col in all_cols:\n",
    "        unique_vals = data_encoded[col].dropna().unique()\n",
    "        mapping = {val: idx for idx, val in enumerate(sorted(unique_vals))}\n",
    "        reverse_mapping = {idx: val for val, idx in mapping.items()}\n",
    "        label_mappings[col] = mapping\n",
    "        label_mappings[f'{col}_reverse'] = reverse_mapping\n",
    "        data_encoded[col] = data_encoded[col].map(mapping)\n",
    "    \n",
    "    # One-hot encode TODAS\n",
    "    encoded_dfs = []\n",
    "    for col in all_cols:\n",
    "        dummies = pd.get_dummies(data_encoded[col], prefix=col, drop_first=False, dtype=float)\n",
    "        mask = data_encoded[col].isna()\n",
    "        dummies.loc[mask, :] = np.nan\n",
    "        encoded_dfs.append(dummies)\n",
    "    \n",
    "    # Combinar todas as dummies\n",
    "    data_for_knn = pd.concat(encoded_dfs, axis=1)\n",
    "    \n",
    "    print(f\"  [KNN] One-hot: {data_missing.shape[1]} features → {data_for_knn.shape[1]} features\")\n",
    "    \n",
    "    # Imputar (SEM standardização - são binárias)\n",
    "    imputer = KNNImputer(n_neighbors=k, weights='distance')\n",
    "    data_imputed_array = imputer.fit_transform(data_for_knn)\n",
    "    data_imputed_knn = pd.DataFrame(\n",
    "        data_imputed_array,\n",
    "        columns=data_for_knn.columns,\n",
    "        index=data_for_knn.index\n",
    "    )\n",
    "    \n",
    "    # Reverter categóricas\n",
    "    result = pd.DataFrame(index=data_missing.index)\n",
    "    \n",
    "    for col in all_cols:\n",
    "        dummy_cols = [c for c in data_imputed_knn.columns if c.startswith(f\"{col}_\")]\n",
    "        if len(dummy_cols) > 0:\n",
    "            # Escolher categoria com maior valor\n",
    "            dummy_values = data_imputed_knn[dummy_cols].values\n",
    "            max_indices = dummy_values.argmax(axis=1)\n",
    "            reverse_map = label_mappings[f'{col}_reverse']\n",
    "            result[col] = [reverse_map[idx] for idx in max_indices]\n",
    "    \n",
    "    return result\n",
    "\n",
    "def impute_mice_mushroom(data_missing, random_state=42):\n",
    "    \"\"\"MICE para MUSHROOM - BayesianRidge.\"\"\"\n",
    "    data_encoded = data_missing.copy()\n",
    "    label_mappings = {}\n",
    "    all_cols = data_missing.columns.tolist()\n",
    "    \n",
    "    for col in all_cols:\n",
    "        unique_vals = data_encoded[col].dropna().unique()\n",
    "        mapping = {val: idx for idx, val in enumerate(sorted(unique_vals))}\n",
    "        reverse_mapping = {idx: val for val, idx in mapping.items()}\n",
    "        label_mappings[col] = mapping\n",
    "        label_mappings[f'{col}_reverse'] = reverse_mapping\n",
    "        data_encoded[col] = data_encoded[col].map(mapping)\n",
    "    \n",
    "    imputer = IterativeImputer(max_iter=5, random_state=random_state)\n",
    "    data_imputed_array = imputer.fit_transform(data_encoded)\n",
    "    data_imputed = pd.DataFrame(data_imputed_array, columns=data_encoded.columns, index=data_encoded.index)\n",
    "    \n",
    "    for col in all_cols:\n",
    "        n_classes = len(label_mappings[col])\n",
    "        data_imputed[col] = data_imputed[col].round().clip(0, n_classes - 1).astype(int)\n",
    "        reverse_map = label_mappings[f'{col}_reverse']\n",
    "        data_imputed[col] = data_imputed[col].map(reverse_map)\n",
    "    \n",
    "    return data_imputed\n",
    "\n",
    "def impute_missforest_mushroom(data_missing, random_state=42):\n",
    "    \"\"\"MissForest para MUSHROOM - RandomForestClassifier.\"\"\"\n",
    "    data_encoded = data_missing.copy()\n",
    "    label_mappings = {}\n",
    "    all_cols = data_missing.columns.tolist()\n",
    "    \n",
    "    for col in all_cols:\n",
    "        unique_vals = data_encoded[col].dropna().unique()\n",
    "        mapping = {val: idx for idx, val in enumerate(sorted(unique_vals))}\n",
    "        reverse_mapping = {idx: val for val, idx in mapping.items()}\n",
    "        label_mappings[col] = mapping\n",
    "        label_mappings[f'{col}_reverse'] = reverse_mapping\n",
    "        data_encoded[col] = data_encoded[col].map(mapping)\n",
    "    \n",
    "    imputer = IterativeImputer(\n",
    "        estimator=RandomForestClassifier(n_estimators=5, random_state=random_state, max_depth=10),\n",
    "        max_iter=5, sample_posterior=False, random_state=random_state\n",
    "    )\n",
    "    data_imputed_array = imputer.fit_transform(data_encoded)\n",
    "    data_imputed = pd.DataFrame(data_imputed_array, columns=data_encoded.columns, index=data_encoded.index)\n",
    "    \n",
    "    for col in all_cols:\n",
    "        n_classes = len(label_mappings[col])\n",
    "        data_imputed[col] = data_imputed[col].round().clip(0, n_classes - 1).astype(int)\n",
    "        reverse_map = label_mappings[f'{col}_reverse']\n",
    "        data_imputed[col] = data_imputed[col].map(reverse_map)\n",
    "    \n",
    "    return data_imputed\n",
    "\n",
    "print(\"\\n✓ Funções de imputação definidas:\")\n",
    "print(\"  - ISCA-k: detecção automática (todas nominais)\")\n",
    "print(\"  - KNN: one-hot encode (~100 features) → impute → reverse\")\n",
    "print(\"  - MICE: label encode → RandomForestClassifier → reverse\")\n",
    "print(\"  - MissForest: aproximado (MICE+RFC)\")\n",
    "print(\"\\n⚠️  AVISO: One-hot encoding cria ~100 features - pode ser lento!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d95eb48-cd9a-4a3e-9c7c-2b46183d57bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset dataset\n",
    "data_mushroom = data_mushroom_complete.copy()\n",
    "\n",
    "# Resultados\n",
    "results_mushroom = {}\n",
    "\n",
    "# TESTE: Limitar a 2 runs\n",
    "TEST_MODE = False\n",
    "MAX_TEST_RUNS = 2\n",
    "\n",
    "# OTIMIZAÇÃO: Reduzir runs totais de 50 para 5\n",
    "SEEDS_MUSHROOM = [42, 123, 456, 789, 101]  # 5 seeds em vez de 50\n",
    "\n",
    "if TEST_MODE:\n",
    "    print(f\"\\n⚠️  MODO TESTE: Limitado a {MAX_TEST_RUNS} runs\")\n",
    "    total_runs = MAX_TEST_RUNS\n",
    "else:\n",
    "    # Usar apenas 5 seeds para Mushroom (em vez de len(SEEDS))\n",
    "    total_runs = len(MECHANISMS) * len(MISSING_RATES) * len(SEEDS_MUSHROOM) * len(METHOD_ORDER)\n",
    "    print(f\"\\n⚠️  MUSHROOM OTIMIZADO: 5 runs por configuração (em vez de 50)\")\n",
    "\n",
    "current_run = 0\n",
    "\n",
    "for mechanism in MECHANISMS:\n",
    "    print(f\"\\n{mechanism}:\")\n",
    "    \n",
    "    for missing_rate in MISSING_RATES:\n",
    "        print(f\"  Missing rate {int(missing_rate*100)}%: \", end=\"\")\n",
    "        \n",
    "        # Usar SEEDS_MUSHROOM em vez de SEEDS\n",
    "        seeds_to_use = SEEDS if TEST_MODE else SEEDS_MUSHROOM\n",
    "        \n",
    "        for run_idx, seed in enumerate(seeds_to_use):\n",
    "            \n",
    "            # TESTE: Parar\n",
    "            if TEST_MODE and current_run >= MAX_TEST_RUNS:\n",
    "                print(\"\\n⚠️  Limite de teste atingido, parando...\")\n",
    "                break\n",
    "            \n",
    "            # Introduzir missings\n",
    "            if mechanism == 'MCAR':\n",
    "                data_missing = introduce_mcar(data_mushroom, missing_rate, seed)\n",
    "            elif mechanism == 'MAR':\n",
    "                data_missing = introduce_mar(data_mushroom, missing_rate, seed)\n",
    "            else:\n",
    "                data_missing = introduce_mnar(data_mushroom, missing_rate, seed)\n",
    "            \n",
    "            missing_mask = data_missing.isna()\n",
    "            \n",
    "            # Testar cada método\n",
    "            for method in METHOD_ORDER:\n",
    "                current_run += 1\n",
    "                \n",
    "                try:\n",
    "                    start_time = time.time()\n",
    "                    \n",
    "                    if method == 'ISCA-k':\n",
    "                        data_imputed = impute_iscak_mushroom(data_missing)\n",
    "                    \n",
    "                    elif method == 'KNN':\n",
    "                        # Grid search REDUZIDO para Mushroom (só 3 valores de k)\n",
    "                        best_result = None\n",
    "                        best_metrics = None\n",
    "                        best_k = None\n",
    "                        \n",
    "                        # OTIMIZAÇÃO: testar apenas k=[3, 7, 11] em vez de [3,5,7,9,11]\n",
    "                        k_values = [3, 7, 11]\n",
    "                        \n",
    "                        for k in k_values:\n",
    "                            data_imputed_k = impute_knn_mushroom(data_missing, k=k)\n",
    "                            metrics_k = calculate_metrics(data_mushroom, data_imputed_k, missing_mask)\n",
    "                            \n",
    "                            # Para categórico: escolher k com melhor Accuracy\n",
    "                            if best_metrics is None or (\n",
    "                                not np.isnan(metrics_k.get('Accuracy', 0)) and \n",
    "                                metrics_k.get('Accuracy', 0) > best_metrics.get('Accuracy', 0)\n",
    "                            ):\n",
    "                                best_result = data_imputed_k\n",
    "                                best_metrics = metrics_k\n",
    "                                best_k = k\n",
    "                        \n",
    "                        data_imputed = best_result\n",
    "                        elapsed_time = time.time() - start_time\n",
    "                        \n",
    "                        best_metrics['Time'] = elapsed_time\n",
    "                        best_metrics['Best_k'] = best_k\n",
    "                        \n",
    "                        key = (mechanism, missing_rate, method)\n",
    "                        if key not in results_mushroom:\n",
    "                            results_mushroom[key] = []\n",
    "                        results_mushroom[key].append(best_metrics)\n",
    "                        \n",
    "                        continue\n",
    "                    \n",
    "                    elif method == 'MICE':\n",
    "                        data_imputed = impute_mice_mushroom(data_missing, random_state=seed)\n",
    "                    \n",
    "                    else:  # MissForest\n",
    "                        data_imputed = impute_missforest_mushroom(data_missing, random_state=seed)\n",
    "                    \n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    \n",
    "                    # Métricas\n",
    "                    metrics = calculate_metrics(data_mushroom, data_imputed, missing_mask)\n",
    "                    metrics['Time'] = elapsed_time\n",
    "                    \n",
    "                    # Guardar\n",
    "                    key = (mechanism, missing_rate, method)\n",
    "                    if key not in results_mushroom:\n",
    "                        results_mushroom[key] = []\n",
    "                    results_mushroom[key].append(metrics)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"\\nERRO em {method}: {e}\")\n",
    "                    import traceback\n",
    "                    traceback.print_exc()\n",
    "                    continue\n",
    "            \n",
    "            print(\".\", end=\"\", flush=True)\n",
    "        \n",
    "        # TESTE: Parar\n",
    "        if TEST_MODE and current_run >= MAX_TEST_RUNS:\n",
    "            break\n",
    "        \n",
    "        print(\" OK\")\n",
    "    \n",
    "    # TESTE: Parar\n",
    "    if TEST_MODE and current_run >= MAX_TEST_RUNS:\n",
    "        break\n",
    "\n",
    "print(\"\\n✓ Benchmark completo!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0284ab-4515-42b8-8504-e6a83d715669",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mechanism in MECHANISMS:\n",
    "    display_results_table(results_mushroom, mechanism, \"MUSHROOM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fa3197-74d0-4f91-bee7-fc203f44f605",
   "metadata": {},
   "source": [
    "for mechanism in MECHANISMS:\n",
    "    print(f\"\\n{mechanism} - 30%:\")\n",
    "    display_statistical_tests(results_mushroom, mechanism, 0.30, \"MUSHROOM\", metric='Accuracy')\n",
    "\n",
    "print(\"\\n\")\n",
    "display_friedman_results(results_mushroom, 'MCAR', 0.30, \"MUSHROOM\", metric='Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c17582-5a92-4d87-9bf4-b98b1470ab89",
   "metadata": {},
   "source": [
    "### APMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7078c1bc-593a-4b97-ae30-89ebb9cddc7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
