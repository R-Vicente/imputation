{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teste de Robustez do Scale Factor\n",
    "\n",
    "Testes adicionais antes de implementar:\n",
    "1. Robustez em taxas extremas (10-60%)\n",
    "2. Normalização por dimensionalidade efectiva\n",
    "3. Teste A/B com missings não aleatórios (viés)\n",
    "4. Visualização de distribuição de pesos vs missings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"Imports OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pds_distance_extended(sample, donors, weights, scale_mode='linear'):\n",
    "    \"\"\"\n",
    "    Calcula distâncias PDS com modos de scale factor estendidos.\n",
    "    \n",
    "    scale_mode:\n",
    "        - 'sqrt': sqrt(n/overlap) - actual\n",
    "        - 'none': 1.0\n",
    "        - 'linear': n/overlap - normalização por feature\n",
    "        - 'effective_dims': sqrt(n/effective_dims) onde effective = n - missing_both\n",
    "        - 'capped_linear': min(n/overlap, 2.0) - linear com cap\n",
    "        - 'adaptive': linear se overlap < 70%, senão none\n",
    "    \"\"\"\n",
    "    n_features = len(sample)\n",
    "    min_overlap = max(2, n_features // 2)\n",
    "    \n",
    "    distances = []\n",
    "    overlaps = []\n",
    "    scales_used = []\n",
    "    \n",
    "    sample_avail = ~np.isnan(sample)\n",
    "    sample_missing = np.isnan(sample)\n",
    "    \n",
    "    for donor in donors:\n",
    "        donor_avail = ~np.isnan(donor)\n",
    "        donor_missing = np.isnan(donor)\n",
    "        overlap_mask = sample_avail & donor_avail\n",
    "        overlap = overlap_mask.sum()\n",
    "        \n",
    "        # Missing em ambos (para effective_dims)\n",
    "        missing_both = (sample_missing & donor_missing).sum()\n",
    "        effective_dims = n_features - missing_both\n",
    "        \n",
    "        if overlap < min_overlap:\n",
    "            distances.append(np.inf)\n",
    "            overlaps.append(overlap)\n",
    "            scales_used.append(np.inf)\n",
    "            continue\n",
    "        \n",
    "        # Calcular distância raw\n",
    "        dist_sq = 0.0\n",
    "        weight_sum = 0.0\n",
    "        for j in range(n_features):\n",
    "            if overlap_mask[j]:\n",
    "                diff = sample[j] - donor[j]\n",
    "                dist_sq += weights[j] * diff * diff\n",
    "                weight_sum += weights[j]\n",
    "        \n",
    "        dist_raw = np.sqrt(dist_sq) if weight_sum > 0 else np.inf\n",
    "        \n",
    "        # Calcular scale factor\n",
    "        ratio = n_features / overlap\n",
    "        \n",
    "        if scale_mode == 'sqrt':\n",
    "            scale = np.sqrt(ratio)\n",
    "        elif scale_mode == 'none':\n",
    "            scale = 1.0\n",
    "        elif scale_mode == 'linear':\n",
    "            scale = ratio\n",
    "        elif scale_mode == 'effective_dims':\n",
    "            scale = np.sqrt(n_features / effective_dims) if effective_dims > 0 else 1.0\n",
    "        elif scale_mode == 'capped_linear':\n",
    "            scale = min(ratio, 2.0)  # Cap at 2x\n",
    "        elif scale_mode == 'adaptive':\n",
    "            # Linear se overlap < 70%, senão none\n",
    "            if overlap / n_features < 0.7:\n",
    "                scale = ratio\n",
    "            else:\n",
    "                scale = 1.0\n",
    "        else:\n",
    "            scale = 1.0\n",
    "        \n",
    "        distances.append(dist_raw * scale)\n",
    "        overlaps.append(overlap)\n",
    "        scales_used.append(scale)\n",
    "    \n",
    "    return np.array(distances), np.array(overlaps), np.array(scales_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_correlated_dataset(n_samples=150):\n",
    "    \"\"\"Dataset com features correlacionadas\"\"\"\n",
    "    F1 = np.random.normal(0, 1, n_samples)\n",
    "    F2 = F1 * 0.8 + np.random.normal(0, 0.5, n_samples)\n",
    "    F3 = np.random.normal(0, 1, n_samples)\n",
    "    F4 = F1 * 0.3 + F3 * 0.3 + np.random.normal(0, 0.7, n_samples)\n",
    "    Target = F1 * 2 + F2 * 1.5 + F4 * 0.5 + np.random.normal(0, 1, n_samples)\n",
    "    return pd.DataFrame({'F1': F1, 'F2': F2, 'F3': F3, 'F4': F4, 'Target': Target})\n",
    "\n",
    "def introduce_mcar(df, rate=0.2, seed=42):\n",
    "    \"\"\"Missings completamente aleatórios\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    mask = np.random.random(df.shape) < rate\n",
    "    return df.mask(mask)\n",
    "\n",
    "def introduce_biased_missings(df, rate=0.2, bias_cols=['F1', 'F2'], seed=42):\n",
    "    \"\"\"\n",
    "    Missings com viés: remove sempre as mesmas colunas em metade das amostras.\n",
    "    Simula padrão de missings não aleatório.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    df_missing = df.copy()\n",
    "    n = len(df)\n",
    "    \n",
    "    # Metade das amostras: remove bias_cols\n",
    "    biased_rows = np.random.choice(n, n//2, replace=False)\n",
    "    for col in bias_cols:\n",
    "        if col in df.columns:\n",
    "            df_missing.loc[biased_rows, col] = np.nan\n",
    "    \n",
    "    # Adicionar mais missings aleatórios para atingir rate\n",
    "    current_rate = df_missing.isna().sum().sum() / (n * len(df.columns))\n",
    "    if current_rate < rate:\n",
    "        additional_rate = (rate - current_rate) / (1 - current_rate)\n",
    "        mask = np.random.random(df.shape) < additional_rate\n",
    "        df_missing = df_missing.mask(mask | df_missing.isna())\n",
    "    \n",
    "    return df_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_imputation_test(df_complete, df_missing, scale_mode, k=5):\n",
    "    \"\"\"Executa imputação e retorna MAE\"\"\"\n",
    "    result = df_missing.copy()\n",
    "    n_features = len(df_missing.columns)\n",
    "    weights = np.ones(n_features) / n_features\n",
    "    \n",
    "    missing_positions = []\n",
    "    \n",
    "    for col in df_missing.columns:\n",
    "        missing_mask = df_missing[col].isna()\n",
    "        if not missing_mask.any():\n",
    "            continue\n",
    "        \n",
    "        donor_mask = ~df_missing[col].isna()\n",
    "        if donor_mask.sum() < 2:\n",
    "            continue\n",
    "        \n",
    "        donor_indices = df_missing[donor_mask].index.tolist()\n",
    "        donor_values = df_missing.loc[donor_mask, col].values\n",
    "        \n",
    "        for idx in df_missing[missing_mask].index:\n",
    "            missing_positions.append((idx, col))\n",
    "            \n",
    "            sample = df_missing.loc[idx].values.astype(float)\n",
    "            donors = df_missing.loc[donor_indices].values.astype(float)\n",
    "            \n",
    "            distances, _, _ = pds_distance_extended(sample, donors, weights, scale_mode)\n",
    "            \n",
    "            valid_mask = np.isfinite(distances)\n",
    "            if valid_mask.sum() < 1:\n",
    "                continue\n",
    "            \n",
    "            valid_distances = distances[valid_mask]\n",
    "            valid_values = donor_values[valid_mask]\n",
    "            \n",
    "            k_actual = min(k, len(valid_distances))\n",
    "            top_k_idx = np.argsort(valid_distances)[:k_actual]\n",
    "            \n",
    "            top_distances = valid_distances[top_k_idx]\n",
    "            top_values = valid_values[top_k_idx]\n",
    "            \n",
    "            if np.any(top_distances < 1e-10):\n",
    "                imputed = np.mean(top_values[top_distances < 1e-10])\n",
    "            else:\n",
    "                w = 1 / (top_distances + 1e-6)\n",
    "                w = w / w.sum()\n",
    "                imputed = np.average(top_values, weights=w)\n",
    "            \n",
    "            result.loc[idx, col] = imputed\n",
    "    \n",
    "    # Calcular MAE\n",
    "    errors = []\n",
    "    for idx, col in missing_positions:\n",
    "        true_val = df_complete.loc[idx, col]\n",
    "        imp_val = result.loc[idx, col]\n",
    "        if pd.notna(imp_val):\n",
    "            errors.append(abs(imp_val - true_val))\n",
    "    \n",
    "    return np.mean(errors) if errors else np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TESTE 1: Robustez em Taxas Extremas (10-60%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TESTE 1: ROBUSTEZ EM TAXAS EXTREMAS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "df_complete = create_correlated_dataset(200)\n",
    "\n",
    "scale_modes = ['sqrt', 'linear', 'none', 'capped_linear', 'adaptive', 'effective_dims']\n",
    "missing_rates = [0.10, 0.20, 0.30, 0.40, 0.50, 0.60]\n",
    "\n",
    "results_robustness = {mode: [] for mode in scale_modes}\n",
    "\n",
    "for rate in missing_rates:\n",
    "    print(f\"\\nTaxa: {rate*100:.0f}%\", end=\" \")\n",
    "    \n",
    "    for mode in scale_modes:\n",
    "        maes = []\n",
    "        for run in range(3):\n",
    "            df_missing = introduce_mcar(df_complete, rate, seed=42+run)\n",
    "            mae = run_imputation_test(df_complete, df_missing, mode)\n",
    "            if pd.notna(mae):\n",
    "                maes.append(mae)\n",
    "        \n",
    "        results_robustness[mode].append(np.mean(maes) if maes else np.nan)\n",
    "    \n",
    "    print(\"✓\")\n",
    "\n",
    "print(\"\\nConcluído!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabela de resultados\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTADOS: MAE por Taxa de Missing\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "header = f\"{'Mode':<15}\" + \"\".join([f\"{int(r*100)}%{'':>6}\" for r in missing_rates]) + \"Média\"\n",
    "print(header)\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for mode in scale_modes:\n",
    "    vals = results_robustness[mode]\n",
    "    mean_val = np.nanmean(vals)\n",
    "    row = f\"{mode:<15}\" + \"\".join([f\"{v:<10.4f}\" if pd.notna(v) else f\"{'N/A':<10}\" for v in vals])\n",
    "    row += f\"{mean_val:.4f}\"\n",
    "    print(row)\n",
    "\n",
    "# Encontrar melhor por taxa\n",
    "print(\"\\n--- MELHOR POR TAXA ---\")\n",
    "for i, rate in enumerate(missing_rates):\n",
    "    best_mode = min(scale_modes, key=lambda m: results_robustness[m][i] if pd.notna(results_robustness[m][i]) else 999)\n",
    "    best_val = results_robustness[best_mode][i]\n",
    "    print(f\"  {int(rate*100)}%: {best_mode} (MAE={best_val:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de evolução\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for mode in scale_modes:\n",
    "    vals = results_robustness[mode]\n",
    "    plt.plot([int(r*100) for r in missing_rates], vals, marker='o', label=mode, linewidth=2)\n",
    "\n",
    "plt.xlabel('Taxa de Missing (%)', fontsize=12)\n",
    "plt.ylabel('MAE', fontsize=12)\n",
    "plt.title('Evolução do MAE por Taxa de Missing', fontsize=14)\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Verificar ponto de viragem\n",
    "print(\"\\n--- ANÁLISE DE PONTO DE VIRAGEM ---\")\n",
    "linear_vals = results_robustness['linear']\n",
    "sqrt_vals = results_robustness['sqrt']\n",
    "\n",
    "for i, rate in enumerate(missing_rates):\n",
    "    diff = linear_vals[i] - sqrt_vals[i] if pd.notna(linear_vals[i]) and pd.notna(sqrt_vals[i]) else np.nan\n",
    "    winner = \"LINEAR\" if diff < 0 else \"SQRT\" if diff > 0 else \"EMPATE\"\n",
    "    print(f\"  {int(rate*100)}%: linear={linear_vals[i]:.4f}, sqrt={sqrt_vals[i]:.4f} → {winner} (diff={diff:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TESTE 2: Missings com Viés (Não Aleatórios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TESTE 2: MISSINGS COM VIÉS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nViés: F1 e F2 (features mais importantes) removidas em metade das amostras\")\n",
    "\n",
    "df_complete = create_correlated_dataset(200)\n",
    "\n",
    "# Testar com missings viesados\n",
    "results_biased = {mode: [] for mode in scale_modes}\n",
    "results_mcar = {mode: [] for mode in scale_modes}  # Comparação\n",
    "\n",
    "for rate in [0.20, 0.30, 0.40]:\n",
    "    print(f\"\\nTaxa: {rate*100:.0f}%\")\n",
    "    \n",
    "    for mode in scale_modes:\n",
    "        # Com viés\n",
    "        maes_biased = []\n",
    "        for run in range(3):\n",
    "            df_biased = introduce_biased_missings(df_complete, rate, bias_cols=['F1', 'F2'], seed=42+run)\n",
    "            mae = run_imputation_test(df_complete, df_biased, mode)\n",
    "            if pd.notna(mae):\n",
    "                maes_biased.append(mae)\n",
    "        \n",
    "        results_biased[mode].append(np.mean(maes_biased) if maes_biased else np.nan)\n",
    "        \n",
    "        # MCAR (comparação)\n",
    "        maes_mcar = []\n",
    "        for run in range(3):\n",
    "            df_mcar = introduce_mcar(df_complete, rate, seed=42+run)\n",
    "            mae = run_imputation_test(df_complete, df_mcar, mode)\n",
    "            if pd.notna(mae):\n",
    "                maes_mcar.append(mae)\n",
    "        \n",
    "        results_mcar[mode].append(np.mean(maes_mcar) if maes_mcar else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- COMPARAÇÃO: MCAR vs VIÉS ---\")\n",
    "print(f\"\\n{'Mode':<15} {'MCAR 20%':<12} {'Viés 20%':<12} {'MCAR 30%':<12} {'Viés 30%':<12} {'MCAR 40%':<12} {'Viés 40%':<12}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for mode in scale_modes:\n",
    "    mcar_vals = results_mcar[mode]\n",
    "    bias_vals = results_biased[mode]\n",
    "    row = f\"{mode:<15}\"\n",
    "    for i in range(3):\n",
    "        row += f\"{mcar_vals[i]:<12.4f}{bias_vals[i]:<12.4f}\"\n",
    "    print(row)\n",
    "\n",
    "# Qual método é mais robusto ao viés?\n",
    "print(\"\\n--- ROBUSTEZ AO VIÉS (menor degradação MCAR→Viés) ---\")\n",
    "for mode in scale_modes:\n",
    "    mcar_mean = np.nanmean(results_mcar[mode])\n",
    "    bias_mean = np.nanmean(results_biased[mode])\n",
    "    degradation = (bias_mean - mcar_mean) / mcar_mean * 100 if mcar_mean > 0 else np.nan\n",
    "    print(f\"  {mode:<15}: MCAR={mcar_mean:.4f}, Viés={bias_mean:.4f}, Degradação={degradation:+.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TESTE 3: Visualização de Distribuição de Pesos vs Missings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TESTE 3: DISTRIBUIÇÃO DE SCALE FACTORS vs OVERLAP\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Criar dataset com missings variados\n",
    "df_complete = create_correlated_dataset(200)\n",
    "df_missing = introduce_mcar(df_complete, rate=0.3, seed=42)\n",
    "\n",
    "n_features = len(df_missing.columns)\n",
    "weights = np.ones(n_features) / n_features\n",
    "\n",
    "# Recolher dados de overlap e scale factors\n",
    "overlap_data = {mode: [] for mode in scale_modes}\n",
    "scale_data = {mode: [] for mode in scale_modes}\n",
    "distance_data = {mode: [] for mode in scale_modes}\n",
    "\n",
    "# Para cada par sample-donor, recolher overlap e scale usado\n",
    "for idx in df_missing.index[:50]:  # Primeiras 50 amostras\n",
    "    sample = df_missing.loc[idx].values.astype(float)\n",
    "    donors = df_missing.values.astype(float)\n",
    "    \n",
    "    for mode in scale_modes:\n",
    "        distances, overlaps, scales = pds_distance_extended(sample, donors, weights, mode)\n",
    "        \n",
    "        valid = np.isfinite(distances)\n",
    "        overlap_data[mode].extend(overlaps[valid])\n",
    "        scale_data[mode].extend(scales[valid])\n",
    "        distance_data[mode].extend(distances[valid])\n",
    "\n",
    "print(f\"Recolhidos {len(overlap_data['linear'])} pares sample-donor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização: Scale Factor vs Overlap\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, mode in enumerate(scale_modes):\n",
    "    ax = axes[i]\n",
    "    overlaps = np.array(overlap_data[mode])\n",
    "    scales = np.array(scale_data[mode])\n",
    "    \n",
    "    # Scatter plot\n",
    "    ax.scatter(overlaps, scales, alpha=0.3, s=10)\n",
    "    \n",
    "    # Linha de tendência\n",
    "    unique_overlaps = np.unique(overlaps)\n",
    "    mean_scales = [np.mean(scales[overlaps == o]) for o in unique_overlaps]\n",
    "    ax.plot(unique_overlaps, mean_scales, 'r-', linewidth=2, label='Média')\n",
    "    \n",
    "    ax.set_xlabel('Overlap (n features)')\n",
    "    ax.set_ylabel('Scale Factor')\n",
    "    ax.set_title(f'{mode.upper()}')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "\n",
    "plt.suptitle('Scale Factor vs Overlap por Método', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlação entre overlap e scale factor\n",
    "print(\"\\n--- CORRELAÇÃO: Overlap vs Scale Factor ---\")\n",
    "print(\"(Idealmente próximo de 0 para independência, ou negativo para penalizar baixo overlap)\")\n",
    "\n",
    "for mode in scale_modes:\n",
    "    overlaps = np.array(overlap_data[mode])\n",
    "    scales = np.array(scale_data[mode])\n",
    "    \n",
    "    # Remover infinitos\n",
    "    valid = np.isfinite(scales)\n",
    "    if valid.sum() > 10:\n",
    "        corr = np.corrcoef(overlaps[valid], scales[valid])[0, 1]\n",
    "        print(f\"  {mode:<15}: r = {corr:+.4f}\")\n",
    "    else:\n",
    "        print(f\"  {mode:<15}: N/A (poucos dados)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização: Distribuição de distâncias por método\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, mode in enumerate(scale_modes):\n",
    "    ax = axes[i]\n",
    "    distances = np.array(distance_data[mode])\n",
    "    distances = distances[np.isfinite(distances)]  # Remover inf\n",
    "    \n",
    "    ax.hist(distances, bins=50, alpha=0.7, edgecolor='black')\n",
    "    ax.axvline(np.median(distances), color='r', linestyle='--', label=f'Mediana: {np.median(distances):.2f}')\n",
    "    ax.axvline(np.mean(distances), color='g', linestyle='--', label=f'Média: {np.mean(distances):.2f}')\n",
    "    \n",
    "    ax.set_xlabel('Distância')\n",
    "    ax.set_ylabel('Frequência')\n",
    "    ax.set_title(f'{mode.upper()}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Distribuição de Distâncias por Método', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## RESUMO FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*70)\nprint(\"TESTE 4: OTIMIZAÇÃO DO THRESHOLD ADAPTIVE\")\nprint(\"=\"*70)\n\ndef pds_distance_adaptive_threshold(sample, donors, weights, threshold):\n    \"\"\"\n    Adaptive com threshold configurável.\n    - overlap < threshold: usa linear (penalização forte)\n    - overlap >= threshold: usa sqrt (penalização suave)\n    \"\"\"\n    n_features = len(sample)\n    min_overlap = max(2, n_features // 2)\n    \n    distances = []\n    sample_avail = ~np.isnan(sample)\n    \n    for donor in donors:\n        donor_avail = ~np.isnan(donor)\n        overlap_mask = sample_avail & donor_avail\n        overlap = overlap_mask.sum()\n        \n        if overlap < min_overlap:\n            distances.append(np.inf)\n            continue\n        \n        # Distância raw\n        dist_sq = 0.0\n        weight_sum = 0.0\n        for j in range(n_features):\n            if overlap_mask[j]:\n                diff = sample[j] - donor[j]\n                dist_sq += weights[j] * diff * diff\n                weight_sum += weights[j]\n        \n        dist_raw = np.sqrt(dist_sq) if weight_sum > 0 else np.inf\n        \n        # Scale factor adaptive\n        ratio = n_features / overlap\n        overlap_ratio = overlap / n_features\n        \n        if overlap_ratio < threshold:\n            scale = ratio  # linear\n        else:\n            scale = np.sqrt(ratio)  # sqrt\n        \n        distances.append(dist_raw * scale)\n    \n    return np.array(distances)\n\n\ndef run_imputation_adaptive_threshold(df_complete, df_missing, threshold, k=5):\n    \"\"\"Imputação com threshold específico\"\"\"\n    result = df_missing.copy()\n    n_features = len(df_missing.columns)\n    weights = np.ones(n_features) / n_features\n    \n    missing_positions = []\n    \n    for col in df_missing.columns:\n        missing_mask = df_missing[col].isna()\n        if not missing_mask.any():\n            continue\n        \n        donor_mask = ~df_missing[col].isna()\n        if donor_mask.sum() < 2:\n            continue\n        \n        donor_indices = df_missing[donor_mask].index.tolist()\n        donor_values = df_missing.loc[donor_mask, col].values\n        \n        for idx in df_missing[missing_mask].index:\n            missing_positions.append((idx, col))\n            \n            sample = df_missing.loc[idx].values.astype(float)\n            donors = df_missing.loc[donor_indices].values.astype(float)\n            \n            distances = pds_distance_adaptive_threshold(sample, donors, weights, threshold)\n            \n            valid_mask = np.isfinite(distances)\n            if valid_mask.sum() < 1:\n                continue\n            \n            valid_distances = distances[valid_mask]\n            valid_values = donor_values[valid_mask]\n            \n            k_actual = min(k, len(valid_distances))\n            top_k_idx = np.argsort(valid_distances)[:k_actual]\n            \n            top_distances = valid_distances[top_k_idx]\n            top_values = valid_values[top_k_idx]\n            \n            if np.any(top_distances < 1e-10):\n                imputed = np.mean(top_values[top_distances < 1e-10])\n            else:\n                w = 1 / (top_distances + 1e-6)\n                w = w / w.sum()\n                imputed = np.average(top_values, weights=w)\n            \n            result.loc[idx, col] = imputed\n    \n    # MAE\n    errors = []\n    for idx, col in missing_positions:\n        true_val = df_complete.loc[idx, col]\n        imp_val = result.loc[idx, col]\n        if pd.notna(imp_val):\n            errors.append(abs(imp_val - true_val))\n    \n    return np.mean(errors) if errors else np.nan\n\n\n# Testar thresholds\nthresholds = [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]  # 1.0 = sempre linear\nmissing_rates = [0.10, 0.20, 0.30, 0.40, 0.50]\n\ndf_complete = create_correlated_dataset(200)\n\nresults_threshold = {t: [] for t in thresholds}\n\nfor rate in missing_rates:\n    print(f\"\\nTaxa: {rate*100:.0f}%\", end=\" \")\n    \n    for threshold in thresholds:\n        maes = []\n        for run in range(3):\n            df_missing = introduce_mcar(df_complete, rate, seed=42+run)\n            mae = run_imputation_adaptive_threshold(df_complete, df_missing, threshold)\n            if pd.notna(mae):\n                maes.append(mae)\n        \n        results_threshold[threshold].append(np.mean(maes) if maes else np.nan)\n    \n    print(\"✓\")\n\nprint(\"\\nConcluído!\")"
  },
  {
   "cell_type": "code",
   "source": "# Tabela de resultados por threshold\nprint(\"\\n\" + \"=\"*70)\nprint(\"RESULTADOS: MAE por Threshold Adaptive\")\nprint(\"=\"*70)\n\nheader = f\"{'Threshold':<12}\" + \"\".join([f\"{int(r*100)}%{'':>6}\" for r in missing_rates]) + \"Média\"\nprint(header)\nprint(\"-\" * 70)\n\nfor threshold in thresholds:\n    vals = results_threshold[threshold]\n    mean_val = np.nanmean(vals)\n    row = f\"{threshold:<12.1f}\" + \"\".join([f\"{v:<10.4f}\" if pd.notna(v) else f\"{'N/A':<10}\" for v in vals])\n    row += f\"{mean_val:.4f}\"\n    print(row)\n\n# Encontrar melhor threshold global\nbest_threshold = min(thresholds, key=lambda t: np.nanmean(results_threshold[t]))\nbest_mae = np.nanmean(results_threshold[best_threshold])\nprint(f\"\\n>>> MELHOR THRESHOLD GLOBAL: {best_threshold} (MAE médio = {best_mae:.4f})\")\n\n# Melhor por taxa\nprint(\"\\n--- MELHOR THRESHOLD POR TAXA ---\")\nfor i, rate in enumerate(missing_rates):\n    best_t = min(thresholds, key=lambda t: results_threshold[t][i] if pd.notna(results_threshold[t][i]) else 999)\n    best_v = results_threshold[best_t][i]\n    print(f\"  {int(rate*100)}%: threshold={best_t} (MAE={best_v:.4f})\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Gráfico: MAE vs Threshold por taxa de missing\nplt.figure(figsize=(12, 6))\n\nfor rate_idx, rate in enumerate(missing_rates):\n    maes = [results_threshold[t][rate_idx] for t in thresholds]\n    plt.plot(thresholds, maes, marker='o', label=f'{int(rate*100)}% missing', linewidth=2)\n\nplt.xlabel('Threshold (overlap ratio)', fontsize=12)\nplt.ylabel('MAE', fontsize=12)\nplt.title('MAE vs Threshold Adaptive por Taxa de Missing', fontsize=14)\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.xticks(thresholds)\nplt.tight_layout()\nplt.show()\n\n# Gráfico: MAE médio vs Threshold\nplt.figure(figsize=(10, 5))\n\nmean_maes = [np.nanmean(results_threshold[t]) for t in thresholds]\ncolors = ['green' if m == min(mean_maes) else 'steelblue' for m in mean_maes]\n\nplt.bar([str(t) for t in thresholds], mean_maes, color=colors, edgecolor='black')\nplt.xlabel('Threshold', fontsize=12)\nplt.ylabel('MAE Médio', fontsize=12)\nplt.title('MAE Médio por Threshold (verde = melhor)', fontsize=14)\nplt.grid(True, alpha=0.3, axis='y')\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nNota: threshold=1.0 significa 'sempre linear' (nunca usa sqrt)\")\nprint(f\"      threshold=0.0 significaria 'sempre sqrt' (nunca usa linear)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## CONCLUSÕES E RECOMENDAÇÕES\n\n### Interpretação do Threshold:\n- **threshold = 0.4**: Se overlap < 40% → linear, senão sqrt\n- **threshold = 0.7**: Se overlap < 70% → linear, senão sqrt  \n- **threshold = 1.0**: Sempre linear (sqrt nunca usado)\n\n### O que esperamos:\n- **Threshold baixo (0.4-0.5)**: Mais uso de sqrt → menos penalização → potencialmente pior em taxas altas\n- **Threshold alto (0.9-1.0)**: Mais uso de linear → mais penalização → melhor discriminação de donors",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}