{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teste de Scale Factors para PDS\n",
    "\n",
    "Este notebook testa diferentes opções de scale factor para encontrar a configuração óptima.\n",
    "\n",
    "**Hipótese a testar**: O problema pode ser over-correction (pesos MI re-normalizados + scale factor PDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from copy import deepcopy\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"Imports OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Funções de Distância com Scale Factors Configuráveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pds_distance_configurable(sample, donors, weights, scale_mode='sqrt'):\n",
    "    \"\"\"\n",
    "    Calcula distâncias PDS com diferentes modos de scale factor.\n",
    "    \n",
    "    scale_mode:\n",
    "        - 'sqrt': sqrt(n/overlap) - actual\n",
    "        - 'none': 1.0 - sem escala\n",
    "        - 'linear': n/overlap - linear\n",
    "        - 'log': 1 + log(n/overlap) - logarítmico\n",
    "        - 'conditional': só aplica se overlap < 70%\n",
    "        - 'inverse': sqrt(overlap/n) - recompensa overlap\n",
    "    \"\"\"\n",
    "    n_features = len(sample)\n",
    "    n_donors = len(donors)\n",
    "    min_overlap = max(2, n_features // 2)\n",
    "    \n",
    "    distances = []\n",
    "    \n",
    "    sample_avail = ~np.isnan(sample)\n",
    "    \n",
    "    for donor in donors:\n",
    "        donor_avail = ~np.isnan(donor)\n",
    "        overlap_mask = sample_avail & donor_avail\n",
    "        overlap = overlap_mask.sum()\n",
    "        \n",
    "        if overlap < min_overlap:\n",
    "            distances.append(np.inf)\n",
    "            continue\n",
    "        \n",
    "        # Calcular distância raw\n",
    "        dist_sq = 0.0\n",
    "        weight_sum = 0.0\n",
    "        for j in range(n_features):\n",
    "            if overlap_mask[j]:\n",
    "                diff = sample[j] - donor[j]\n",
    "                dist_sq += weights[j] * diff * diff\n",
    "                weight_sum += weights[j]\n",
    "        \n",
    "        dist_raw = np.sqrt(dist_sq) if weight_sum > 0 else np.inf\n",
    "        \n",
    "        # Aplicar scale factor conforme modo\n",
    "        ratio = n_features / overlap\n",
    "        \n",
    "        if scale_mode == 'sqrt':\n",
    "            scale = np.sqrt(ratio)\n",
    "        elif scale_mode == 'none':\n",
    "            scale = 1.0\n",
    "        elif scale_mode == 'linear':\n",
    "            scale = ratio\n",
    "        elif scale_mode == 'log':\n",
    "            scale = 1 + np.log(ratio)\n",
    "        elif scale_mode == 'conditional':\n",
    "            # Só aplica se overlap < 70%\n",
    "            if overlap / n_features < 0.7:\n",
    "                scale = np.sqrt(ratio)\n",
    "            else:\n",
    "                scale = 1.0\n",
    "        elif scale_mode == 'inverse':\n",
    "            # Recompensa overlap alto\n",
    "            scale = np.sqrt(overlap / n_features)\n",
    "        else:\n",
    "            scale = 1.0\n",
    "        \n",
    "        distances.append(dist_raw * scale)\n",
    "    \n",
    "    return np.array(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_with_scale_mode(df_missing, df_complete, scale_mode='sqrt', k=5):\n",
    "    \"\"\"\n",
    "    Imputa valores usando PDS com scale_mode configurável.\n",
    "    Versão simplificada para testes (sem MI, pesos iguais).\n",
    "    \"\"\"\n",
    "    result = df_missing.copy()\n",
    "    n_features = len(df_missing.columns)\n",
    "    weights = np.ones(n_features) / n_features  # Pesos iguais\n",
    "    \n",
    "    # Para cada célula missing\n",
    "    for col in df_missing.columns:\n",
    "        missing_mask = df_missing[col].isna()\n",
    "        if not missing_mask.any():\n",
    "            continue\n",
    "        \n",
    "        # Donors: linhas com este valor preenchido\n",
    "        donor_mask = ~df_missing[col].isna()\n",
    "        if donor_mask.sum() < 2:\n",
    "            continue\n",
    "        \n",
    "        donor_indices = df_missing[donor_mask].index.tolist()\n",
    "        donor_values = df_missing.loc[donor_mask, col].values\n",
    "        \n",
    "        for idx in df_missing[missing_mask].index:\n",
    "            sample = df_missing.loc[idx].values.astype(float)\n",
    "            donors = df_missing.loc[donor_indices].values.astype(float)\n",
    "            \n",
    "            # Calcular distâncias\n",
    "            distances = pds_distance_configurable(sample, donors, weights, scale_mode)\n",
    "            \n",
    "            # Seleccionar k vizinhos mais próximos\n",
    "            valid_mask = np.isfinite(distances)\n",
    "            if valid_mask.sum() < 1:\n",
    "                continue\n",
    "            \n",
    "            valid_distances = distances[valid_mask]\n",
    "            valid_values = donor_values[valid_mask]\n",
    "            \n",
    "            k_actual = min(k, len(valid_distances))\n",
    "            top_k_idx = np.argsort(valid_distances)[:k_actual]\n",
    "            \n",
    "            top_distances = valid_distances[top_k_idx]\n",
    "            top_values = valid_values[top_k_idx]\n",
    "            \n",
    "            # Média ponderada por distância inversa\n",
    "            if np.any(top_distances < 1e-10):\n",
    "                imputed = np.mean(top_values[top_distances < 1e-10])\n",
    "            else:\n",
    "                w = 1 / (top_distances + 1e-6)\n",
    "                w = w / w.sum()\n",
    "                imputed = np.average(top_values, weights=w)\n",
    "            \n",
    "            result.loc[idx, col] = imputed\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Criar Datasets de Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clustered_dataset(n_samples=100, n_features=10, n_clusters=3, noise=0.2):\n",
    "    \"\"\"Dataset com clusters bem separados\"\"\"\n",
    "    samples_per_cluster = n_samples // n_clusters\n",
    "    data = []\n",
    "    \n",
    "    for c in range(n_clusters):\n",
    "        center = c * 3  # Clusters em 0, 3, 6\n",
    "        target_center = (c + 1) * 10  # Targets em 10, 20, 30\n",
    "        \n",
    "        features = np.random.normal(center, noise, (samples_per_cluster, n_features))\n",
    "        targets = np.random.normal(target_center, 1.0, samples_per_cluster)\n",
    "        \n",
    "        cluster_data = np.column_stack([features, targets])\n",
    "        data.append(cluster_data)\n",
    "    \n",
    "    all_data = np.vstack(data)\n",
    "    cols = [f'F{i}' for i in range(n_features)] + ['Target']\n",
    "    return pd.DataFrame(all_data, columns=cols)\n",
    "\n",
    "def create_correlated_dataset(n_samples=150, n_features=5):\n",
    "    \"\"\"Dataset com features correlacionadas (mais realista)\"\"\"\n",
    "    F1 = np.random.normal(0, 1, n_samples)\n",
    "    F2 = F1 * 0.8 + np.random.normal(0, 0.5, n_samples)  # Correlação forte\n",
    "    F3 = np.random.normal(0, 1, n_samples)  # Independente\n",
    "    F4 = F1 * 0.3 + F3 * 0.3 + np.random.normal(0, 0.7, n_samples)  # Correlação média\n",
    "    Target = F1 * 2 + F2 * 1.5 + F4 * 0.5 + np.random.normal(0, 1, n_samples)\n",
    "    \n",
    "    return pd.DataFrame({'F1': F1, 'F2': F2, 'F3': F3, 'F4': F4, 'Target': Target})\n",
    "\n",
    "def introduce_missings(df, rate=0.2, seed=42):\n",
    "    \"\"\"Introduz missings MCAR\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    df_missing = df.copy()\n",
    "    mask = np.random.random(df.shape) < rate\n",
    "    df_missing = df_missing.mask(mask)\n",
    "    return df_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar datasets\n",
    "print(\"Criando datasets de teste...\")\n",
    "\n",
    "# Dataset 1: Clusters separados\n",
    "df_clustered = create_clustered_dataset(n_samples=150, n_features=8)\n",
    "print(f\"Dataset Clustered: {df_clustered.shape}\")\n",
    "\n",
    "# Dataset 2: Features correlacionadas\n",
    "df_correlated = create_correlated_dataset(n_samples=150)\n",
    "print(f\"Dataset Correlated: {df_correlated.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testar Scale Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_scale_test(df_complete, missing_rate, scale_modes, k=5, n_runs=3):\n",
    "    \"\"\"\n",
    "    Testa diferentes scale_modes num dataset.\n",
    "    Retorna MAE médio para cada modo.\n",
    "    \"\"\"\n",
    "    results = {mode: [] for mode in scale_modes}\n",
    "    \n",
    "    for run in range(n_runs):\n",
    "        # Introduzir missings com seed diferente\n",
    "        df_missing = introduce_missings(df_complete, rate=missing_rate, seed=42+run)\n",
    "        \n",
    "        # Guardar posições dos missings\n",
    "        missing_positions = []\n",
    "        for col in df_complete.columns:\n",
    "            for idx in df_missing.index:\n",
    "                if pd.isna(df_missing.loc[idx, col]):\n",
    "                    missing_positions.append((idx, col))\n",
    "        \n",
    "        if len(missing_positions) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Testar cada scale mode\n",
    "        for mode in scale_modes:\n",
    "            result = impute_with_scale_mode(df_missing.copy(), df_complete, scale_mode=mode, k=k)\n",
    "            \n",
    "            # Calcular MAE\n",
    "            errors = []\n",
    "            for idx, col in missing_positions:\n",
    "                true_val = df_complete.loc[idx, col]\n",
    "                imp_val = result.loc[idx, col]\n",
    "                if pd.notna(imp_val):\n",
    "                    errors.append(abs(imp_val - true_val))\n",
    "            \n",
    "            mae = np.mean(errors) if errors else np.nan\n",
    "            results[mode].append(mae)\n",
    "    \n",
    "    # Média dos runs\n",
    "    return {mode: np.mean(maes) for mode, maes in results.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir scale modes a testar\n",
    "scale_modes = ['sqrt', 'none', 'linear', 'log', 'conditional', 'inverse']\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TESTE DE SCALE FACTORS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nModos a testar: {scale_modes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste no dataset CLUSTERED\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATASET: CLUSTERED (clusters bem separados)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "clustered_results = {}\n",
    "for rate in [0.10, 0.20, 0.30]:\n",
    "    print(f\"\\nTaxa de missing: {rate*100:.0f}%\")\n",
    "    results = run_scale_test(df_clustered, rate, scale_modes, k=5, n_runs=3)\n",
    "    clustered_results[rate] = results\n",
    "    \n",
    "    # Ordenar por MAE\n",
    "    sorted_results = sorted(results.items(), key=lambda x: x[1] if pd.notna(x[1]) else 999)\n",
    "    for mode, mae in sorted_results:\n",
    "        marker = \"★\" if mode == sorted_results[0][0] else \" \"\n",
    "        print(f\"  {marker} {mode:<12}: MAE = {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste no dataset CORRELATED\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATASET: CORRELATED (features correlacionadas)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "correlated_results = {}\n",
    "for rate in [0.10, 0.20, 0.30]:\n",
    "    print(f\"\\nTaxa de missing: {rate*100:.0f}%\")\n",
    "    results = run_scale_test(df_correlated, rate, scale_modes, k=5, n_runs=3)\n",
    "    correlated_results[rate] = results\n",
    "    \n",
    "    # Ordenar por MAE\n",
    "    sorted_results = sorted(results.items(), key=lambda x: x[1] if pd.notna(x[1]) else 999)\n",
    "    for mode, mae in sorted_results:\n",
    "        marker = \"★\" if mode == sorted_results[0][0] else \" \"\n",
    "        print(f\"  {marker} {mode:<12}: MAE = {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Análise Comparativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANÁLISE COMPARATIVA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Criar tabela resumo\n",
    "print(\"\\n--- DATASET CLUSTERED ---\")\n",
    "print(f\"{'Mode':<12} {'10%':<10} {'20%':<10} {'30%':<10} {'Média':<10}\")\n",
    "print(\"-\" * 52)\n",
    "for mode in scale_modes:\n",
    "    vals = [clustered_results[r].get(mode, np.nan) for r in [0.10, 0.20, 0.30]]\n",
    "    mean_val = np.nanmean(vals)\n",
    "    print(f\"{mode:<12} {vals[0]:<10.4f} {vals[1]:<10.4f} {vals[2]:<10.4f} {mean_val:<10.4f}\")\n",
    "\n",
    "print(\"\\n--- DATASET CORRELATED ---\")\n",
    "print(f\"{'Mode':<12} {'10%':<10} {'20%':<10} {'30%':<10} {'Média':<10}\")\n",
    "print(\"-\" * 52)\n",
    "for mode in scale_modes:\n",
    "    vals = [correlated_results[r].get(mode, np.nan) for r in [0.10, 0.20, 0.30]]\n",
    "    mean_val = np.nanmean(vals)\n",
    "    print(f\"{mode:<12} {vals[0]:<10.4f} {vals[1]:<10.4f} {vals[2]:<10.4f} {mean_val:<10.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontrar melhor modo para cada dataset\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MELHOR SCALE MODE POR DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Clustered\n",
    "clustered_means = {mode: np.nanmean([clustered_results[r].get(mode, np.nan) for r in [0.10, 0.20, 0.30]]) \n",
    "                   for mode in scale_modes}\n",
    "best_clustered = min(clustered_means.items(), key=lambda x: x[1])\n",
    "print(f\"\\nCLUSTERED: Melhor = '{best_clustered[0]}' (MAE médio = {best_clustered[1]:.4f})\")\n",
    "\n",
    "# Correlated\n",
    "correlated_means = {mode: np.nanmean([correlated_results[r].get(mode, np.nan) for r in [0.10, 0.20, 0.30]]) \n",
    "                    for mode in scale_modes}\n",
    "best_correlated = min(correlated_means.items(), key=lambda x: x[1])\n",
    "print(f\"CORRELATED: Melhor = '{best_correlated[0]}' (MAE médio = {best_correlated[1]:.4f})\")\n",
    "\n",
    "# Geral\n",
    "overall_means = {mode: (clustered_means[mode] + correlated_means[mode]) / 2 for mode in scale_modes}\n",
    "best_overall = min(overall_means.items(), key=lambda x: x[1])\n",
    "print(f\"\\nGERAL: Melhor = '{best_overall[0]}' (MAE médio = {best_overall[1]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Teste da Hipótese de Over-Correction\n",
    "\n",
    "Testar se a combinação de pesos MI re-normalizados + scale factor causa over-correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pds_distance_with_options(sample, donors, weights, \n",
    "                               scale_mode='sqrt', \n",
    "                               renormalize_weights=True):\n",
    "    \"\"\"\n",
    "    Calcula distâncias PDS com opções para testar over-correction.\n",
    "    \n",
    "    renormalize_weights: Se True, re-normaliza pesos para features disponíveis\n",
    "    \"\"\"\n",
    "    n_features = len(sample)\n",
    "    min_overlap = max(2, n_features // 2)\n",
    "    \n",
    "    distances = []\n",
    "    sample_avail = ~np.isnan(sample)\n",
    "    \n",
    "    for donor in donors:\n",
    "        donor_avail = ~np.isnan(donor)\n",
    "        overlap_mask = sample_avail & donor_avail\n",
    "        overlap = overlap_mask.sum()\n",
    "        \n",
    "        if overlap < min_overlap:\n",
    "            distances.append(np.inf)\n",
    "            continue\n",
    "        \n",
    "        # Pesos para features disponíveis\n",
    "        if renormalize_weights:\n",
    "            # Re-normalizar (como fazemos actualmente)\n",
    "            available_weights = weights[overlap_mask]\n",
    "            if available_weights.sum() > 0:\n",
    "                available_weights = available_weights / available_weights.sum()\n",
    "            else:\n",
    "                available_weights = np.ones(overlap) / overlap\n",
    "        else:\n",
    "            # Usar pesos originais sem re-normalizar\n",
    "            available_weights = weights[overlap_mask]\n",
    "        \n",
    "        # Calcular distância\n",
    "        dist_sq = 0.0\n",
    "        j_avail = 0\n",
    "        for j in range(n_features):\n",
    "            if overlap_mask[j]:\n",
    "                diff = sample[j] - donor[j]\n",
    "                dist_sq += available_weights[j_avail] * diff * diff\n",
    "                j_avail += 1\n",
    "        \n",
    "        dist_raw = np.sqrt(dist_sq)\n",
    "        \n",
    "        # Scale factor\n",
    "        ratio = n_features / overlap\n",
    "        if scale_mode == 'sqrt':\n",
    "            scale = np.sqrt(ratio)\n",
    "        elif scale_mode == 'none':\n",
    "            scale = 1.0\n",
    "        else:\n",
    "            scale = 1.0\n",
    "        \n",
    "        distances.append(dist_raw * scale)\n",
    "    \n",
    "    return np.array(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_over_correction(df_complete, missing_rate=0.2, k=5):\n",
    "    \"\"\"\n",
    "    Testa 4 combinações:\n",
    "    1. renorm=True, scale=sqrt (actual)\n",
    "    2. renorm=True, scale=none\n",
    "    3. renorm=False, scale=sqrt\n",
    "    4. renorm=False, scale=none\n",
    "    \"\"\"\n",
    "    combinations = [\n",
    "        ('renorm+scale', True, 'sqrt'),\n",
    "        ('renorm+noscale', True, 'none'),\n",
    "        ('norenorm+scale', False, 'sqrt'),\n",
    "        ('norenorm+noscale', False, 'none'),\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, renorm, scale in combinations:\n",
    "        maes = []\n",
    "        \n",
    "        for run in range(3):\n",
    "            df_missing = introduce_missings(df_complete, rate=missing_rate, seed=42+run)\n",
    "            result = df_missing.copy()\n",
    "            \n",
    "            n_features = len(df_missing.columns)\n",
    "            weights = np.ones(n_features) / n_features\n",
    "            \n",
    "            missing_positions = []\n",
    "            \n",
    "            for col in df_missing.columns:\n",
    "                missing_mask = df_missing[col].isna()\n",
    "                if not missing_mask.any():\n",
    "                    continue\n",
    "                \n",
    "                donor_mask = ~df_missing[col].isna()\n",
    "                if donor_mask.sum() < 2:\n",
    "                    continue\n",
    "                \n",
    "                donor_indices = df_missing[donor_mask].index.tolist()\n",
    "                donor_values = df_missing.loc[donor_mask, col].values\n",
    "                \n",
    "                for idx in df_missing[missing_mask].index:\n",
    "                    missing_positions.append((idx, col))\n",
    "                    \n",
    "                    sample = df_missing.loc[idx].values.astype(float)\n",
    "                    donors = df_missing.loc[donor_indices].values.astype(float)\n",
    "                    \n",
    "                    distances = pds_distance_with_options(\n",
    "                        sample, donors, weights,\n",
    "                        scale_mode=scale,\n",
    "                        renormalize_weights=renorm\n",
    "                    )\n",
    "                    \n",
    "                    valid_mask = np.isfinite(distances)\n",
    "                    if valid_mask.sum() < 1:\n",
    "                        continue\n",
    "                    \n",
    "                    valid_distances = distances[valid_mask]\n",
    "                    valid_values = donor_values[valid_mask]\n",
    "                    \n",
    "                    k_actual = min(k, len(valid_distances))\n",
    "                    top_k_idx = np.argsort(valid_distances)[:k_actual]\n",
    "                    \n",
    "                    top_distances = valid_distances[top_k_idx]\n",
    "                    top_values = valid_values[top_k_idx]\n",
    "                    \n",
    "                    if np.any(top_distances < 1e-10):\n",
    "                        imputed = np.mean(top_values[top_distances < 1e-10])\n",
    "                    else:\n",
    "                        w = 1 / (top_distances + 1e-6)\n",
    "                        w = w / w.sum()\n",
    "                        imputed = np.average(top_values, weights=w)\n",
    "                    \n",
    "                    result.loc[idx, col] = imputed\n",
    "            \n",
    "            # Calcular MAE\n",
    "            errors = []\n",
    "            for idx, col in missing_positions:\n",
    "                true_val = df_complete.loc[idx, col]\n",
    "                imp_val = result.loc[idx, col]\n",
    "                if pd.notna(imp_val):\n",
    "                    errors.append(abs(imp_val - true_val))\n",
    "            \n",
    "            if errors:\n",
    "                maes.append(np.mean(errors))\n",
    "        \n",
    "        results[name] = np.mean(maes) if maes else np.nan\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TESTE DE OVER-CORRECTION\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nTestando combinações de re-normalização de pesos + scale factor...\")\n",
    "\n",
    "print(\"\\n--- DATASET CLUSTERED ---\")\n",
    "oc_clustered = test_over_correction(df_clustered, missing_rate=0.2)\n",
    "for name, mae in sorted(oc_clustered.items(), key=lambda x: x[1]):\n",
    "    marker = \"★\" if mae == min(oc_clustered.values()) else \" \"\n",
    "    print(f\"  {marker} {name:<20}: MAE = {mae:.4f}\")\n",
    "\n",
    "print(\"\\n--- DATASET CORRELATED ---\")\n",
    "oc_correlated = test_over_correction(df_correlated, missing_rate=0.2)\n",
    "for name, mae in sorted(oc_correlated.items(), key=lambda x: x[1]):\n",
    "    marker = \"★\" if mae == min(oc_correlated.values()) else \" \"\n",
    "    print(f\"  {marker} {name:<20}: MAE = {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONCLUSÕES DO TESTE DE OVER-CORRECTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Comparar actual (renorm+scale) com alternativas\n",
    "actual_clustered = oc_clustered['renorm+scale']\n",
    "best_clustered = min(oc_clustered.values())\n",
    "best_name_clustered = [k for k, v in oc_clustered.items() if v == best_clustered][0]\n",
    "\n",
    "actual_correlated = oc_correlated['renorm+scale']\n",
    "best_correlated = min(oc_correlated.values())\n",
    "best_name_correlated = [k for k, v in oc_correlated.items() if v == best_correlated][0]\n",
    "\n",
    "print(f\"\\nCLUSTERED:\")\n",
    "print(f\"  Actual (renorm+scale): {actual_clustered:.4f}\")\n",
    "print(f\"  Melhor ({best_name_clustered}): {best_clustered:.4f}\")\n",
    "if best_clustered < actual_clustered:\n",
    "    improvement = (actual_clustered - best_clustered) / actual_clustered * 100\n",
    "    print(f\"  → Melhoria potencial: {improvement:.1f}%\")\n",
    "\n",
    "print(f\"\\nCORRELATED:\")\n",
    "print(f\"  Actual (renorm+scale): {actual_correlated:.4f}\")\n",
    "print(f\"  Melhor ({best_name_correlated}): {best_correlated:.4f}\")\n",
    "if best_correlated < actual_correlated:\n",
    "    improvement = (actual_correlated - best_correlated) / actual_correlated * 100\n",
    "    print(f\"  → Melhoria potencial: {improvement:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Resumo Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESUMO FINAL\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "Este notebook testou diferentes configurações de scale factor para PDS.\n",
    "\n",
    "VARIÁVEIS TESTADAS:\n",
    "1. Scale factor: sqrt, none, linear, log, conditional, inverse\n",
    "2. Re-normalização de pesos: com vs sem\n",
    "\n",
    "DATASETS:\n",
    "- Clustered: clusters bem separados (caso fácil)\n",
    "- Correlated: features correlacionadas (caso realista)\n",
    "\n",
    "ANÁLISE:\n",
    "- Ver qual scale factor funciona melhor em cada dataset\n",
    "- Ver se há over-correction (renorm + scale)\n",
    "- Identificar configuração óptima\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}